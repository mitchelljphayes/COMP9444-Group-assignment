{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating LLM Variants on the Deep Learning Domain\n",
    "----\n",
    "The explosive popularity of large langauge models (LLMs) since the introduction of chatGPT has brought incredible attention and enthusiasm for businesses and people in general to use and benefit from these breakthrough foundational technologies. However there still exists significant hurdles and gatekeepers to wide spread adoption of this technology. One of these is the closed source nature of OpenAI, another is the monumental computation power needed to train and deploy such a model, and finally the general concern about who's data is being used and how can you trust the information being provided. LLMs are known to confidently state things which are absolutely not factual, yet these non-facts are phrased in very absolute terms, this phenomenon is known as hallucination. \n",
    "\n",
    "Hallucinations are a major cause of concern as human users are not accustomed to such confident falsehoods, and generally believe something if it is presented with high levels of confidence. Hallucinations also pose major issues in business applications, where businesses cannot trust an LLM to represent them nor their products truthfully. Another area that hallucinations will hold back the adoption of these advanced tools is in education, whereby a student cannot be confident that the LLM is telling them a fact that should be noted or a complete fabrication. In this notebook we atempt to explore some potential solutions to this situation. \n",
    "\n",
    "## Problem statement\n",
    "Large language models often suffer from hallucinations, or are largely closed source systems with unknown training data making them inaccessible and unreliable for a large number of use cases. We want to understand if \"Retrieval Augmented Generation\" and or \"Fine Tuning\" on domain specific datasets can decrease the incidence of hallucinations and improve accuracy of responses from open source LLMs.\n",
    "\n",
    "## Project\n",
    "In order to research this question we will explore the topic of building a \"study buddy\" who is an expert in a specific domain, and can answer questions related to the Deep Learning domain. \n",
    "\n",
    "### Two domain specific datasets\n",
    "First we needed access to a domain specific corpus that we could help augment our base models. We obtained 5 common and publicly available textbooks in the Machine Learning and Deep learning domain. We then converted them into a text corpus containing 8605 rows, and a set of 636 question answer pairs, by using [GPT3.5-turbo](https://platform.openai.com/docs/models/gpt-3-5) to analyse the texts and create question answer pairs. \n",
    "The textbooks included were: \n",
    "- Deep Learning - Goodfellow et al\n",
    "- Data Science and ML, Mathematical and Statistical Methods - Kroese et al\n",
    "- Approaching (Almost) Any Machine Learning Problem - Abhishek Thakur \n",
    "- Machine Learning for Humans - Maini et al   \n",
    "- Mathematics for Machine Learning - Deisenroth et al\n",
    "\n",
    "\n",
    "### Baseline Question answering performance from two Open Source LLMs\n",
    "We looked at the [Falcon-7B-Instruct model](https://huggingface.co/tiiuae/falcon-7b-instruct), as it performs well on public benchmarks as the basis for SOTA methods in the open source community. We also looked at GPT2, as a smaller simpler model that is lightweight and easy to finetune. Specifically we looked at [distilGPT2](https://huggingface.co/distilgpt2) a re-engineered open source version of GPT2 that is trained on [OpenWebText](https://huggingface.co/datasets/Skylion007/openwebtext)\n",
    "\n",
    "### Finetuning vs Retrival Augmented Generation\n",
    "We looked at two methods of introducing domain specific data and decreasing hallucination; finetuning and retrival augmented generation (RAG). \n",
    "\n",
    "For both base models we applied the same finetuning approach. Initially we finetuned the models on the [textbook corpus](https://huggingface.co/datasets/mjphayes/textbook_dataset), then to re-align the model to the nature of our intended prompting we ran a small finetuning training on roughly 20% of the [custom question answer pairs](https://huggingface.co/datasets/mjphayes/machine_learning_questions)\n",
    "\n",
    "For the RAG approach, we ingested the text corpus of the text books into a vector store, by tokenizing and converting overlapping chunks of text into vector emmbeddings, which were then searchable for semantic similarity with the presented question. When the question is presented, it is tokenised in the same way and the most similar chunks of text are presented to the model as part of the prompt. This allows the model to essentially \"get the answers\" before needing to generate an answer to the question. \n",
    "\n",
    "Unfortunately there wasn't a suitable adapter for the GPT2 model, so this technique was only applied to the Falcon model.\n",
    "\n",
    "## This Notebook - How to use\n",
    "You can run this notebook top to bottom, although it will need to down load the datasets and the falcon model, which are large and take a lot of time. Inference over the evaluation questions also takes approximately 1 hour per model (less on GPT2 more on Falcon). So running this notebook top to bottom will take you ~6-8 hours including training times. Not recommended. Instead, the datasets and results have already been run and their outputs are visible. You can also run just the results section, which will download the precomputed CSV's of the results from a github Gist. \n",
    "\n",
    "With that said, all the datasources can be downloaded and ran from within the notebook, it will also create a cache and directory structure, so if you do download a model to machine that can run the models then it will cache it to prevent you from needing to run it again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the notebook environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -U bitsandbytes\n",
    "# !pip install -q -U pip\n",
    "# !pip install -q -U datasets\n",
    "# !pip install -q -U accelerate\n",
    "# !pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "# !pip install -q -U einops\n",
    "# !pip install -q -U safetensors\n",
    "# !pip install -q -U torch\n",
    "# !pip install -q -U xformers\n",
    "# !pip install -q -U langchain\n",
    "# !pip install -q -U pypdf\n",
    "# !pip install -q -U pymupdf\n",
    "# !pip install -q -U faiss-gpu\n",
    "# !pip install -q -U bert_score\n",
    "# !pip install -q -U openai\n",
    "# !pip install -q typing-inspect==0.8.0 \n",
    "# !pip install -q typing_extensions==4.5.0\n",
    "# !pip install -q pydantic==1.10.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import logging\n",
    "import transformers\n",
    "import bert_score\n",
    "import torch\n",
    "import openai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import bitsandbytes as bnb\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from matplotlib import rcParams\n",
    "from bert_score import score, BERTScorer\n",
    "from datasets import load_dataset\n",
    "from operator import itemgetter\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, OpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader, DirectoryLoader\n",
    "from huggingface_hub import notebook_login\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "## just to suppress warnings for things like not running on GPU when using langchain\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else :\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up constants to manage local storage and retrieve models and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = '..'\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, 'data')\n",
    "\n",
    "PDF_DIR = os.path.join(PROJECT_ROOT, DATA_DIR, 'pdfs')\n",
    "TEXT_DIR = os.path.join(PROJECT_ROOT, DATA_DIR, 'texts')\n",
    "\n",
    "MODELS_DIR = os.path.join(PROJECT_ROOT, 'models')\n",
    "RESULTS_DIR = os.path.join(PROJECT_ROOT, 'results')\n",
    "\n",
    "TRAIN_DATA_SOURCE = 'mjphayes/textbook_dataset'\n",
    "TRAIN_DATA_CACHE = os.path.join(PROJECT_ROOT, DATA_DIR, 'textbook-dataset')\n",
    "\n",
    "EVAL_DATA_SOURCE = 'mjphayes/machine_learning_questions'\n",
    "EVAL_DATA_CACHE = os.path.join(PROJECT_ROOT, DATA_DIR, 'machine-learning-questions')\n",
    "\n",
    "GPT2_CHECKPOINT = 'distilgpt2'\n",
    "GPT2_CACHE_DIR = os.path.join(PROJECT_ROOT, MODELS_DIR, GPT2_CHECKPOINT)\n",
    "\n",
    "GPT2_FINETUNE_CHECKPOINT = 'mjphayes/distilgpt2-finetuned-textbook_dataset'\n",
    "GPT2_FINETUNE_CACHE_DIR = os.path.join(PROJECT_ROOT, MODELS_DIR, 'distilgpt2-finetuned-textbook_dataset')\n",
    "\n",
    "FALCON_CHECKPOINT = \"vilsonrodrigues/falcon-7b-instruct-sharded\"\n",
    "FALCON_CACHE_DIR = os.path.join(PROJECT_ROOT, MODELS_DIR, 'falcon-7b-instruct')\n",
    "\n",
    "FALCON_FINETUNE_CHECKPOINT = 'mjphayes/falcon-7b-instruct-textbook_dataset'\n",
    "FALCON_FINETUNE_CACHE_DIR = os.path.join(PROJECT_ROOT, MODELS_DIR, 'falcon-7b-instruct-textbook_dataset')\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "if not os.path.exists(MODELS_DIR):\n",
    "    os.makedirs(MODELS_DIR)\n",
    "if not os.path.exists(RESULTS_DIR):\n",
    "    os.makedirs(RESULTS_DIR)\n",
    "if not os.path.exists(PDF_DIR):\n",
    "    os.makedirs(PDF_DIR)\n",
    "if not os.path.exists(TEXT_DIR):\n",
    "    os.makedirs(TEXT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the Question Answer pairs, you'll need an OpenAI API key. To save the trained models in huggingface you'll need a Huggingface Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = os.environ.get('HF_TOKEN')\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the two custom domain specific datasets\n",
    "\n",
    "First we obtain the PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://everythingcomputerscience.com/books/Machine%20Learning%20for%20Humans.pdf -P $PDF_DIR\n",
    "!wget https://mml-book.github.io/book/mml-book.pdf -P $PDF_DIR\n",
    "!wget https://github.com/abhishekkrthakur/approachingalmost/raw/master/AAAMLP.pdf -P $PDF_DIR\n",
    "!wget https://people.smp.uq.edu.au/DirkKroese/DSML/DSML.pdf -P $PDF_DIR\n",
    "!wget https://raw.githubusercontent.com/janishar/mit-deep-learning-book-pdf/master/complete-book-pdf/deeplearningbook.pdf -P $PDF_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting raw text from the textbooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_text(filename):\n",
    "    file = open(filename, 'rb')\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    text = ''\n",
    "    for page in range(len(reader.pages)):\n",
    "        page = reader.pages[page]\n",
    "        text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_text_from_dir(dir):\n",
    "    text = ''\n",
    "    for filename in glob.glob(os.path.join(PDF_DIR, '*.pdf')):\n",
    "        text += extract_pdf_text(filename) + '\\n\\n\\n\\n\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = extract_pdf_text_from_dir(PDF_DIR)\n",
    "file = open(os.path.join(TEXT_DIR, 'combined_textbooks.txt'), 'w')\n",
    "file.write(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the huggingface datasets datamodel from the raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('text', data_files=os.path.join(TEXT_DIR, 'combined_textbooks.txt'))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset will be almost identical to the dataset used in during our training. However we found that we got better results when trying to convert the PDFs to text when we loaded them into google docs and exported them as plain txt files. It's not very programatic, but the PyPDF landscape resulted in a lot of misinterpetations and null characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.push_to_hub(TRAIN_DATA_SOURCE, use_auth_token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we provide the actual compiled text dataset we used to create the textbook_dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_texts = os.path.join(DATA_DIR, 'textbooks.txt')\n",
    "try:\n",
    "    text_books = open(path_to_texts, 'r', encofing=\"utf-8\").read()\n",
    "except:\n",
    "    !wget 'https://gist.githubusercontent.com/mitchelljphayes/82de40eb4ec9275c9b3403fa53665fde/raw/88b0d35d78b4b65d02384980b3e106f20767f7c6/textbooks.txt' -P $DATA_DIR\n",
    "    text_books = open(path_to_texts, 'r', encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues with this dataset\n",
    "\n",
    "due to the formating and type setting of this data, there are a number of rows which add little to no context or information aid our models in this task. We assumed that this would be resolved during the training process as these rows would largely be intepretted as nonsense tokens and ideally would not draw much attention from the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the custom Question Answer dataset\n",
    "\n",
    "In order to create this dataset we needed to use a state of the art model and utilise it's text extraction properties. Todo so we utilised GPT3.5-turbo and a library called langchain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.2, openai_api_key=OPENAI_API_KEY, model='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = {\n",
    "    \"Question\": \"A thoughtful question about the text.\",\n",
    "    \"Answer\": \"A factual and complete, yet concise answer to the question.\",\n",
    "    \"required\": [\"Question\", \"Answer\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a world class algorithm for extracting information in structured formats.\n",
    "The user input is messy raw text. That was extracted from a PDF of a textbook page by PyPDF.\n",
    "Your job is to analyse the text and extract a useful question from the information. You will also need to provive a concise answer to the question.\n",
    "Make sure you use the correct format for the output.\n",
    "\"\"\".strip()\n",
    "\n",
    "system_prompt_schema = \"\"\"\n",
    "Use the following schema to extract the questions and answers. \\n\\n\n",
    "Schema:\n",
    "\\n\\n\\n\n",
    "\"\"\"\n",
    "\n",
    "def question_extraction(text, json_schema, llm=llm, system_prompt=system_prompt, system_prompt_schema=system_prompt_schema):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"{system_prompt_schema} + {json_schema}\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"raw pdf text; extract and format into json:\" + text\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    api_params = {\"model\": llm, \"messages\": messages, \"stream\": True}\n",
    "    try:\n",
    "        api_response = openai.ChatCompletion.create(**api_params)\n",
    "        reply = \"\"\n",
    "        for delta in api_response:\n",
    "            if not delta['choices'][0]['finish_reason']:\n",
    "                word = delta['choices'][0]['delta']['content']\n",
    "                reply += word\n",
    "                print(word, end =\"\")       \n",
    "        return reply\n",
    "    except Exception as err:\n",
    "        error_message = f\"API Error: {str(err)}\"\n",
    "        print(error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=48,\n",
    "    length_function=len,\n",
    "    add_start_index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.create_documents([text_books])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[0]\n",
    "page = texts[0].page_content\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_and_answers = []\n",
    "for text in texts[:10]:\n",
    "    qa = question_extraction(text.page_content, json_schema, llm, system_prompt, system_prompt_schema)\n",
    "    questions_and_answers.append(qa)\n",
    "\n",
    "combined_qa = '\\n\\n'.join(questions_and_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_raw = open(os.path.join(DATA_DIR, 'question_answer_raw.txt'), 'w')\n",
    "question_answer_raw.write(combined_qa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The questions generated are still ill formatted and require manual intervention before they can be used. In this notebook, we have included a link to the final JSON output used to generate the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_questions = os.path.join(DATA_DIR, 'machine-learning-questions.json')\n",
    "try:\n",
    "    questions = pd.read_json(path_to_questions)\n",
    "except:\n",
    "    !wget https://gist.githubusercontent.com/mitchelljphayes/7cb8fafc2959f2ef2d54020e70ff5283/raw/f93ca8dd70378fb434fa5acb3fc69e27ed54b05e/machine-learning-questions.json -P $DATA_DIR\n",
    "    questions = pd.read_json(path_to_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating sentence similarity evaluations using BertScore\n",
    "In order to evaluate the effectiveness of the LLMs in answering the Questions, we used the BertScore, to allow evaluate sentance similarity to the intended response. More information about BertScore can be found at the [repo](https://github.com/Tiiiger/bert_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = BERTScorer(lang=\"en\")\n",
    "def evaluate_answers(dataset, generated_answers, scorer=scorer):\n",
    "    ideal = dataset['answer']\n",
    "    P, R, F1 = scorer.score(ideal, generated_answers)\n",
    "    df = pd.DataFrame({'question': dataset['question'], 'ideal': ideal, 'generated': generated_answers, 'P': P, 'R': R, 'F1': F1})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Falcon 7 B\n",
    "[Falcon-7B-Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) is the 7 Billion parameter verison of the Falcon model. It is finetuned on a mixture of chat and instruction datasets and is open source. The creators of the model, the [technology innovation institute](https://www.tii.ae/) are planning to release a paper to explain the model more completely, but as of writing this is yet to be published. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising the baseline falcon-7b-instruct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "        FALCON_CHECKPOINT, \n",
    "        device_map='auto',\n",
    "        quantization_config=quantization_config,\n",
    "        cache_dir=FALCON_CACHE_DIR,\n",
    "        )\n",
    "falcon_tokenizer = AutoTokenizer.from_pretrained(FALCON_CHECKPOINT, cache_dir=FALCON_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_tokenizer.pad_token = falcon_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "  \"\"\"\n",
    "  Prints the number of trainable parameters in the model.\n",
    "  \"\"\"\n",
    "  trainable_params = 0\n",
    "  all_param = 0\n",
    "  for _, param in model.named_parameters():\n",
    "    all_param += param.numel()\n",
    "    if param.requires_grad:\n",
    "      trainable_params += param.numel()\n",
    "  print(\n",
    "      f\"trainable params: {trainable_params} || all params: {all_param} || trainables%: {100 * trainable_params / all_param}\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_4bit.gradient_checkpointing_enable()\n",
    "falcon_4bit = prepare_model_for_kbit_training(falcon_4bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "falcon_4bit = get_peft_model(falcon_4bit, config)\n",
    "print_trainable_parameters(falcon_4bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    generation_config = falcon_4bit.generation_config\n",
    "    generation_config.max_new_tokens = 312\n",
    "    generation_config.temperature = 0.7\n",
    "    generation_config.top_p = 0.7\n",
    "    generation_config.num_return_sequences = 1\n",
    "    generation_config.pad_token_id = falcon_tokenizer.eos_token_id\n",
    "    generation_config.eos_token_id = falcon_tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Question: What is machine learning?\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "%time\n",
    "encoding = falcon_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "  outputs = falcon_4bit.generate(\n",
    "      input_ids = encoding.input_ids,\n",
    "      attention_mask = encoding.attention_mask,\n",
    "      generation_config = generation_config\n",
    "  )\n",
    "\n",
    "print(falcon_tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the baseline falcon model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def falcon_inference(question, model, tokenizer, device):\n",
    "    prompt = f\"\"\"\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\".strip()\n",
    "    generation_config = model.generation_config\n",
    "    generation_config.max_new_tokens = 312\n",
    "    generation_config.temperature = 0.7\n",
    "    generation_config.top_p = 0.7\n",
    "    generation_config.num_return_sequences = 1\n",
    "    generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "    generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.inference_mode():\n",
    "      outputs = model.generate(\n",
    "          input_ids = encoding.input_ids,\n",
    "          attention_mask = encoding.attention_mask,\n",
    "          generation_config = generation_config\n",
    "      )\n",
    "    initial_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer = initial_answer.split('Answer:')[-1]\n",
    "    return answer.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(evaluation_dataset, inference_function, model, tokenizer, device):\n",
    "    results = []\n",
    "    for i in tqdm(range(len(evaluation_dataset))):\n",
    "        question = evaluation_dataset[i]['question']\n",
    "        answer = inference_function(question, model, tokenizer, device)\n",
    "        results.append(answer)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = load_dataset(EVAL_DATA_SOURCE, cache_dir=EVAL_DATA_CACHE, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = falcon_inference('What is machine learning?', falcon_4bit, falcon_tokenizer, device)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_answers = run_evaluation(eval_data, falcon_inference, falcon_4bit, falcon_tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_results = evaluate_answers(eval_data, falcon_answers)\n",
    "falcon_results.to_csv(os.path.join(RESULTS_DIR, 'falcon_results.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning falcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textbook_dataset = load_dataset(TRAIN_DATA_SOURCE, cache_dir=TRAIN_DATA_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_for_falcon(examples):\n",
    "    return falcon_tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = textbook_dataset.map(tokenize_for_falcon, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data = tokenized_text['train'].train_test_split(test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = falcon_tokenizer.model_max_length\n",
    "# block_size = 128\n",
    "block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_inputs(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = split_data.map(\n",
    "    group_inputs,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=8,\n",
    "    evaluation_strategy=\"epoch\", \n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=1,\n",
    "    output_dir=\"experiments\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=falcon_4bit,\n",
    "    train_dataset=grouped_data['train'],\n",
    "    eval_dataset=grouped_data['test'],\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForLanguageModeling(falcon_tokenizer, mlm=False)\n",
    ")\n",
    "falcon_4bit.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(FALCON_FINETUNE_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    falcon_finetune = falcon_4bit\n",
    "except:  \n",
    "    falcon_finetune = AutoModelForCausalLM.from_pretrained(\n",
    "        FALCON_FINETUNE_CHECKPOINT,\n",
    "        quantization_config=quantization_config, \n",
    "        cache_dir=FALCON_FINETUNE_CACHE_DIR,\n",
    "        device_map='auto'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "  return f\"\"\"\n",
    "Question: {data_point[\"question\"]}\n",
    "Answer: {data_point[\"answer\"]}\n",
    "\"\"\".strip()\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "  full_prompt = generate_prompt(data_point)\n",
    "  tokenized_full_prompt = falcon_tokenizer(full_prompt, padding=True, truncation=True)\n",
    "  return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_data = load_dataset(EVAL_DATA_SOURCE, cache_dir=EVAL_DATA_CACHE, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_data = alignment_data.map(generate_and_tokenize_prompt).remove_columns(['question', 'answer']).train_test_split(test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_trainer = transformers.Trainer(\n",
    "    model=falcon_finetune,\n",
    "    train_dataset=alignment_data['train'],\n",
    "    eval_dataset=alignment_data['test'],\n",
    "    args=training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(falcon_tokenizer, mlm=False)\n",
    ")\n",
    "falcon_finetune.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_finetune.save_pretrained(FALCON_FINETUNE_CACHE_DIR)\n",
    "falcon_tokenizer.save_pretrained(FALCON_FINETUNE_CACHE_DIR)\n",
    "# falcon_finetune.push_to_hub(FALCON_FINETUNE_CHECKPOINT, token=HF_TOKEN)\n",
    "# falocn_tokenizer.push_to_hub(FALCON_FINETUNE_CHECKPOINT, token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now reload the model into memory, so that it is configured for inference, and we have a point at which we can run the notebook, to skip the training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_finetune = AutoModelForCausalLM.from_pretrained(FALCON_FINETUNE_CHECKPOINT, cache_dir=FALCON_FINETUNE_CACHE_DIR, device_map='auto', quantization_config=quantization_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the finetuned falcon model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Questin: What is an LSTM?\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "encoding = falcon_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "# with torch.no_grad():\n",
    "  outputs = falcon_finetune.generate(\n",
    "      input_ids = encoding.input_ids,\n",
    "      attention_mask = encoding.attention_mask,\n",
    "      generation_config = generation_config\n",
    "  )\n",
    "print(falcon_tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = falcon_inference(\"What is an LSTM\", falcon_finetune, falcon_tokenizer, device)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_finetune_answers = run_evaluation(eval_data, falcon_inference, falcon_finetune, falcon_tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_finetune_results = evaluate_answers(eval_data, falcon_finetune_answers)\n",
    "falcon_finetune_results.to_csv(os.path.join(RESULTS_DIR, 'falcon_finetune_results.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = AutoModelForCausalLM.from_pretrained(GPT2_CHECKPOINT, cache_dir=GPT2_CACHE_DIR)\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(GPT2_CHECKPOINT, cache_dir=GPT2_CACHE_DIR, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = {'eos_token': '<|endoftext|>'}\n",
    "num_added_toks = gpt2_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "gpt2.resize_token_embeddings(len(gpt2_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2_inference(question, model, tokenizer, device):\n",
    "    prompt = f\"\"\"\n",
    "    Question: {question} \\n\n",
    "    Answer:\n",
    "    \"\"\".strip()\n",
    "    model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    generated_tokens = model.generate(**model_inputs, max_length=512)\n",
    "    response = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    answer = response.split('Answer:')[-1]\n",
    "    return answer.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = gpt2_inference(\"Explain how neural networks work\", gpt2, gpt2_tokenizer, device)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_answers = run_evaluation(eval_data, gpt2_inference, gpt2, gpt2_tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_results = evaluate_answers(eval_data, gpt2_answers)\n",
    "gpt2_results.to_csv(os.path.join(RESULTS_DIR, 'gpt2_results.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textbook_dataset = load_dataset(TRAIN_DATA_SOURCE, cache_dir=TRAIN_DATA_CACHE)\n",
    "textbook_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_for_gpt2(examples):\n",
    "    return gpt2_tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenized_text = textbook_dataset.map(tokenize_for_gpt2, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = gpt2_tokenizer.model_max_length\n",
    "block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_inputs(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_text_data = gpt2_tokenized_text.map(\n",
    "    group_inputs,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_text_data = grouped_text_data['train'].train_test_split(test_size=0.05, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    GPT2_FINETUNE_CACHE_DIR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=8,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=gpt2,\n",
    "    args=training_args,\n",
    "    train_dataset=grouped_text_data['train'],\n",
    "    eval_dataset=grouped_text_data['test'],\n",
    "    data_collator=DataCollatorForLanguageModeling(gpt2_tokenizer, mlm=False),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_for_q_and_a(examples):\n",
    "    return {\"text\": f\"Question: {examples['question']} \\n Answer: {examples['answer']}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(GPT2_FINETUNE_CACHE_DIR)\n",
    "gpt2_tokenizer.save_pretrained(GPT2_FINETUNE_CACHE_DIR)\n",
    "gpt2_finetune = AutoModelForCausalLM.from_pretrained(GPT2_FINETUNE_CHECKPOINT, cache_dir=GPT2_FINETUNE_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_data = (load_dataset(EVAL_DATA_SOURCE, cache_dir=EVAL_DATA_CACHE, split='test')\n",
    "                    .train_test_split(test_size=0.1, shuffle=True)\n",
    "                    .map(transform_for_q_and_a, num_proc=4)\n",
    "                    .map(tokenize_for_gpt2, batched=True, num_proc=4, remove_columns=[\"text\"])\n",
    "                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    GPT2_FINETUNE_CACHE_DIR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=8,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "alignment_trainer = Trainer(\n",
    "    model=gpt2_finetune,\n",
    "    args=training_args,\n",
    "    train_dataset=alignment_data['train'],\n",
    "    eval_dataset=alignment_data['test'],\n",
    "    data_collator=DataCollatorForLanguageModeling(gpt2_tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_trainer.save_model(GPT2_FINETUNE_CACHE_DIR)\n",
    "gpt2_tokenizer.save_pretrained(GPT2_FINETUNE_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt2.push_to_hub(GPT2_FINETUNE_CHECKPOINT, use_auth_token=HF_TOKEN)\n",
    "# gpt2_tokenizer.push_to_hub(GPT2_FINETUNE_CHECKPOINT, use_auth_token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_finetune = AutoModelForCausalLM.from_pretrained(GPT2_FINETUNE_CHECKPOINT, cache_dir=GPT2_FINETUNE_CACHE_DIR).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_finetune_answers = run_evaluation(eval_data, gpt2_inference, gpt2_finetune, gpt2_tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_finetune_results = evaluate_answers(eval_data, gpt2_finetune_answers)\n",
    "gpt2_finetune_results.to_csv(os.path.join(RESULTS_DIR, 'gpt2_finetune_results.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading falcon for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_4bit_rag = AutoModelForCausalLM.from_pretrained(\n",
    "        FALCON_CHECKPOINT, \n",
    "        device_map='auto',\n",
    "        quantization_config=rag_quantization_config,\n",
    "        cache_dir=FALCON_CACHE_DIR,\n",
    "        )\n",
    "falcon_tokenizer_rag = AutoTokenizer.from_pretrained(FALCON_CHECKPOINT, cache_dir=FALCON_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=falcon_4bit_rag,\n",
    "        tokenizer=falcon_tokenizer_rag,\n",
    "        use_cache=True,\n",
    "        device_map=\"auto\",\n",
    "        max_length=2048,\n",
    "        do_sample=True,\n",
    "        top_k=3,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=falcon_tokenizer_rag.eos_token_id,\n",
    "        pad_token_id=falcon_tokenizer_rag.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_books = open(path_to_texts, 'r', encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_falcon = HuggingFacePipeline(pipeline=rag_pipeline)\n",
    "rag_embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=48,\n",
    "    length_function=len,\n",
    "    add_start_index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_books = text_books.strip()\n",
    "text_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.create_documents([text_books])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(texts, embeddings)\n",
    "vector_store.save_local('./storage/vector_store')\n",
    "vector_store = FAISS.load_local('./storage/vector_store', embeddings)\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = vector_store.similarity_search(\"What is a neural network?\", k=3)\n",
    "source = search[0].page_content\n",
    "source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = retriever.get_relevant_documents(\"What is a neural network?\", k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the context provided bellow:\n",
    "\n",
    "INSTRUCTIONS: \n",
    "Ensure that the answer is grammatically correct and relevant to the context.\n",
    "DO NOT include any HTML tags in your answer.\n",
    "DO NOT include any information that is not directly relevant to the question.\n",
    "Be succinct and to the point. Keep your answer to one or two sentences.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | rag_falcon\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = load_dataset(EVAL_DATA_SOURCE, cache_dir=EVAL_DATA_CACHE, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "rand = randint(0, len(eval_data))\n",
    "random_question = eval_data[rand]['question']\n",
    "ideal_answer = eval_data[rand]['answer']\n",
    "print(random_question)\n",
    "generated_answer = chain.invoke({\"question\": random_question})\n",
    "generated_answer = generated_answer.split('Answer: ')[-1].strip().split('\\n')[0]\n",
    "print(generated_answer)\n",
    "print(\"=\"*86)\n",
    "print(ideal_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_inference(question, model, tokenizer, device):\n",
    "    chain = model\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    answer = response.split('Answer: ')[-1].strip().split('\\n')[0]\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_answers = run_evaluation(eval_data, rag_inference, chain, falcon_tokenizer_rag, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_results = evaluate_answers(eval_data, rag_answers)\n",
    "rag_results.to_csv(os.path.join(RESULTS_DIR, 'rag_results.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_results_df = pd.read_csv(os.path.join(RESULTS_DIR, 'gpt2_results.csv'))\n",
    "gpt2_finetune_results_df = pd.read_csv(os.path.join(RESULTS_DIR, 'gpt2_finetune_results.csv'))\n",
    "falcon_results_df = pd.read_csv(os.path.join(RESULTS_DIR, 'falcon_results.csv'))\n",
    "falcon_finetune_results_df = pd.read_csv(os.path.join(RESULTS_DIR, 'falcon_finetune_results.csv'))\n",
    "rag_results_df = pd.read_csv(os.path.join(RESULTS_DIR, 'rag_results.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(df, model_name):\n",
    "    print(f\"{model_name:20s} Mean Precision: {df['P'].mean()}\")\n",
    "    print(f\"{model_name:20s} Mean Recall:    {df['R'].mean()}\")\n",
    "    print(f\"{model_name:20s} Mean F1:        {df['F1'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_with_results = {\n",
    "    'GPT2': gpt2_results_df,\n",
    "    'GPT2 Finetuned': gpt2_finetune_results_df,\n",
    "    'FALCON': falcon_results_df,\n",
    "    'FALCON Finetuned': falcon_finetune_results_df,\n",
    "    'RAG': rag_results_df\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, results_df in models_with_results.items():\n",
    "    print(\"=\"*55)\n",
    "    print_summary(results_df, model_name)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_f1_scores(df, model_name):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.hist(df['F1'], bins=100)\n",
    "    plt.title(f\"{model_name} F1 Scores\")\n",
    "    plt.style.use('ggplot')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, results_df in models_with_results.items():\n",
    "    show_f1_scores(results_df, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_average_scores(df, model_name):\n",
    "    average_precision = df['P'].mean()\n",
    "    average_recall = df['R'].mean()\n",
    "    average_f1 = df['F1'].mean()\n",
    "\n",
    "    scores = [average_precision, average_recall, average_f1]\n",
    "    labels = ['Precision', 'Recall', 'F1 Score']\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    barlist = plt.bar(labels, scores, color=['blue', 'green', 'red'])\n",
    "    for idx, bar in enumerate(barlist):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height, f'{scores[idx]:.3f}', ha='center', va='bottom')\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Scores')\n",
    "    plt.title(f'{model_name} Average Precision, Recall, and F1 Score')\n",
    "    plt.style.use('ggplot')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(df):\n",
    "    above_threshold = df['F1'] > 0.8\n",
    "\n",
    "    accuracy_score = above_threshold.sum() / len(df)\n",
    "\n",
    "    return accuracy_score\n",
    "\n",
    "\n",
    "accuracy_gpt2 = calculate_accuracy(gpt2_results_df)\n",
    "accuracy_gpt2_finetune = calculate_accuracy(gpt2_finetune_results_df)\n",
    "accuracy_falcon = calculate_accuracy(falcon_results_df)\n",
    "accuracy_falcon_finetune = calculate_accuracy(falcon_finetune_results_df)\n",
    "accuracy_rag = calculate_accuracy(rag_results_df)\n",
    "\n",
    "\n",
    "print(\"Accuracy GPT2:\", accuracy_gpt2)\n",
    "print(\"Accuracy Finetuned GPT2:\", accuracy_gpt2_finetune)\n",
    "print(\"Accuracy Falcon:\", accuracy_falcon)\n",
    "print(\"Accuracy Finetuned Falcon:\", accuracy_falcon_finetune)\n",
    "print(\"Accuracy RAG:\", accuracy_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision_gpt2 = gpt2_results_df['P'].mean()\n",
    "average_recall_gpt2 = gpt2_results_df['R'].mean()\n",
    "average_f1_gpt2 = gpt2_results_df['F1'].mean()\n",
    "\n",
    "average_precision_gpt2_finetune = gpt2_finetune_results_df['P'].mean()\n",
    "average_recall_gpt2_finetune = gpt2_finetune_results_df['R'].mean()\n",
    "average_f1_gpt2_finetune = gpt2_finetune_results_df['F1'].mean()\n",
    "\n",
    "average_precision_falcon = falcon_results_df['P'].mean()\n",
    "average_recall_falcon = falcon_results_df['R'].mean()\n",
    "average_f1_falcon = falcon_results_df['F1'].mean()\n",
    "\n",
    "average_precision_falcon_finetune = falcon_finetune_results_df['P'].mean()\n",
    "average_recall_falcon_finetune = falcon_finetune_results_df['R'].mean()\n",
    "average_f1_falcon_finetune = falcon_finetune_results_df['F1'].mean()\n",
    "\n",
    "average_precision_rag = rag_results_df['P'].mean()\n",
    "average_recall_rag = rag_results_df['R'].mean()\n",
    "average_f1_rag = rag_results_df['F1'].mean()\n",
    "precisions = [average_precision_gpt2, average_precision_gpt2_finetune, average_precision_falcon, average_precision_falcon_finetune, average_precision_rag]\n",
    "recalls = [average_recall_gpt2, average_recall_gpt2_finetune, average_recall_falcon, average_recall_falcon_finetune, average_recall_rag]\n",
    "f1_scores = [average_f1_gpt2, average_f1_gpt2_finetune, average_f1_falcon, average_f1_falcon_finetune, average_f1_rag]\n",
    "accuracies = [accuracy_gpt2, accuracy_gpt2_finetune, accuracy_falcon, accuracy_falcon_finetune, accuracy_rag]\n",
    "\n",
    "\n",
    "labels = ['GPT2', 'Finetuned GPT2', 'Falcon 7B', 'Finetuned Falcon 7B', 'RAG']\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.15\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "rects1 = ax.bar(x - 1.5*width, precisions, width, label='Precision', color='blue')\n",
    "rects2 = ax.bar(x - 0.5*width, recalls, width, label='Recall', color='green')\n",
    "rects3 = ax.bar(x + 0.5*width, f1_scores, width, label='F1', color='red')\n",
    "rects4 = ax.bar(x + 1.5*width, accuracies, width, label='Accuracy', color='purple')\n",
    "\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Comparison of Precision, Recall, F1, and Accuracy Across Different Models')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "def add_labels(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "\n",
    "add_labels(rects1)\n",
    "add_labels(rects2)\n",
    "add_labels(rects3)\n",
    "add_labels(rects4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discusion \n",
    "The base model of Falcon outperforms its other iterations as it demonstrated superior performance right from the start. We can also see that GPT2 performed more poorly as well. When lookin at the F1 scores it seems there were a lot of empty scores for GPT2-finetuned which may have been caused by a token length issue. \n",
    "\n",
    "The RAG model performed reasonably well with a slight improvement in accuracy, when the threshold for correctness is an F1 score greater than 0.8\n",
    "\n",
    "We hypothesize that the reason is the training on raw textbook content which might affect the previous alignment training that had already been done on these models. \n",
    "\n",
    "It seems clear that the finetuning methodology used caused the models to include nonsensical trailing data which negatively impacted the BertScore Metric.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "The obstacles encountered in this project were diverse and spanned across both data curation and technological elements of our models. Data curationa vital process for our model's domain awarenessrequires extensive time and resources. Both fine-tuned and RAG models also exhibited a propensity to generate incorrect or \"hallucinated\" responses when faced with queries beyond their augmented data's context. The increased duration for RAG inference due to an additional retrieval phase was another issue. Moreover, evaluating these models is currently restricted to text-based data, thereby hindering their interaction with different modalities such as diagrams and confining their usefulness in varied educational environments.\n",
    "\n",
    "Furthermore the generated questions often contained overly specific questions based on figures or examples listed in the text, rather than simply domain specific questions. \n",
    "\n",
    "Should timing allow, there are multiple potential paths to improving our outcomes. One method might include bolstering synthetic data generation through additional annotations or testing different prompting tactics. Expanding the vector database to cover a wider array of contentfor instance, integrating more textbookswould likely enhance the RAG model's performance. Alternatives such as optimizing tokenization and indexing strategies or utilizing summarized content instead of entire texts might further improve RAG's efficiency. Finally, extending the training and evaluation of these models to incorporate multi-modal data could potentially widen their use and augment their interaction abilities in educational scenarios."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
