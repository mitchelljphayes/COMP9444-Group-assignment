{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mitchelljphayes/COMP9444-group-assignment/blob/main/nx_base_FALCON_7Binstruct.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers accelerate datasets bitsandbytes einops wandb bert_score\n",
    "import os\n",
    "import logging\n",
    "# import transformers\n",
    "import bert_score\n",
    "import pandas as pd\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from bert_score import score, BERTScorer\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "# transformers.tokenization_utils.logger.setLevel(logging.ERROR)\n",
    "# transformers.configuration_utils.logger.setLevel(logging.ERROR)\n",
    "# transformers.modeling_utils.logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "bert_score.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "# model = \"mrm8488/falcoder-7b\"\n",
    "model = \"tiiuae/falcon-7b-instruct\"\n",
    "# model = \"tiiuae/falcon-40b-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"mjphayes/machine_learning_questions\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [i['question'] for i in dataset]\n",
    "\n",
    "# Batch processing\n",
    "batch_size = 8  # Adjust as needed\n",
    "batches = [questions[i:i+batch_size] for i in range(0, len(questions), batch_size)]\n",
    "\n",
    "# Initialize an empty DataFrame to store results\n",
    "result = pd.DataFrame(columns=['questions', 'answer', 'model_answer'])\n",
    "\n",
    "# Process batches\n",
    "for batch in batches:\n",
    "    gens = pipeline(\n",
    "        batch,\n",
    "        max_length=200,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    for i, gen in enumerate(gens):\n",
    "        result.loc[len(result)] = [batch[i], dataset[i]['answer'], gen[0]['generated_text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_b = result.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_b.to_csv('nx_7binstruct_base_trainset_output.csv',index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0ymIuNel0Op"
   },
   "source": [
    "STOP HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgNgF_VBl2FM"
   },
   "source": [
    "gen2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]['__index_level_0__']\n",
    "result = pd.DataFrame(columns=['questions','answer','model_answer'])\n",
    "for i in dataset:\n",
    "  result.loc[i['__index_level_0__']] = [i['question'],i['answer'],str(i['__index_level_0__'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(columns=['questions','answer','model_answer'])\n",
    "# for i in dataset:\n",
    "#   result.loc[i['__index_level_0__']] = [i['question'],i['answer'],str(i['__index_level_0__'])]\n",
    "\n",
    "for i in dataset:\n",
    "  prompt = i['question']\n",
    "  gen = pipeline(\n",
    "    prompt,\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "  )\n",
    "  result.loc[i['__index_level_0__']] = [i['question'],i['answer'],gen[0]['generated_text']]\n",
    "  # result.loc[i['__index_level_0__']] = [i['question'],i['answer'],str(i['__index_level_0__'])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_b = result.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_b.to_csv('nx_7binstruct_base_trainset_output1.csv',index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRL7d7_6maja"
   },
   "source": [
    "if model_answer still includes the q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['model_answer'][2].split('\\n')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sanitised = result.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sanitised['model_answer'] = result_sanitised['model_answer'].apply(lambda x: x.split('\\n')[1] if '\\n' in x else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sanitised[2:3]['model_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sanitised.to_csv('nx_7binstruct_baseline_output3.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KXf4tHjlxNP"
   },
   "source": [
    "run eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = pd.read_csv('nx_7binstruct_baseline_output2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_answers = eval_set['answer']\n",
    "gen_answers = eval_set['model_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, R, F1 = score(gen_answers, ideal_answers, lang='en', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"System level F1 score: {F1.mean():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
