{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U pip\n",
    "!pip install -q -U datasets\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U einops\n",
    "!pip install -q -U safetensors\n",
    "!pip install -q -U torch\n",
    "!pip install -q -U xformers\n",
    "!pip install -q -U langchain\n",
    "!pip install -q -U pypdf\n",
    "!pip install -q -U pymupdf\n",
    "!pip install -q -U faiss-gpu\n",
    "!pip install -q -U bert_score\n",
    "!pip install -q -U spacy\n",
    "!pip install -q typing-inspect==0.8.0 \n",
    "!pip install -q typing_extensions==4.5.0\n",
    "!pip install -q pydantic==1.10.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import transformers\n",
    "import bert_score\n",
    "import torch\n",
    "import pandas as pd\n",
    "import bitsandbytes as bnb\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from matplotlib import rcParams\n",
    "from bert_score import score, BERTScorer\n",
    "from datasets import load_dataset\n",
    "from operator import itemgetter\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain import PromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader, DirectoryLoader\n",
    "from huggingface_hub import notebook_login\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "## just to suppress warnings for things like not running on GPU when using langchain\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else :\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = '.'\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, 'data')\n",
    "MODELS_DIR = os.path.join(PROJECT_ROOT, 'models')\n",
    "RESULTS_DIR = os.path.join(PROJECT_ROOT, 'results')\n",
    "\n",
    "TRAIN_DATA_SOURCE = 'mjphayes/textbook_dataset'\n",
    "TRAIN_DATA_CACHE = os.path.join(PROJECT_ROOT, DATA_DIR, 'textbook-dataset')\n",
    "\n",
    "EVAL_DATA_SOURCE = 'mjphayes/machine_learning_questions'\n",
    "EVAL_DATA_CACHE = os.path.join(PROJECT_ROOT, DATA_DIR, 'machine-learning-questions')\n",
    "\n",
    "GPT2_CHECKPOINT = 'distilgpt2'\n",
    "GPT2_CACHE_DIR = os.path.join(PROJECT_ROOT, MODELS_DIR, GPT2_CHECKPOINT)\n",
    "\n",
    "GPT2_FINETUNE_CHECKPOINT = 'mjphayes/distilgpt2-finetuned-textbook_dataset'\n",
    "GPT2_FINETUNE_CACHE_DIR = os.path.join(PROJECT_ROOT, MODELS_DIR, GPT2_FINETUNE_CHECKPOINT)\n",
    "\n",
    "FALCON_CHECKPOINT = \"vilsonrodrigues/falcon-7b-instruct-sharded\"\n",
    "FALCON_CACHE_DIR = os.path.join(PROJECT_ROOT, MODELS_DIR, 'falcon-7b-instruct')\n",
    "\n",
    "FALCON_FINETUNE_CHECKPOINT = 'mjphayes/falcon-7b-instruct-textbook_dataset'\n",
    "FALCON_FINETUNE_CACHE_DIR = os.path.join(PROJECT_ROOT, MODELS_DIR, FALCON_FINETUNE_CHECKPOINT)\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "if not os.path.exists(MODELS_DIR):\n",
    "    os.makedirs(MODELS_DIR)\n",
    "if not os.path.exists(RESULTS_DIR):\n",
    "    os.makedirs(RESULTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_texts = os.path.join(DATA_DIR, 'textbooks.txt')\n",
    "try:\n",
    "    text_books = open(path_to_texts, 'r').read()\n",
    "except:\n",
    "    !wget 'https://gist.githubusercontent.com/mitchelljphayes/82de40eb4ec9275c9b3403fa53665fde/raw/88b0d35d78b4b65d02384980b3e106f20767f7c6/textbooks.txt' -P $DATA_DIR\n",
    "    text_books = open(path_to_texts, 'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Falcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "        FALCON_CHECKPOINT, \n",
    "        device_map='auto',\n",
    "        quantization_config=quantization_config,\n",
    "        cache_dir=FALCON_CACHE_DIR,\n",
    "        )\n",
    "falcon_tokenizer = AutoTokenizer.from_pretrained(FALCON_CHECKPOINT, cache_dir=FALCON_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_tokenizer.pad_token = falcon_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "  \"\"\"\n",
    "  Prints the number of trainable parameters in the model.\n",
    "  \"\"\"\n",
    "  trainable_params = 0\n",
    "  all_param = 0\n",
    "  for _, param in model.named_parameters():\n",
    "    all_param += param.numel()\n",
    "    if param.requires_grad:\n",
    "      trainable_params += param.numel()\n",
    "  print(\n",
    "      f\"trainable params: {trainable_params} || all params: {all_param} || trainables%: {100 * trainable_params / all_param}\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_4bit.gradient_checkpointing_enable()\n",
    "falcon_4bit = prepare_model_for_kbit_training(falcon_4bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "falcon_4bit = get_peft_model(falcon_4bit, config)\n",
    "print_trainable_parameters(falcon_4bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "<human>: What is machine learning?\n",
    "<assistant>:\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = falcon_4bit.generation_config\n",
    "generation_config.max_new_tokens = 512\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = falcon_tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = falcon_tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "encoding = falcon_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "  outputs = falcon_4bit.generate(\n",
    "      input_ids = encoding.input_ids,\n",
    "      attention_mask = encoding.attention_mask,\n",
    "      generation_config = generation_config\n",
    "  )\n",
    "\n",
    "print(falcon_tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def falcon_inference(question, model, tokenizer, device, generation_config=generation_config):\n",
    "    prompt = f\"\"\"\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\".strip()\n",
    "\n",
    "    model.to(device)\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "      outputs = model.generate(\n",
    "          input_ids = encoding.input_ids,\n",
    "          attention_mask = encoding.attention_mask,\n",
    "          generation_config = generation_config\n",
    "      )\n",
    "    inital_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer = inital_answer.split('Answer:')[-1]\n",
    "    return first_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(evaluation_dataset, inference_function, model, tokenizer, device):\n",
    "    results = []\n",
    "    for i in tqdm(range(len(evaluation_dataset))):\n",
    "        question = evaluation_dataset[i]['question']\n",
    "        answer = inference_function(question, model, tokenizer, device)\n",
    "        results.append(answer)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = BERTScorer(lang=\"en\")\n",
    "def evaluate_answers(dataset, generated_answers, scorer=scorer):\n",
    "    ideal = dataset['answer']\n",
    "    P, R, F1 = scorer.score(ideal, generated_answers)\n",
    "    df = pd.DataFrame({'question': dataset['question'], 'ideal': ideal, 'generated': generated_answers, 'P': P, 'R': R, 'F1': F1})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning falcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textbook_dataset = load_dataset(TRAIN_DATA_SOURCE, cache_dir=TRAIN_DATA_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_for_falcon(examples):\n",
    "    return falcon_tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = textbook_dataset.map(tokenize_for_falcon, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data = tokenized_text['train'].train_test_split(test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = falcon_tokenizer.model_max_length\n",
    "# block_size = 128\n",
    "block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_inputs(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = split_data.map(\n",
    "    group_inputs,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=8,\n",
    "    evaluation_strategy=\"epoch\", \n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=1,\n",
    "    output_dir=\"experiments\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=falcon_4bit,\n",
    "    train_dataset=grouped_data['train'],\n",
    "    eval_dataset=grouped_data['test'],\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForLanguageModeling(falcon_tokenizer, mlm=False)\n",
    ")\n",
    "falcon_4bit.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(FALCON_FINETUNE_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    falcon_finetune = falcon_4bit\n",
    "except:  \n",
    "    falcon_finetune = AutoModelForCausalLM.from_pretrained(\n",
    "        FALCON_FINETUNE_CHECKPOINT,\n",
    "        quantization_config=quantization_config, \n",
    "        cache_dir=FALCON_FINETUNE_CACHE_DIR,\n",
    "        device_map='auto'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = load_dataset(EVAL_DATA_SOURCE, cache_dir=EVAL_DATA_CACHE)\n",
    "eval_train_data = eval_data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "  return f\"\"\"\n",
    "<human>: {data_point[\"question\"]}\n",
    "<assistant>: {data_point[\"answer\"]}\n",
    "\"\"\".strip()\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "  full_prompt = generate_prompt(data_point)\n",
    "  tokenized_full_prompt = falcon_tokenizer(full_prompt, padding=True, truncation=True)\n",
    "  return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = load_dataset(EVAL_DATA_SOURCE, cache_dir=EVAL_DATA_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_train = eval_data[\"test\"].shuffle().map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_trainer = transformers.Trainer(\n",
    "    model=falcon_finetune,\n",
    "    train_dataset=eval_train,\n",
    "    eval_dataset=eval_train,\n",
    "    args=training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(falcon_tokenizer, mlm=False)\n",
    ")\n",
    "falcon_finetune.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_finetune.save_pretrained(FALCON_FINETUNE_CHECKPOINT)\n",
    "# falcon_finetune.push_to_hub(FALCON_FINETUNE_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "<human>: What is machine learning?\n",
    "\"\"\".strip()\n",
    "\n",
    "encoding = falcon_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "# with torch.no_grad():\n",
    "  outputs = falcon_finetune.generate(\n",
    "      input_ids = encoding.input_ids,\n",
    "      attention_mask = encoding.attention_mask,\n",
    "      generation_config = generation_config\n",
    "  )\n",
    "print(falcon_tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval = eval_data[\"train\"].map(generate_and_tokenize_prompt)\n",
    "test_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = falcon_inference(\"Explain how neural networks work\", falcon_finetune, falcon_tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_answers = run_evaluation(test_eval, falcon_inference, falcon_finetune, falcon_tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_finetune_results = evaluate_answers(test_eval, falcon_finetune_answers)\n",
    "falcon_finetune_results.to_csv(os.path.join(RESULTS_DIR, 'falcon_finetune_results.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = AutoModelForCausalLM.from_pretrained(GPT2_CHECKPOINT, cache_dir=GPT2_CACHE_DIR)\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(GPT2_CHECKPOINT, cache_dir=GPT2_CACHE_DIR, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_dict = {'eos_token': '<|endoftext|>', 'pad_token': '<|pad|>'}\n",
    "num_added_toks = gpt2_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "# gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "gpt2.resize_token_embeddings(len(gpt2_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2_inference(question, model, tokenizer, device):\n",
    "    prompt = f\"\"\"\n",
    "    Question: {question} \\n\n",
    "    Answer:\n",
    "    \"\"\".strip()\n",
    "    model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    generated_tokens = model.generate(**model_inputs, max_length=512)\n",
    "    response = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "    answer = response.split('Answer:')[-1]\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = gpt2_inference(\"Explain how neural networks work\", gpt2, gpt2_tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_answers = run_evaluation(test_eval, gpt2_inference, gpt2, gpt2_tokenizer, device)\n",
    "gpt2_results = evaluate_answers(test_eval, gpt2_answers)\n",
    "gpt2_results.to_csv(os.path.join(RESULTS_DIR, 'gpt2_results.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textbook_dataset = load_dataset(TRAIN_DATA_SOURCE, cache_dir=TRAIN_DATA_CACHE)\n",
    "textbook_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_for_gpt2(examples):\n",
    "    return falcon_tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenized_text = textbook_dataset.map(tokenize_for_gpt2, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = gpt2_tokenizer.model_max_length\n",
    "block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_inputs(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_text_data = gpt2_tokenized_text.map(\n",
    "    group_inputs,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_text_data = grouped_text_data['train'].train_test_split(test_size=0.05, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    GPT2_FINETUNE_CACHE_DIR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=8,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=gpt2,\n",
    "    args=training_args,\n",
    "    train_dataset=grouped_text_data['train'],\n",
    "    eval_dataset=grouped_text_data['test'],\n",
    "    data_collator=DataCollatorForLanguageModeling(gpt2_tokenizer, mlm=False),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_for_q_and_a(examples):\n",
    "    return {\"text\": f\"Question: {examples['question']} \\n Answer: {examples['answer']}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(GPT2_FINETUNE_CHECKPOINT)\n",
    "gpt2_tokenizer.save_pretrained(GPT2_FINETUNE_CHECKPOINT)\n",
    "gpt2_finetune = AutoModelForCausalLM.from_pretrained(GPT2_FINETUNE_CHECKPOINT, cache_dir=GPT2_FINETUNE_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_data = load_dataset(EVAL_DATA_SOURCE, cache_dir=EVAL_DATA_CACHE, split='test')\n",
    "                            .train_test_split(test_size=0.1, shuffle=True)\n",
    "                            .map(transform_for_q_and_a, batched=True, num_proc=4)\n",
    "                            .map(tokenize_for_gpt2, batched=True, num_proc=4, remove_columns=[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    GPT2_FINETUNE_CACHE_DIR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=8,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    predictin_loss_only=True,\n",
    ")\n",
    "alignment_trainer = Trainer(\n",
    "    model=gpt2_finetune,\n",
    "    args=training_args,\n",
    "    train_dataset=alignment_data['train'],\n",
    "    eval_dataset=alignment_data['test'],\n",
    "    data_collator=DataCollatorForLanguageModeling(gpt2_tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_trainer.save_model(GPT2_FINETUNE_CHECKPOINT)\n",
    "gpt2_tokenizer.save_pretrained(GPT2_FINETUNE_CHECKPOINT)\n",
    "gpt2_finetune = AutoModelForCausalLM.from_pretrained(GPT2_FINETUNE_CHECKPOINT, cache_dir=GPT2_FINETUNE_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_trainer.push_to_hub(GPT2_FINETUNE_CHECKPOINT)\n",
    "gpt2_tokenizer.push_to_hub(GPT2_FINETUNE_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading falcon for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_4bit_rag = AutoModelForCausalLM.from_pretrained(\n",
    "        FALCON_CHECKPOINT, \n",
    "        device_map='auto',\n",
    "        quantization_config=rag_quantization_config,\n",
    "        cache_dir=FALCON_CACHE_DIR,\n",
    "        )\n",
    "falcon_tokenizer_rag = AutoTokenizer.from_pretrained(FALCON_CHECKPOINT, cache_dir=FALCON_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=falcon_4bit_rag,\n",
    "        tokenizer=falcon_tokenizer_rag,\n",
    "        use_cache=True,\n",
    "        device_map=\"auto\",\n",
    "        max_length=2048,\n",
    "        do_sample=True,\n",
    "        top_k=3,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=falcon_tokenizer_rag.eos_token_id,\n",
    "        pad_token_id=falcon_tokenizer_rag.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_falcon = HuggingFacePipeline(pipeline=rag_pipeline)\n",
    "rag_embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=48,\n",
    "    length_function=len,\n",
    "    add_start_index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_books = text_books.strip()\n",
    "text_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.create_documents([text_books])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(texts, embeddings)\n",
    "vector_store.save_local('./content/vector_store')\n",
    "vector_store = FAISS.load_local('./content/vector_store', embeddings)\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = vector_store.similarity_search(\"What is a neural network?\", k=3)\n",
    "source = search[0].page_content\n",
    "source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = retriever.get_relevant_documents(\"What is a neural network?\", k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the context provided bellow:\n",
    "\n",
    "INSTRUCTIONS: \n",
    "Ensure that the answer is grammatically correct and relevant to the context.\n",
    "DO NOT include any HTML tags in your answer.\n",
    "DO NOT include any information that is not directly relevant to the question.\n",
    "Be succinct and to the point. Keep your answer to one or two sentences.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = load_dataset(dataset_name, cache_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = eval_dataset['train']\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "rand = randint(0, len(test_data))\n",
    "random_question = test_data[rand]['question']\n",
    "ideal_answer = test_data[rand]['answer']\n",
    "print(random_question)\n",
    "generated_answer = chain.invoke({\"question\": random_question})\n",
    "generated_answer = generated_answer.split('Answer: ')[-1].strip().split('\\n')[0]\n",
    "print(generated_answer)\n",
    "print(\"=\"*86)\n",
    "print(ideal_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(evaluation_dataset, inference_function):\n",
    "    results = []\n",
    "    for i in tqdm(range(len(evaluation_dataset))):\n",
    "        question = evaluation_dataset[i]['question']\n",
    "        answer = inference_function(question)\n",
    "        results.append(answer)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_inference(question, chain=chain):\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    answer = response.split('Answer: ')[-1].strip().split('\\n')[0]\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_answers = run_evaluation(test_data, rag_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answers(dataset, generated_answers, scorer=scorer):\n",
    "    ideal = dataset['answer']\n",
    "    P, R, F1 = scorer.score(ideal, generated_answers)\n",
    "    df = pd.DataFrame({'question': dataset['question'], 'ideal': ideal, 'generated': generated_answers, 'P': P, 'R': R, 'F1': F1})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_results = evaluate_answers(test_data, rag_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_results.to_csv('./rag_generated_answers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_P = rag_results['P'].mean()\n",
    "mean_R = rag_results['R'].mean()\n",
    "mean_F1 = rag_results['F1'].mean()\n",
    "print(f\"Mean Precision: {mean_P}\")\n",
    "print(f\"Mean Recall: {mean_R}\")\n",
    "print(f\"Mean F1: {mean_F1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'falcon-7b-instruct-RAG'\n",
    "F1 = rag_results['F1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"System level F1 score: {F1.mean():.3f}\")\n",
    "\n",
    "plt.hist(F1, bins=20)\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(f'{model_name} F1 Score Distribution')\n",
    "plt.suptitle(f'Mean F1 Score: {F1.mean():.3f}')\n",
    "plt.style.use('ggplot')\n",
    "plt.savefig(f'./{model_name}_f1_score_distribution.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
