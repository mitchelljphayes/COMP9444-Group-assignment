{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bert_score import score, BERTScorer\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline = pd.read_csv('falcon7b_baseline_eval.csv')\n",
    "df_qa = pd.read_csv('andrew_fine_tune_falcon_7b.csv')\n",
    "df_raw = pd.read_csv('falcon7b_finetuned_eval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_BL = df_baseline['questions'].tolist()\n",
    "ideal_answers_BL = df_baseline['answer'].tolist()\n",
    "gen_answers_BL = df_baseline['model_answer'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_BL, R_BL, F1_BL = scorer.score(gen_answers_BL, ideal_answers_BL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(F1_BL, bins=20)\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title('Falcon 7b Instruct Baseline F1 Score Distribution')\n",
    "plt.style.use('ggplot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision_BL = P_BL.mean()\n",
    "average_recall_BL = R_BL.mean()\n",
    "average_f1_BL = F1_BL.mean()\n",
    "\n",
    "scores = [average_precision_BL, average_recall_BL, average_f1_BL]\n",
    "labels = ['Precision', 'Recall', 'F1 Score']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "barlist = plt.bar(labels, scores, color=['blue', 'green', 'red'])\n",
    "\n",
    "for idx, bar in enumerate(barlist):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height, f'{scores[idx]:.3f}', ha='center', va='bottom')\n",
    "\n",
    "\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Falcon 7b Instruct Baseline Average Precision, Recall, and F1 Score')\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "questions_QA = df_qa['Question'].tolist()\n",
    "ideal_answers_QA = df_qa['Expected Answer'].tolist()\n",
    "gen_answers_QA = df_qa['Actual Answer'].tolist()\n",
    "\n",
    "# Score the QA dataset\n",
    "P_QA, R_QA, F1_QA = scorer.score(gen_answers_QA, ideal_answers_QA)\n",
    "\n",
    "# Plot the F1 score distribution for the QA data\n",
    "plt.hist(F1_QA, bins=20)\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title('Falcon 7b Instruct QA F1 Score Distribution')\n",
    "plt.style.use('ggplot')\n",
    "plt.show()\n",
    "\n",
    "# Calculate average scores for the QA data\n",
    "average_precision_QA = P_QA.mean()\n",
    "average_recall_QA = R_QA.mean()\n",
    "average_f1_QA = F1_QA.mean()\n",
    "\n",
    "# Define the scores and labels for plotting\n",
    "scores_QA = [average_precision_QA, average_recall_QA, average_f1_QA]\n",
    "labels_QA = ['Precision', 'Recall', 'F1 Score']\n",
    "\n",
    "# Create the bar chart for average scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "barlist = plt.bar(labels_QA, scores_QA, color=['blue', 'green', 'red'])\n",
    "\n",
    "# Add text labels to each bar\n",
    "for idx, bar in enumerate(barlist):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height, f'{scores_QA[idx]:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Set the labels and title for the bar chart\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Falcon 7b Instruct QA Average Precision, Recall, and F1 Score')\n",
    "plt.style.use('ggplot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_RAW = df_raw['Question'].tolist()\n",
    "ideal_answers_RAW = df_raw['Expected Answer'].tolist()\n",
    "gen_answers_RAW = df_raw['Actual Answer'].tolist()\n",
    "\n",
    "# Score the RAW dataset\n",
    "P_RAW, R_RAW, F1_RAW = scorer.score(gen_answers_RAW, ideal_answers_RAW)\n",
    "\n",
    "# Plot the F1 score distribution for the RAW data\n",
    "plt.hist(F1_RAW, bins=20)\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title('Falcon 7b Instruct RAW + QA F1 Score Distribution')\n",
    "plt.style.use('ggplot')\n",
    "plt.show()\n",
    "\n",
    "# Calculate average scores for the RAW data\n",
    "average_precision_RAW = P_RAW.mean()\n",
    "average_recall_RAW = R_RAW.mean()\n",
    "average_f1_RAW = F1_RAW.mean()\n",
    "\n",
    "# Define the scores and labels for plotting\n",
    "scores_RAW = [average_precision_RAW, average_recall_RAW, average_f1_RAW]\n",
    "labels_RAW = ['Precision', 'Recall', 'F1 Score']\n",
    "\n",
    "# Create the bar chart for average scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "barlist = plt.bar(labels_RAW, scores_RAW, color=['blue', 'green', 'red'])\n",
    "\n",
    "# Add text labels to each bar\n",
    "for idx, bar in enumerate(barlist):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height, f'{scores_RAW[idx]:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Set the labels and title for the bar chart\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Falcon 7b Instruct RAW + QA Average Precision, Recall, and F1 Score')\n",
    "plt.style.use('ggplot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Grouping data\n",
    "precisions = [average_precision_RAW, average_precision_QA, average_precision_BL]\n",
    "recalls = [average_recall_RAW, average_recall_QA, average_recall_BL]\n",
    "f1_scores = [average_f1_RAW, average_f1_QA, average_f1_BL]\n",
    "\n",
    "# Setting up the bar plot\n",
    "labels = ['RAW Text + QA', 'QA', 'Baseline']\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.25  # the width of the bars\n",
    "\n",
    "# Creating the figure and axes\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Creating bars\n",
    "rects1 = ax.bar(x - width, precisions, width, label='Precision', color='blue')\n",
    "rects2 = ax.bar(x, recalls, width, label='Recall', color='green')\n",
    "rects3 = ax.bar(x + width, f1_scores, width, label='F1', color='red')\n",
    "\n",
    "# Adding labels, title, and custom x-axis tick labels\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Comparison of Precision, Recall, and F1 Across RAW, QA, and BL')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "# Adding text labels to each bar\n",
    "def add_labels(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "add_labels(rects1)\n",
    "add_labels(rects2)\n",
    "add_labels(rects3)\n",
    "\n",
    "# Showing the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questions = df['Question'].tolist()\n",
    "#ideal_answers = df['Expected Answer'].tolist()\n",
    "#gen_answers = df['Actual Answer'].tolist()\n",
    "\n",
    "questions = df['questions'].tolist()\n",
    "ideal_answers = df['answer'].tolist()\n",
    "gen_answers = df['model_answer'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, R, F1 = scorer.score([gen_answers[0]], [ideal_answers[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, R, F1 = scorer.score([\"an eigenvector for a matrix is a column vector of the form (n - 1, a n ) such that (n - 1) n is the size of the matrix. The column vector corresponding to the eigenvector is the `i`-th element of the matrix, where `i` is the index of the row and column.\"], [\"An eigenvector of a matrix is a nonzero vector that changes at most by a scalar factor when that matrix is applied to it.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Precision: {P[0]:.3f}, Recall: {R[0]:.3f}, F1 Score: {F1[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n",
    "\n",
    "\n",
    "P, R, F1 = scorer.score(gen_answers, ideal_answers)\n",
    "\n",
    "print(f\"System level F1 score: {F1.mean():.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"System level F1 score: {F1.mean():.3f}\")\n",
    "\n",
    "\n",
    "plt.hist(F1, bins=20)\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title('Falcon 7b Instruct Baseline F1 Score Distribution')\n",
    "plt.style.use('ggplot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "P = np.array(P)\n",
    "R = np.array(R)\n",
    "F1 = np.array(F1)\n",
    "\n",
    "\n",
    "num_points = len(P)\n",
    "\n",
    "\n",
    "idx = np.arange(num_points)\n",
    "\n",
    "\n",
    "bar_width = 0.2\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "\n",
    "plt.bar(idx, P, bar_width, label='Precision')\n",
    "plt.bar(idx + bar_width, R, bar_width, label='Recall')\n",
    "plt.bar(idx + 2 * bar_width, F1, bar_width, label='F1 Score')\n",
    "\n",
    "plt.xlabel('Data Points')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Precision, Recall, and F1 Score for Each Data Point')\n",
    "plt.xticks(idx + bar_width, idx)\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "average_precision = P.mean()\n",
    "average_recall = R.mean()\n",
    "average_f1 = F1.mean()\n",
    "\n",
    "scores = [average_precision, average_recall, average_f1]\n",
    "labels = ['Precision', 'Recall', 'F1 Score']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "barlist = plt.bar(labels, scores, color=['blue', 'green', 'red'])\n",
    "\n",
    "for idx, bar in enumerate(barlist):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height, f'{scores[idx]:.3f}', ha='center', va='bottom')\n",
    "\n",
    "\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Falcon 7b Instruct Baseline Average Precision, Recall, and F1 Score')\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
