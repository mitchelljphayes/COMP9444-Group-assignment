{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bert_score import score, BERTScorer\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../eval/distilgpt2_eval.csv')\n",
    "# model_name = 'distilgpt2'\n",
    "# df = pd.read_csv('../eval/distilgpt2-finetuned-textbook_dataset_eval.csv')\n",
    "# model_name = 'distilgpt2-finetuned'\n",
    "# df = pd.read_csv('../eval/falcon7b_rag_eval.csv')\n",
    "# model_name = 'falcon7b_rag'\n",
    "df = pd.read_csv('../eval/falcon7b_finetuned_eval.csv')\n",
    "model_name = 'falcon7b_finetuned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questions = df['Question'].tolist()\n",
    "ideal_answers = df['Expected Answer'].tolist()\n",
    "gen_answers = df['Actual Answer'].tolist()\n",
    "\n",
    "# questions = df['question'].tolist()\n",
    "# ideal_answers = df['Expected'].tolist()\n",
    "# gen_answers = df['Generated'].tolist()\n",
    "\n",
    "ideal_answers = [str(x) for x in ideal_answers]\n",
    "gen_answers = [str(x) for x in gen_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)\n",
    "\n",
    "\n",
    "P, R, F1 = scorer.score(gen_answers, ideal_answers)\n",
    "\n",
    "# print(f\"System level F1 score: {F1.mean():.3f}\")\n",
    "\n",
    "\n",
    "# plt.hist(F1, bins=20)\n",
    "# plt.xlabel(\"Score\")\n",
    "# plt.ylabel(\"Counts\")\n",
    "# plt.title('Precision, Recall, and F1 Score for Each Data Point')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BERTScore_f1'] = F1\n",
    "df['BERTScore_precision'] = P\n",
    "df['BERTScore_recall'] = R\n",
    "\n",
    "df.to_csv(f'../eval/{model_name}_eval.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"System level F1 score: {F1.mean():.3f}\")\n",
    "\n",
    "plt.hist(F1, bins=20)\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(f'{model_name} F1 Score Distribution')\n",
    "# plt.suptitle(f'Mean F1 Score: {F1.mean():.3f}')\n",
    "plt.style.use('ggplot')\n",
    "plt.savefig(f'../eval/{model_name}_f1_score_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "P = np.array(P)\n",
    "R = np.array(R)\n",
    "F1 = np.array(F1)\n",
    "\n",
    "\n",
    "num_points = len(P)\n",
    "\n",
    "\n",
    "idx = np.arange(num_points)\n",
    "\n",
    "\n",
    "bar_width = 0.2\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "\n",
    "plt.bar(idx, P, bar_width, label='Precision')\n",
    "plt.bar(idx + bar_width, R, bar_width, label='Recall')\n",
    "plt.bar(idx + 2 * bar_width, F1, bar_width, label='F1 Score')\n",
    "\n",
    "plt.xlabel('Data Points')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Precision, Recall, and F1 Score for Each Data Point')\n",
    "plt.xticks(idx + bar_width, idx)\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.style.use('ggplot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "average_precision = P.mean()\n",
    "average_recall = R.mean()\n",
    "average_f1 = F1.mean()\n",
    "\n",
    "scores = [average_precision, average_recall, average_f1]\n",
    "labels = ['Precision', 'Recall', 'F1 Score']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "barlist = plt.bar(labels, scores, color=['blue', 'green', 'red'])\n",
    "for idx, bar in enumerate(barlist):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height, f'{scores[idx]:.3f}', ha='center', va='bottom')\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Scores')\n",
    "plt.title(f'{model_name} Average Precision, Recall, and F1 Score')\n",
    "plt.style.use('ggplot')\n",
    "plt.savefig(f'../eval/{model_name}_average_scores.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
