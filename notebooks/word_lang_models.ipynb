{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Tokenizer' from 'tiktoken' (/Users/mjp/.local/share/virtualenvs/group-assignment-f09OljWU/lib/python3.11/site-packages/tiktoken/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/notebooks/word_lang_models.ipynb Cell 1\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/notebooks/word_lang_models.ipynb#X45sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtime\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/notebooks/word_lang_models.ipynb#X45sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtiktoken\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/notebooks/word_lang_models.ipynb#X45sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtiktoken\u001b[39;00m \u001b[39mimport\u001b[39;00m Tokenizer\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/notebooks/word_lang_models.ipynb#X45sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/notebooks/word_lang_models.ipynb#X45sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mio\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39mopen\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Tokenizer' from 'tiktoken' (/Users/mjp/.local/share/virtualenvs/group-assignment-f09OljWU/lib/python3.11/site-packages/tiktoken/__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import tiktoken\n",
    "from datasets import load_dataset\n",
    "from io import open\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "nhead = 16\n",
    "ninp = 256\n",
    "em_size = 256\n",
    "nhid = 256\n",
    "nlayers = 2\n",
    "dropout = 0.2\n",
    "bptt = 35\n",
    "batch_size = 20\n",
    "learning_rate = 1e-3\n",
    "clip = 0.25\n",
    "epochs = 3\n",
    "log_interval = 500\n",
    "models_dir = '../models/'\n",
    "data_dir = '../data/wikitext'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Taken from https://github.com/pytorch/examples/blob/main/word_language_model/data.py on 01/11/2023\n",
    "# class Dictionary(object):\n",
    "#     def __init__(self):\n",
    "#         self.word2idx = {}\n",
    "#         self.idx2word = []\n",
    "#         self.counter = 0\n",
    "    \n",
    "#     def add_word(self, word):\n",
    "#         if word not in self.idx2word:\n",
    "#             self.idx2word.append(word)\n",
    "#             self.word2idx[word] = self.counter\n",
    "#             self.counter += 1\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9906, 1917, 0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "hello_world_encoded = encoder.encode(\"Hello world!\")\n",
    "assert encoder.decode(hello_world_encoded) == \"Hello world!\"\n",
    "\n",
    "hello_world_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<|endofprompt|>',\n",
       " '<|endoftext|>',\n",
       " '<|fim_middle|>',\n",
       " '<|fim_prefix|>',\n",
       " '<|fim_suffix|>'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.special_tokens_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100277"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenise_prompt(input, corpus):\n",
    "#     words = input.split() + ['<eos>']\n",
    "#     ids = torch.LongTensor(len(words))\n",
    "#     for i, word in enumerate(words):\n",
    "#         ids[i] = corpus.dictionary.word2idx[word]\n",
    "#     return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Taken from https://github.com/pytorch/examples/blob/main/word_language_model/data.py on 01/11/2023\n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        # self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        assert os.path.exists(path)\n",
    "        text = ''\n",
    "        chunk_size = 1024\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for chunk in f.read(chunk_size):\n",
    "                text += chunk\n",
    "\n",
    "            # for line in f:\n",
    "            #     words = line.split()\n",
    "            #     for word in words:\n",
    "            #         text += word\n",
    "            #     text += \"\\n\"\n",
    "        return encoder.encode(text)\n",
    "            # return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir('data_dir')\n",
    "    data = load_dataset('wikitext', 'wikitext-2-v1')\n",
    "    data.save_to_disk(os.path.join(data_dir, 'wikitext-2'))\n",
    "data = load_dataset('wikitext', 'wikitext-2-v1', data_dir=data_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 36718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting from HF dataset to plain text files can probably improve this\n",
    "train_data = '\\n'.join(data['train']['text'])\n",
    "valid_data = '\\n'.join(data['validation']['text'])\n",
    "test_data = '\\n'.join(data['test']['text'])\n",
    "with open(os.path.join(data_dir, 'train.txt'), 'w') as f:\n",
    "    f.write(train_data)\n",
    "with open(os.path.join(data_dir, 'valid.txt'), 'w') as f:\n",
    "    f.write(valid_data)\n",
    "with open(os.path.join(data_dir, 'test.txt'), 'w') as f:\n",
    "    f.write(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    nbatch = data.size(0) // bsz\n",
    "    data = data.narrow(0, 0, nbatch*bsz)\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_batch_size = 10\n",
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.lstm = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "        self.init_weights()\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.ntokens = ntoken\n",
    "        self.model_type = 'LSTM' \n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.lstm(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output)\n",
    "        decoded = decoded.view(-1, self.ntokens)\n",
    "        return F.log_softmax(decoded, dim=1), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "                weight.new_zeros(self.nlayers, bsz, self.nhid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoisitionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PoisitionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2)*(-math.log(10000.0)/d_model))\n",
    "        pe[:, 0::2] = torch.sin(position*div_term)\n",
    "        pe[:, 1::2] = torch.cos(position*div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Transformer):\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__(d_model=ninp, nhead=nhead, dim_feedforward=nhid, num_decoder_layers=nlayers)\n",
    "        # self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PoisitionalEncoding(ninp, dropout)\n",
    "\n",
    "        self.input_emb = nn.Embedding(ntoken, ninp)\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "        self.ninp = ninp\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.ntokens = ntoken\n",
    "        self.model_type = 'Transformer'\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz) == 1).transpose(0, 1))\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.input_emb.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def forward(self, src, has_mask=True):\n",
    "        if has_mask:\n",
    "            device = src.device\n",
    "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "                self.src_mask = mask\n",
    "        else:\n",
    "            self.src_mask = None\n",
    "        \n",
    "        src = self.input_emb(src)*math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.encoder(src, mask=self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return F.log_softmax(output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mjp/.local/share/virtualenvs/group-assignment-f09OljWU/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "transformer_model = TransformerModel(ntoken=ntokens, ninp=ninp, nhead=nhead, nhid=nhid, nlayers=nlayers, dropout=dropout).to(device)\n",
    "lstm_model = LSTMModel(ntoken=ntokens, ninp=ninp, nhid=nhid, nlayers=nlayers,dropout=dropout).to(device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source)-1-i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_type, model, data_source):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    if model_type != 'Transformer':\n",
    "        hidden = model.init_hidden(eval_batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0)-1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            if model_type == 'Transformer':\n",
    "                output = model(data)\n",
    "                output = output.view(-1, ntokens)\n",
    "            else:\n",
    "                output, hidden = model(data, hidden)\n",
    "                hidden = repackage_hidden(hidden)\n",
    "            total_loss += len(data)*criterion(output, targets).item()\n",
    "    return total_loss/(len(data_source)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_type, model, epoch, lr):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    if model_type != 'Transformer':\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0)-1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        model.zero_grad()\n",
    "        if model_type == 'Transformer':\n",
    "            output = model(data)\n",
    "            output = output.view(-1, ntokens)\n",
    "        else:\n",
    "            output, hidden = model(data, hidden)\n",
    "            hidden = repackage_hidden(hidden)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(p.grad, alpha=-lr)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch%log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss/log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                        epoch, batch, len(train_data)//bptt, elapsed*1000/log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_onnx(path, model, batch_size, seq_len):\n",
    "    model.eval()\n",
    "    x = torch.rand(seq_len, batch_size).to(device)\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    torch.onnx.export(model, (x, hidden), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model, epochs, learning_rate):\n",
    "    lr = learning_rate\n",
    "    best_val_loss = None\n",
    "    model_type = model.model_type\n",
    "    model_path = os.path.join(models_dir, (model_type + '.pt'))\n",
    "    for epoch in range(1, epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model_type, model, epoch, lr)\n",
    "        val_loss = evaluate(model_type, model, val_data)\n",
    "        print('-'*89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time()-epoch_start_time), val_loss, math.exp(val_loss)))\n",
    "        print('-'*89)\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(model_path, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            lr /= 4.0\n",
    "    # with open(model_path, 'rb') as f:\n",
    "    #     model.torch.load(f)\n",
    "    #     if model_type == 'LSTM':\n",
    "    #         model.flatten_parameters()\n",
    "\n",
    "    test_loss = evaluate(model_type, model, test_data)\n",
    "    print('='*89)\n",
    "    print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(test_loss, math.exp(test_loss)))\n",
    "    print('='*89)\n",
    "    # export_onnx(os.path.join(onnx_export_dir, model_type), model, batch_size=1, seq_len=bptt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 3017 batches | ms/batch 109.69 | loss  7.92 | ppl  2756.35\n",
      "| epoch   1 |  1000/ 3017 batches | ms/batch 107.54 | loss  7.95 | ppl  2833.62\n",
      "| epoch   1 |  1500/ 3017 batches | ms/batch 120.08 | loss  7.91 | ppl  2717.10\n",
      "| epoch   1 |  2000/ 3017 batches | ms/batch 147.74 | loss  7.91 | ppl  2734.95\n",
      "| epoch   1 |  2500/ 3017 batches | ms/batch 147.48 | loss  7.88 | ppl  2644.44\n",
      "| epoch   1 |  3000/ 3017 batches | ms/batch 159.71 | loss  7.84 | ppl  2548.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 418.03s | valid loss  7.62 | valid ppl  2042.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   500/ 3017 batches | ms/batch 160.48 | loss  7.84 | ppl  2537.02\n",
      "| epoch   2 |  1000/ 3017 batches | ms/batch 159.24 | loss  7.87 | ppl  2607.71\n",
      "| epoch   2 |  1500/ 3017 batches | ms/batch 166.97 | loss  7.83 | ppl  2510.58\n",
      "| epoch   2 |  2000/ 3017 batches | ms/batch 173.04 | loss  7.84 | ppl  2532.06\n",
      "| epoch   2 |  2500/ 3017 batches | ms/batch 171.03 | loss  7.80 | ppl  2447.72\n",
      "| epoch   2 |  3000/ 3017 batches | ms/batch 168.10 | loss  7.77 | ppl  2358.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 522.65s | valid loss  7.54 | valid ppl  1882.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  7.48 | test ppl  1769.25\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "run_training(transformer_model, epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training(lstm_model, epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  83,  427,   16, 1127,   26,    0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenise_prompt('The meaning of life is', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if transformer_model is None:\n",
    "    transformer_model = torch.load('../models/Transformer.pt', map_location=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, prompt, output_file, corpus=corpus, temp=0.4, device=device):\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    if model.model_type != 'Transformer':\n",
    "        hidden = model.init_hidden(1)\n",
    "        # lstm_model.flatten_parameters()\n",
    "    model.to(device)\n",
    "    # input = tokenise_prompt(prompt, corpus).to(device)\n",
    "    input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
    "    with open(output_file, 'w') as f:\n",
    "        with torch.no_grad():\n",
    "            for i in range(1000):\n",
    "                if model.model_type == 'Transformer':\n",
    "                    output = model(input, has_mask=False)\n",
    "                    word_weights = output[-1].squeeze().div(temp).exp().cpu()\n",
    "                    word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "                    word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
    "                    input = torch.cat([input, word_tensor], 0)\n",
    "                else:\n",
    "                    output, hidden = model(input, hidden)\n",
    "                    word_weights = output.squeeze().div(temp).exp().cpu()\n",
    "                    word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "                    input.fill_(word_idx)\n",
    "                word = corpus.dictionary.idx2word[word_idx]\n",
    "                f.write(word + ('\\n' if i%20 == 19 else ' '))\n",
    "                if i % log_interval == 0:\n",
    "                    print('| Generated {}/{} words'.format(i, 1000))\n",
    "            print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'dictionary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/notebooks/word_lang_modesl.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/notebooks/word_lang_modesl.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m generate_text(transformer_model, \u001b[39m'\u001b[39;49m\u001b[39mThe meaning of life is\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39moutput/transformer_generated.txt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/notebooks/word_lang_modesl.ipynb Cell 28\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/notebooks/word_lang_modesl.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_text\u001b[39m(model, prompt, corpus\u001b[39m=\u001b[39mcorpus, temp\u001b[39m=\u001b[39m\u001b[39m0.4\u001b[39m, device\u001b[39m=\u001b[39mdevice):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/notebooks/word_lang_modesl.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     ntokens \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(corpus\u001b[39m.\u001b[39;49mdictionary)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/notebooks/word_lang_modesl.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39mmodel_type \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mTransformer\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/notebooks/word_lang_modesl.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         hidden \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39minit_hidden(\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'dictionary'"
     ]
    }
   ],
   "source": [
    "generate_text(transformer_model, 'The meaning of life is', 'output/transformer_generated.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_text('LSTM.pt', 'LSTM', corpus, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "group-assignment-f09OljWU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
