{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import tiktoken\n",
    "from datasets import load_dataset\n",
    "from io import open\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "nhead = 4\n",
    "ninp = 128\n",
    "em_size = 1500\n",
    "nhid = 1500\n",
    "nlayers = 4\n",
    "dropout = 0.4\n",
    "bptt = 150\n",
    "batch_size = 32\n",
    "eval_batch_size = 16\n",
    "learning_rate = 4\n",
    "clip = 0.25\n",
    "epochs = 40\n",
    "log_interval = 100\n",
    "models_dir = '../models/'\n",
    "data_dir = '../data/wikitext'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Taken from https://github.com/pytorch/examples/blob/main/word_language_model/data.py on 01/11/2023\n",
    "# class Dictionary(object):\n",
    "#     def __init__(self):\n",
    "#         self.word2idx = {}\n",
    "#         self.idx2word = []\n",
    "#         self.counter = 0\n",
    "    \n",
    "#     def add_word(self, word):\n",
    "#         if word not in self.idx2word:\n",
    "#             self.idx2word.append(word)\n",
    "#             self.word2idx[word] = self.counter\n",
    "#             self.counter += 1\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9906, 1917, 0]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "hello_world_encoded = encoder.encode(\"Hello world!\")\n",
    "assert encoder.decode(hello_world_encoded) == \"Hello world!\"\n",
    "\n",
    "hello_world_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<|endofprompt|>',\n",
       " '<|endoftext|>',\n",
       " '<|fim_middle|>',\n",
       " '<|fim_prefix|>',\n",
       " '<|fim_suffix|>'}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.special_tokens_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100277"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenise_prompt(input, corpus):\n",
    "#     words = input.split() + ['<eos>']\n",
    "#     ids = torch.LongTensor(len(words))\n",
    "#     for i, word in enumerate(words):\n",
    "#         ids[i] = corpus.dictionary.word2idx[word]\n",
    "#     return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Taken from https://github.com/pytorch/examples/blob/main/word_language_model/data.py on 01/11/2023\n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        # self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        assert os.path.exists(path)\n",
    "        text = ''\n",
    "        chunk_size = 1024\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for chunk in f.read(chunk_size):\n",
    "                text += chunk\n",
    "\n",
    "            # for line in f:\n",
    "            #     words = line.split()\n",
    "            #     for word in words:\n",
    "            #         text += word\n",
    "            #     text += \"\\n\"\n",
    "        return encoder.encode(text)\n",
    "            # return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir('data_dir')\n",
    "    data = load_dataset('wikitext', 'wikitext-2-v1')\n",
    "    data.save_to_disk(os.path.join(data_dir, 'wikitext-2'))\n",
    "data = load_dataset('wikitext', 'wikitext-2-v1', data_dir=data_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 36718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting from HF dataset to plain text files can probably improve this\n",
    "train_data = '\\n'.join(data['train']['text'])\n",
    "valid_data = '\\n'.join(data['validation']['text'])\n",
    "test_data = '\\n'.join(data['test']['text'])\n",
    "with open(os.path.join(data_dir, 'train.txt'), 'w') as f:\n",
    "    f.write(train_data)\n",
    "with open(os.path.join(data_dir, 'valid.txt'), 'w') as f:\n",
    "    f.write(valid_data)\n",
    "with open(os.path.join(data_dir, 'test.txt'), 'w') as f:\n",
    "    f.write(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    data = torch.tensor(data)\n",
    "    nbatch = data.size(0) // bsz\n",
    "    data = data.narrow(0, 0, nbatch*bsz)\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.lstm = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "        self.init_weights()\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.ntokens = ntoken\n",
    "        self.model_type = 'LSTM' \n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.lstm(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output)\n",
    "        decoded = decoded.view(-1, self.ntokens)\n",
    "        return F.log_softmax(decoded, dim=1), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "                weight.new_zeros(self.nlayers, bsz, self.nhid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoisitionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PoisitionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2)*(-math.log(10000.0)/d_model))\n",
    "        pe[:, 0::2] = torch.sin(position*div_term)\n",
    "        pe[:, 1::2] = torch.cos(position*div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Transformer):\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__(d_model=ninp, nhead=nhead, dim_feedforward=nhid, num_decoder_layers=nlayers)\n",
    "        # self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PoisitionalEncoding(ninp, dropout)\n",
    "\n",
    "        self.input_emb = nn.Embedding(ntoken, ninp)\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "        self.ninp = ninp\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.ntokens = ntoken\n",
    "        self.model_type = 'Transformer'\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz) == 1).transpose(0, 1))\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.input_emb.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def forward(self, src, has_mask=True):\n",
    "        if has_mask:\n",
    "            device = src.device\n",
    "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "                self.src_mask = mask\n",
    "        else:\n",
    "            self.src_mask = None\n",
    "        \n",
    "        src = self.input_emb(src)*math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.encoder(src, mask=self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return F.log_softmax(output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = encoder.n_vocab\n",
    "transformer_model = TransformerModel(ntoken=ntokens, ninp=ninp, nhead=nhead, nhid=nhid, nlayers=nlayers, dropout=dropout).to(device)\n",
    "lstm_model = LSTMModel(ntoken=ntokens, ninp=ninp, nhid=nhid, nlayers=nlayers,dropout=dropout).to(device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source)-1-i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_type, model, data_source, ntokens):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    if model_type != 'Transformer':\n",
    "        hidden = model.init_hidden(eval_batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0)-1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            if model_type == 'Transformer':\n",
    "                output = model(data)\n",
    "                output = output.view(-1, ntokens)\n",
    "            else:\n",
    "                output, hidden = model(data, hidden)\n",
    "                hidden = repackage_hidden(hidden)\n",
    "            total_loss += len(data)*criterion(output, targets).item()\n",
    "    return total_loss/(len(data_source)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_type, model, epoch, lr, ntokens):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    if model_type != 'Transformer':\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0)-1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        model.zero_grad()\n",
    "        if model_type == 'Transformer':\n",
    "            output = model(data)\n",
    "            output = output.view(-1, ntokens)\n",
    "        else:\n",
    "            output, hidden = model(data, hidden)\n",
    "            hidden = repackage_hidden(hidden)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(p.grad, alpha=-lr)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch%log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss/log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                        epoch, batch, len(train_data)//bptt, elapsed*1000/log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_onnx(path, model, batch_size, seq_len):\n",
    "    model.eval()\n",
    "    x = torch.rand(seq_len, batch_size).to(device)\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    torch.onnx.export(model, (x, hidden), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model, epochs, learning_rate, ntokens):\n",
    "    lr = learning_rate\n",
    "    best_val_loss = None\n",
    "    model_type = model.model_type\n",
    "    model_path = os.path.join(models_dir, (model_type + '.pt'))\n",
    "    for epoch in range(1, epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model_type, model, epoch, lr, ntokens)\n",
    "        val_loss = evaluate(model_type, model, val_data, ntokens)\n",
    "        print('-'*89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time()-epoch_start_time), val_loss, math.exp(val_loss)))\n",
    "        print('-'*89)\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(model_path, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            lr /= 4.0\n",
    "    # with open(model_path, 'rb') as f:\n",
    "    #     model.torch.load(f)\n",
    "    #     if model_type == 'LSTM':\n",
    "    #         model.flatten_parameters()\n",
    "\n",
    "    test_loss = evaluate(model_type, model, test_data, ntokens)\n",
    "    print('='*89)\n",
    "    print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(test_loss, math.exp(test_loss)))\n",
    "    print('='*89)\n",
    "    # export_onnx(os.path.join(onnx_export_dir, model_type), model, batch_size=1, seq_len=bptt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time:  1.33s | valid loss 11.35 | valid ppl 85320.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.13s | valid loss 10.71 | valid ppl 44877.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.11s | valid loss 13.41 | valid ppl 667764.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time:  0.11s | valid loss 10.65 | valid ppl 42099.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time:  0.11s | valid loss 10.10 | valid ppl 24461.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time:  0.11s | valid loss  9.94 | valid ppl 20751.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time:  0.11s | valid loss  9.69 | valid ppl 16189.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time:  0.11s | valid loss  9.61 | valid ppl 14950.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time:  0.11s | valid loss  9.45 | valid ppl 12661.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time:  0.11s | valid loss  9.53 | valid ppl 13823.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time:  0.11s | valid loss  9.36 | valid ppl 11610.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time:  0.18s | valid loss  9.34 | valid ppl 11367.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time:  0.16s | valid loss  9.31 | valid ppl 11055.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time:  0.11s | valid loss  9.31 | valid ppl 11034.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time:  0.11s | valid loss  9.30 | valid ppl 10918.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time:  0.11s | valid loss  9.29 | valid ppl 10858.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time:  0.16s | valid loss  9.29 | valid ppl 10819.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time:  0.13s | valid loss  9.28 | valid ppl 10746.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time:  0.11s | valid loss  9.28 | valid ppl 10758.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time:  0.11s | valid loss  9.28 | valid ppl 10745.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time:  0.11s | valid loss  9.28 | valid ppl 10722.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time:  0.16s | valid loss  9.28 | valid ppl 10717.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time:  0.14s | valid loss  9.28 | valid ppl 10717.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time:  0.11s | valid loss  9.28 | valid ppl 10716.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time:  0.14s | valid loss  9.28 | valid ppl 10715.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time:  0.11s | valid loss  9.28 | valid ppl 10711.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time:  0.11s | valid loss  9.28 | valid ppl 10707.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time:  0.13s | valid loss  9.28 | valid ppl 10708.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time:  0.12s | valid loss  9.28 | valid ppl 10708.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time:  0.11s | valid loss  9.28 | valid ppl 10708.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time:  0.11s | valid loss  9.28 | valid ppl 10708.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time:  0.11s | valid loss  9.28 | valid ppl 10708.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time:  0.11s | valid loss  9.28 | valid ppl 10708.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time:  0.11s | valid loss  9.28 | valid ppl 10708.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time:  0.11s | valid loss  9.28 | valid ppl 10708.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time:  0.11s | valid loss  9.28 | valid ppl 10708.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time:  0.11s | valid loss  9.28 | valid ppl 10708.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time:  0.11s | valid loss  9.28 | valid ppl 10708.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time:  0.11s | valid loss  9.28 | valid ppl 10708.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  8.61 | test ppl  5497.11\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "run_training(transformer_model, epochs, learning_rate, encoder.n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time:  2.46s | valid loss 11.51 | valid ppl 99861.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.56s | valid loss 11.51 | valid ppl 99222.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time:  0.57s | valid loss 11.50 | valid ppl 98565.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time:  0.57s | valid loss 11.49 | valid ppl 97942.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time:  0.56s | valid loss 11.49 | valid ppl 97322.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time:  0.55s | valid loss 11.48 | valid ppl 96715.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time:  0.55s | valid loss 11.47 | valid ppl 96160.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time:  0.57s | valid loss 11.47 | valid ppl 95636.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time:  0.55s | valid loss 11.46 | valid ppl 95149.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time:  0.56s | valid loss 11.46 | valid ppl 94655.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time:  0.56s | valid loss 11.45 | valid ppl 94180.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time:  0.56s | valid loss 11.45 | valid ppl 93796.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time:  0.55s | valid loss 11.44 | valid ppl 93383.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time:  0.56s | valid loss 11.44 | valid ppl 92985.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time:  0.55s | valid loss 11.44 | valid ppl 92581.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time:  0.55s | valid loss 11.43 | valid ppl 92205.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time:  0.57s | valid loss 11.43 | valid ppl 91852.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time:  0.56s | valid loss 11.42 | valid ppl 91518.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time:  0.57s | valid loss 11.42 | valid ppl 91210.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time:  0.57s | valid loss 11.42 | valid ppl 90936.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time:  0.55s | valid loss 11.41 | valid ppl 90632.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time:  0.54s | valid loss 11.41 | valid ppl 90349.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time:  0.55s | valid loss 11.41 | valid ppl 90073.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time:  0.56s | valid loss 11.41 | valid ppl 89775.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time:  0.56s | valid loss 11.40 | valid ppl 89535.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time:  0.56s | valid loss 11.40 | valid ppl 89262.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time:  0.57s | valid loss 11.40 | valid ppl 88988.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time:  0.56s | valid loss 11.39 | valid ppl 88721.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time:  0.55s | valid loss 11.39 | valid ppl 88465.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time:  0.54s | valid loss 11.39 | valid ppl 88293.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time:  0.55s | valid loss 11.39 | valid ppl 88041.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time:  0.56s | valid loss 11.38 | valid ppl 87813.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time:  0.55s | valid loss 11.38 | valid ppl 87569.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time:  0.56s | valid loss 11.38 | valid ppl 87409.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time:  0.55s | valid loss 11.38 | valid ppl 87184.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time:  0.56s | valid loss 11.37 | valid ppl 86941.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time:  0.57s | valid loss 11.37 | valid ppl 86761.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time:  0.55s | valid loss 11.37 | valid ppl 86571.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time:  0.56s | valid loss 11.37 | valid ppl 86401.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss 11.34 | test ppl 84002.76\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "run_training(lstm_model, epochs, learning_rate, encoder.n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenise_prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/notebooks/word_lang_models.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/notebooks/word_lang_models.ipynb#Y111sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tokenise_prompt(\u001b[39m'\u001b[39m\u001b[39mThe meaning of life is\u001b[39m\u001b[39m'\u001b[39m, corpus)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenise_prompt' is not defined"
     ]
    }
   ],
   "source": [
    "# tokenise_prompt('The meaning of life is', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if transformer_model is None:\n",
    "    transformer_model = torch.load('../models/Transformer.pt', map_location=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, prompt, output_file, corpus=corpus, temp=0.4, device=device, ntokens=encoder.n_vocab):\n",
    "    if model.model_type != 'Transformer':\n",
    "        hidden = model.init_hidden(1)\n",
    "        # lstm_model.flatten_parameters()\n",
    "    model.to(device)\n",
    "    # input = tokenise_prompt(prompt, corpus).to(device)\n",
    "    input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
    "    with open(output_file, 'w') as f:\n",
    "        with torch.no_grad():\n",
    "            for i in range(1000):\n",
    "                if model.model_type == 'Transformer':\n",
    "                    output = model(input, has_mask=False)\n",
    "                    word_weights = output[-1].squeeze().div(temp).exp().cpu()\n",
    "                    word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "                    word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
    "                    input = torch.cat([input, word_tensor], 0)\n",
    "                else:\n",
    "                    output, hidden = model(input, hidden)\n",
    "                    word_weights = output.squeeze().div(temp).exp().cpu()\n",
    "                    word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "                    input.fill_(word_idx)\n",
    "                word = corpus.dictionary.idx2word[word_idx]\n",
    "                f.write(word + ('\\n' if i%20 == 19 else ' '))\n",
    "                if i % log_interval == 0:\n",
    "                    print('| Generated {}/{} words'.format(i, 1000))\n",
    "            print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200\n",
      "| Generated 0/1000 words\n",
      "330\n",
      "1174\n",
      "4298\n",
      "66416\n",
      "1174\n",
      "279\n",
      "279\n",
      "18\n",
      "86262\n",
      "330\n",
      "1847\n",
      "18\n",
      "279\n",
      "279\n",
      "662\n",
      "279\n",
      "315\n",
      "1174\n",
      "323\n",
      "18\n",
      "279\n",
      "1847\n",
      "279\n",
      "86262\n",
      "86262\n",
      "279\n",
      "279\n",
      "4298\n",
      "366\n",
      "88\n",
      "86262\n",
      "66416\n",
      "279\n",
      "279\n",
      "279\n",
      "279\n",
      "220\n",
      "279\n",
      "279\n",
      "279\n",
      "279\n",
      "304\n",
      "279\n",
      "279\n",
      "279\n",
      "279\n",
      "1174\n",
      "279\n",
      "86262\n",
      "279\n",
      "279\n",
      "439\n",
      "66416\n",
      "311\n",
      "1174\n",
      "279\n",
      "551\n",
      "1174\n",
      "279\n",
      "86262\n",
      "279\n",
      "1174\n",
      "279\n",
      "304\n",
      "88\n",
      "4298\n",
      "315\n",
      "66416\n",
      "330\n",
      "5089\n",
      "279\n",
      "279\n",
      "86262\n",
      "279\n",
      "279\n",
      "279\n",
      "86262\n",
      "279\n",
      "66416\n",
      "86262\n",
      "1174\n",
      "279\n",
      "1174\n",
      "315\n",
      "279\n",
      "330\n",
      "279\n",
      "86262\n",
      "1174\n",
      "264\n",
      "662\n",
      "1847\n",
      "279\n",
      "279\n",
      "315\n",
      "662\n",
      "304\n",
      "279\n",
      "88\n",
      "279\n",
      "662\n",
      "86262\n",
      "279\n",
      "279\n",
      "662\n",
      "662\n",
      "304\n",
      "86262\n",
      "279\n",
      "279\n",
      "279\n",
      "1174\n",
      "279\n",
      "279\n",
      "279\n",
      "279\n",
      "86262\n",
      "86262\n",
      "1174\n",
      "4298\n",
      "1174\n",
      "66416\n",
      "279\n",
      "3200\n",
      "433\n",
      "1174\n",
      "1174\n",
      "4298\n",
      "279\n",
      "279\n",
      "279\n",
      "1174\n",
      "279\n",
      "1174\n",
      "315\n",
      "279\n",
      "88\n",
      "315\n",
      "4298\n",
      "88\n",
      "88\n",
      "4298\n",
      "279\n",
      "88\n",
      "279\n",
      "88\n",
      "279\n",
      "1174\n",
      "366\n",
      "279\n",
      "279\n",
      "662\n",
      "571\n",
      "304\n",
      "88\n",
      "279\n",
      "279\n",
      "1174\n",
      "1174\n",
      "315\n",
      "279\n",
      "1847\n",
      "86262\n",
      "279\n",
      "279\n",
      "3200\n",
      "1174\n",
      "279\n",
      "88\n",
      "264\n",
      "66416\n",
      "279\n",
      "86262\n",
      "279\n",
      "88\n",
      "330\n",
      "279\n",
      "4298\n",
      "279\n",
      "1174\n",
      "88\n",
      "304\n",
      "18\n",
      "88\n",
      "279\n",
      "86262\n",
      "3200\n",
      "366\n",
      "4298\n",
      "315\n",
      "279\n",
      "439\n",
      "1174\n",
      "220\n",
      "279\n",
      "279\n",
      "662\n",
      "279\n",
      "279\n",
      "279\n",
      "18\n",
      "66416\n",
      "3200\n",
      "279\n",
      "323\n",
      "315\n",
      "88\n",
      "264\n",
      "279\n",
      "304\n",
      "279\n",
      "279\n",
      "662\n",
      "3200\n",
      "86262\n",
      "279\n",
      "279\n",
      "1174\n",
      "1174\n",
      "279\n",
      "4298\n",
      "662\n",
      "279\n",
      "279\n",
      "279\n",
      "279\n",
      "279\n",
      "323\n",
      "279\n",
      "279\n",
      "279\n",
      "279\n",
      "279\n",
      "1174\n",
      "279\n",
      "279\n",
      "315\n",
      "3200\n",
      "662\n",
      "279\n",
      "4298\n",
      "279\n",
      "88\n",
      "279\n",
      "279\n",
      "86262\n",
      "86262\n",
      "66416\n",
      "662\n",
      "662\n",
      "86262\n",
      "279\n",
      "88\n",
      "279\n",
      "4298\n",
      "279\n",
      "304\n",
      "279\n",
      "1174\n",
      "315\n",
      "86262\n",
      "279\n",
      "279\n",
      "279\n",
      "1174\n",
      "1174\n",
      "88\n",
      "366\n",
      "279\n",
      "279\n",
      "315\n",
      "279\n",
      "279\n",
      "662\n",
      "279\n",
      "86262\n",
      "279\n",
      "662\n",
      "3200\n",
      "279\n",
      "88\n",
      "1174\n",
      "1174\n",
      "279\n",
      "279\n",
      "279\n",
      "88\n",
      "330\n",
      "279\n",
      "279\n",
      "330\n",
      "279\n",
      "1174\n",
      "315\n",
      "3200\n",
      "279\n",
      "88\n",
      "88\n",
      "279\n",
      "4298\n",
      "279\n",
      "86262\n",
      "304\n",
      "366\n",
      "315\n",
      "1174\n",
      "86262\n",
      "264\n",
      "323\n",
      "323\n",
      "279\n",
      "330\n",
      "439\n",
      "1174\n",
      "66416\n",
      "315\n",
      "1174\n",
      "1174\n",
      "86262\n",
      "279\n",
      "4298\n",
      "279\n",
      "66416\n",
      "662\n",
      "4298\n",
      "279\n",
      "374\n",
      "330\n",
      "1174\n",
      "304\n",
      "279\n",
      "279\n",
      "1174\n",
      "279\n",
      "1847\n",
      "279\n",
      "279\n",
      "3200\n",
      "1174\n",
      "279\n",
      "315\n",
      "1174\n",
      "279\n",
      "662\n",
      "304\n",
      "88\n",
      "279\n",
      "279\n",
      "279\n",
      "88\n",
      "323\n",
      "279\n",
      "279\n",
      "279\n",
      "279\n",
      "323\n",
      "86262\n",
      "1174\n",
      "279\n",
      "3200\n",
      "279\n",
      "279\n",
      "66416\n",
      "279\n",
      "279\n",
      "1174\n",
      "366\n",
      "66416\n",
      "279\n",
      "279\n",
      "279\n",
      "86262\n",
      "1174\n",
      "4298\n",
      "86262\n",
      "86262\n",
      "220\n",
      "315\n",
      "1174\n",
      "66416\n",
      "4298\n",
      "662\n",
      "279\n",
      "1174\n",
      "86262\n",
      "1174\n",
      "86262\n",
      "279\n",
      "88\n",
      "1847\n",
      "662\n",
      "279\n",
      "88\n",
      "1174\n",
      "4298\n",
      "279\n",
      "279\n",
      "304\n",
      "4101\n",
      "86262\n",
      "1174\n",
      "86262\n",
      "279\n",
      "1174\n",
      "279\n",
      "279\n",
      "279\n",
      "279\n",
      "39747\n",
      "66416\n",
      "1174\n",
      "304\n",
      "279\n",
      "66416\n",
      "279\n",
      "88\n",
      "279\n",
      "3200\n",
      "86262\n",
      "330\n",
      "279\n",
      "4298\n",
      "433\n",
      "86262\n",
      "323\n",
      "279\n",
      "304\n",
      "264\n",
      "4298\n",
      "279\n",
      "279\n",
      "1174\n",
      "279\n",
      "1174\n",
      "315\n",
      "86262\n",
      "86262\n",
      "86262\n",
      "279\n",
      "86262\n",
      "| Generated 440/1000 words\n",
      "279\n",
      "315\n",
      "279\n",
      "279\n",
      "88\n",
      "330\n",
      "279\n",
      "88\n",
      "18\n",
      "279\n",
      "279\n",
      "279\n",
      "1847\n",
      "279\n",
      "315\n",
      "3200\n",
      "662\n",
      "279\n",
      "279\n",
      "1174\n",
      "279\n",
      "279\n",
      "279\n",
      "1174\n",
      "279\n",
      "279\n",
      "86262\n",
      "4298\n",
      "1847\n",
      "1174\n",
      "279\n",
      "279\n",
      "279\n",
      "279\n",
      "279\n",
      "311\n",
      "366\n",
      "279\n",
      "279\n",
      "279\n",
      "279\n",
      "279\n",
      "4298\n",
      "279\n",
      "279\n",
      "279\n",
      "86262\n",
      "279\n",
      "220\n",
      "433\n",
      "279\n",
      "4298\n",
      "279\n",
      "279\n",
      "1174\n",
      "366\n",
      "279\n",
      "279\n",
      "1174\n",
      "1174\n",
      "1174\n",
      "279\n",
      "1174\n",
      "279\n",
      "279\n",
      "66416\n",
      "86262\n",
      "66416\n",
      "86262\n",
      "279\n",
      "662\n",
      "279\n",
      "220\n",
      "279\n",
      "315\n",
      "279\n",
      "366\n",
      "86262\n",
      "279\n",
      "279\n",
      "279\n",
      "4298\n",
      "4298\n",
      "279\n",
      "279\n",
      "279\n",
      "1174\n",
      "86262\n",
      "279\n",
      "315\n",
      "86262\n",
      "279\n",
      "304\n",
      "86262\n",
      "279\n",
      "279\n",
      "1174\n",
      "279\n",
      "315\n",
      "220\n",
      "662\n",
      "279\n",
      "315\n",
      "4298\n",
      "279\n",
      "279\n",
      "1174\n",
      "315\n",
      "88\n",
      "1174\n",
      "4298\n",
      "88\n",
      "279\n",
      "330\n",
      "86262\n",
      "279\n",
      "279\n",
      "3200\n",
      "1174\n",
      "279\n",
      "1847\n",
      "86262\n",
      "279\n",
      "66416\n",
      "279\n",
      "279\n",
      "279\n",
      "279\n",
      "1174\n",
      "279\n",
      "86262\n",
      "18\n",
      "86262\n",
      "3200\n",
      "3200\n",
      "304\n",
      "1174\n",
      "279\n",
      "66416\n",
      "279\n",
      "1174\n",
      "86262\n",
      "279\n",
      "1174\n",
      "4298\n",
      "279\n",
      "86262\n",
      "1174\n",
      "330\n",
      "86262\n",
      "279\n",
      "279\n",
      "220\n",
      "366\n",
      "279\n",
      "279\n",
      "279\n",
      "279\n",
      "1174\n",
      "279\n",
      "1847\n",
      "264\n",
      "366\n",
      "279\n",
      "279\n",
      "279\n",
      "66416\n",
      "330\n",
      "1174\n",
      "279\n",
      "323\n",
      "279\n",
      "86262\n",
      "279\n",
      "86262\n",
      "86262\n",
      "264\n",
      "1174\n",
      "86262\n",
      "1847\n",
      "279\n",
      "662\n",
      "4101\n",
      "279\n",
      "279\n",
      "366\n",
      "279\n",
      "4298\n",
      "18\n",
      "88\n",
      "279\n",
      "311\n",
      "323\n",
      "279\n",
      "279\n",
      "433\n",
      "1174\n",
      "4298\n",
      "4298\n",
      "330\n",
      "1174\n",
      "279\n",
      "86262\n",
      "330\n",
      "1174\n",
      "1847\n",
      "315\n",
      "315\n",
      "374\n",
      "279\n",
      "315\n",
      "86262\n",
      "66416\n",
      "315\n",
      "279\n",
      "279\n",
      "1174\n",
      "1174\n",
      "1174\n",
      "1174\n",
      "330\n",
      "279\n",
      "18\n",
      "86262\n",
      "86262\n",
      "323\n",
      "264\n",
      "304\n",
      "279\n",
      "279\n",
      "304\n",
      "1174\n",
      "264\n",
      "279\n",
      "279\n",
      "279\n",
      "86262\n",
      "4298\n",
      "315\n",
      "279\n",
      "279\n",
      "1174\n",
      "1174\n",
      "279\n",
      "315\n",
      "1174\n",
      "279\n",
      "86262\n",
      "279\n",
      "279\n",
      "1847\n",
      "86262\n",
      "366\n",
      "279\n",
      "279\n",
      "279\n",
      "1174\n",
      "279\n",
      "1174\n",
      "330\n",
      "1174\n",
      "330\n",
      "279\n",
      "86262\n",
      "1847\n",
      "279\n",
      "330\n",
      "279\n",
      "1174\n",
      "315\n",
      "1847\n",
      "1847\n",
      "279\n",
      "279\n",
      "279\n",
      "330\n",
      "86262\n",
      "439\n",
      "433\n",
      "662\n",
      "1174\n",
      "315\n",
      "4298\n",
      "662\n",
      "86262\n",
      "279\n",
      "1174\n",
      "279\n",
      "86262\n",
      "330\n",
      "662\n",
      "279\n",
      "279\n",
      "1174\n",
      "279\n",
      "279\n",
      "66416\n",
      "86262\n",
      "279\n",
      "279\n",
      "279\n",
      "1174\n",
      "315\n",
      "279\n",
      "279\n",
      "279\n",
      "4298\n",
      "279\n",
      "279\n",
      "86262\n",
      "304\n",
      "1847\n",
      "86262\n",
      "323\n",
      "304\n",
      "279\n",
      "4298\n",
      "279\n",
      "323\n",
      "279\n",
      "330\n",
      "4298\n",
      "86262\n",
      "279\n",
      "1174\n",
      "1174\n",
      "88\n",
      "264\n",
      "1847\n",
      "323\n",
      "1174\n",
      "279\n",
      "330\n",
      "279\n",
      "279\n",
      "304\n",
      "279\n",
      "662\n",
      "279\n",
      "366\n",
      "279\n",
      "279\n",
      "88\n",
      "279\n",
      "279\n",
      "220\n",
      "279\n",
      "279\n",
      "330\n",
      "279\n",
      "88\n",
      "279\n",
      "304\n",
      "279\n",
      "4298\n",
      "279\n",
      "279\n",
      "1174\n",
      "220\n",
      "264\n",
      "279\n",
      "279\n",
      "323\n",
      "279\n",
      "279\n",
      "279\n",
      "315\n",
      "86262\n",
      "662\n",
      "1174\n",
      "279\n",
      "279\n",
      "1847\n",
      "279\n",
      "279\n",
      "86262\n",
      "88\n",
      "220\n",
      "279\n",
      "279\n",
      "1847\n",
      "4298\n",
      "88\n",
      "86262\n",
      "86262\n",
      "279\n",
      "279\n",
      "279\n",
      "279\n",
      "279\n",
      "279\n",
      "279\n",
      "279\n",
      "66416\n",
      "88\n",
      "66416\n",
      "1174\n",
      "39747\n",
      "279\n",
      "279\n",
      "279\n",
      "1174\n",
      "279\n",
      "86262\n",
      "1174\n",
      "86262\n",
      "279\n",
      "279\n",
      "279\n",
      "88\n",
      "304\n",
      "4298\n",
      "86262\n",
      "279\n",
      "279\n",
      "279\n",
      "279\n",
      "279\n",
      "279\n",
      "279\n",
      "304\n",
      "279\n",
      "220\n",
      "279\n",
      "1174\n",
      "279\n",
      "279\n",
      "304\n",
      "1174\n",
      "279\n",
      "279\n",
      "323\n",
      "4298\n",
      "279\n",
      "330\n",
      "1174\n",
      "1847\n",
      "279\n",
      "279\n",
      "1847\n",
      "| Generated 880/1000 words\n",
      "279\n",
      "330\n",
      "433\n",
      "1174\n",
      "662\n",
      "279\n",
      "315\n",
      "66416\n",
      "315\n",
      "4298\n",
      "86262\n",
      "366\n",
      "18\n",
      "86262\n",
      "279\n",
      "86262\n",
      "315\n",
      "88\n",
      "279\n",
      "279\n",
      "304\n",
      "315\n",
      "5089\n",
      "220\n",
      "1174\n",
      "330\n",
      "1847\n",
      "86262\n",
      "279\n",
      "279\n",
      "1847\n",
      "279\n",
      "279\n",
      "88\n",
      "31\n",
      "264\n",
      "279\n",
      "1174\n",
      "315\n",
      "4298\n",
      "4298\n",
      "1174\n",
      "86262\n",
      "304\n",
      "66416\n",
      "1174\n",
      "88\n",
      "279\n",
      "279\n",
      "1174\n",
      "279\n",
      "86262\n",
      "279\n",
      "18\n",
      "18\n",
      "279\n",
      "88\n",
      "279\n",
      "1847\n",
      "86262\n",
      "4298\n",
      "315\n",
      "279\n",
      "1174\n",
      "4298\n",
      "88\n",
      "433\n",
      "279\n",
      "1847\n",
      "279\n",
      "330\n",
      "1847\n",
      "279\n",
      "279\n",
      "279\n",
      "279\n",
      "1174\n",
      "279\n",
      "1847\n",
      "279\n",
      "220\n",
      "279\n",
      "4298\n",
      "1174\n",
      "279\n",
      "4298\n",
      "1174\n",
      "279\n",
      "279\n",
      "220\n",
      "374\n",
      "374\n",
      "304\n",
      "1174\n",
      "304\n",
      "279\n",
      "315\n",
      "86262\n",
      "88\n",
      "18\n",
      "330\n",
      "4298\n",
      "279\n",
      "279\n",
      "1174\n",
      "279\n",
      "88\n",
      "279\n",
      "279\n",
      "279\n",
      "279\n",
      "86262\n",
      "18\n",
      "3200\n",
      "66416\n",
      "1174\n",
      "279\n",
      "279\n",
      "315\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "generate_text(transformer_model, 'The meaning of life is', '../output/tokenized_transformer_generated.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_text('LSTM.pt', 'LSTM', corpus, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "group-assignment-f09OljWU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
