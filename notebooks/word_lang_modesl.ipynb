{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch datasets"
      ],
      "metadata": {
        "id": "N1HNrmN3xD-0",
        "outputId": "7d3c0ee5-fe2c-41bd-b2eb-44af6a14bda9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "V1G82GuO-tez",
        "outputId": "8037dea1-aaf1-40b6-a357-c3ced97841c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T09:21:01.580938Z",
          "iopub.status.busy": "2023-11-04T09:21:01.580427Z",
          "iopub.status.idle": "2023-11-04T09:21:01.591148Z",
          "shell.execute_reply": "2023-11-04T09:21:01.588760Z",
          "shell.execute_reply.started": "2023-11-04T09:21:01.580899Z"
        },
        "id": "a7aoXUYPw3jn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "from datasets import load_dataset\n",
        "from io import open\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:1024'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T09:19:42.904693Z",
          "iopub.status.busy": "2023-11-04T09:19:42.904203Z",
          "iopub.status.idle": "2023-11-04T09:19:42.918754Z",
          "shell.execute_reply": "2023-11-04T09:19:42.916393Z",
          "shell.execute_reply.started": "2023-11-04T09:19:42.904655Z"
        },
        "id": "xWLV4vdPw3jp"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "nhead = 4\n",
        "ninp = 128\n",
        "em_size = 200\n",
        "nhid = 200\n",
        "nlayers = 4\n",
        "dropout = 0.2\n",
        "bptt = 150\n",
        "batch_size = 32\n",
        "eval_batch_size = 16\n",
        "learning_rate = 20\n",
        "clip = 0.25\n",
        "epochs = 40\n",
        "torch.manual_seed(1332)\n",
        "log_interval = 250\n",
        "data_dir = './data/wikitext/'\n",
        "models_dir = './models/'\n",
        "out_dir = './output/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T09:14:14.307606Z",
          "iopub.status.busy": "2023-11-04T09:14:14.306207Z",
          "iopub.status.idle": "2023-11-04T09:14:14.663239Z",
          "shell.execute_reply": "2023-11-04T09:14:14.661933Z",
          "shell.execute_reply.started": "2023-11-04T09:14:14.307546Z"
        },
        "id": "KlKjiPxpw3jq",
        "outputId": "c5387cb0-112b-4690-a370-70ae834b225e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(device(type='cuda', index=0), device(type='cpu'))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    cuda = torch.device('cuda:0')\n",
        "    # cuda2 = torch.device('cuda:1')\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "cpu = torch.device('cpu')\n",
        "\n",
        "cuda, cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T09:14:14.667296Z",
          "iopub.status.busy": "2023-11-04T09:14:14.665930Z",
          "iopub.status.idle": "2023-11-04T09:14:14.674904Z",
          "shell.execute_reply": "2023-11-04T09:14:14.673703Z",
          "shell.execute_reply.started": "2023-11-04T09:14:14.667249Z"
        },
        "id": "Ze4e0bYew3jq"
      },
      "outputs": [],
      "source": [
        "## Taken from https://github.com/pytorch/examples/blob/main/word_language_model/data.py on 01/11/2023\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "        self.counter = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.idx2word:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = self.counter\n",
        "            self.counter += 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T09:14:14.677672Z",
          "iopub.status.busy": "2023-11-04T09:14:14.676318Z",
          "iopub.status.idle": "2023-11-04T09:14:14.684279Z",
          "shell.execute_reply": "2023-11-04T09:14:14.683154Z",
          "shell.execute_reply.started": "2023-11-04T09:14:14.677614Z"
        },
        "id": "V9gmzJ5vw3jq"
      },
      "outputs": [],
      "source": [
        "def tokenise_prompt(input, corpus):\n",
        "    words = input.split() + ['<eos>']\n",
        "    ids = torch.LongTensor(len(words))\n",
        "    for i, word in enumerate(words):\n",
        "        ids[i] = corpus.dictionary.word2idx[word]\n",
        "    return ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T09:14:14.687289Z",
          "iopub.status.busy": "2023-11-04T09:14:14.685693Z",
          "iopub.status.idle": "2023-11-04T09:14:14.722724Z",
          "shell.execute_reply": "2023-11-04T09:14:14.721195Z",
          "shell.execute_reply.started": "2023-11-04T09:14:14.687252Z"
        },
        "id": "qApWWP3Rw3jr"
      },
      "outputs": [],
      "source": [
        "## Taken from https://github.com/pytorch/examples/blob/main/word_language_model/data.py on 01/11/2023\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        assert os.path.exists(path)\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            tokens = 0\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                tokens += len(words)\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "            ids = torch.LongTensor(tokens)\n",
        "            token = 0\n",
        "            with open(path, 'r', encoding='utf-8') as f:\n",
        "                for line in f:\n",
        "                    words = line.split() + ['<eos>']\n",
        "                    for word in words:\n",
        "                        ids[token] = self.dictionary.word2idx[word]\n",
        "                        token += 1\n",
        "\n",
        "            return ids.to(cpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T09:14:14.725227Z",
          "iopub.status.busy": "2023-11-04T09:14:14.724280Z",
          "iopub.status.idle": "2023-11-04T09:14:18.570786Z",
          "shell.execute_reply": "2023-11-04T09:14:18.569187Z",
          "shell.execute_reply.started": "2023-11-04T09:14:14.725189Z"
        },
        "id": "72DdvdPMw3jr"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(data_dir):\n",
        "    # os.mkdir(data_dir)\n",
        "    data = load_dataset('wikitext', 'wikitext-2-v1')\n",
        "    data.save_to_disk(os.path.join(data_dir, 'wikitext-2'))\n",
        "data = load_dataset('wikitext', 'wikitext-2-v1', data_dir=data_dir)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T09:14:18.572935Z",
          "iopub.status.busy": "2023-11-04T09:14:18.572569Z",
          "iopub.status.idle": "2023-11-04T09:14:18.581893Z",
          "shell.execute_reply": "2023-11-04T09:14:18.580563Z",
          "shell.execute_reply.started": "2023-11-04T09:14:18.572900Z"
        },
        "id": "iEmEZbF0w3jr",
        "outputId": "9d33b180-d86b-4aef-b36e-610ef5746dcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    test: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 4358\n",
              "    })\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 36718\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 3760\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T09:14:18.583475Z",
          "iopub.status.busy": "2023-11-04T09:14:18.583140Z",
          "iopub.status.idle": "2023-11-04T09:14:18.850893Z",
          "shell.execute_reply": "2023-11-04T09:14:18.849429Z",
          "shell.execute_reply.started": "2023-11-04T09:14:18.583439Z"
        },
        "id": "aflD5ZMDw3jr"
      },
      "outputs": [],
      "source": [
        "## Converting from HF dataset to plain text files can probably improve this\n",
        "train_data = '\\n'.join(data['train']['text'])\n",
        "valid_data = '\\n'.join(data['validation']['text'])\n",
        "test_data = '\\n'.join(data['test']['text'])\n",
        "with open(os.path.join(data_dir, 'train.txt'), 'w') as f:\n",
        "    f.write(train_data)\n",
        "with open(os.path.join(data_dir, 'valid.txt'), 'w') as f:\n",
        "    f.write(valid_data)\n",
        "with open(os.path.join(data_dir, 'test.txt'), 'w') as f:\n",
        "    f.write(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T09:14:18.856606Z",
          "iopub.status.busy": "2023-11-04T09:14:18.855997Z",
          "iopub.status.idle": "2023-11-04T09:14:18.864050Z",
          "shell.execute_reply": "2023-11-04T09:14:18.862699Z",
          "shell.execute_reply.started": "2023-11-04T09:14:18.856565Z"
        },
        "id": "78ccQR9Zw3jr"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T09:14:18.879657Z",
          "iopub.status.busy": "2023-11-04T09:14:18.879251Z",
          "iopub.status.idle": "2023-11-04T09:18:33.362850Z",
          "shell.execute_reply": "2023-11-04T09:18:33.348632Z",
          "shell.execute_reply.started": "2023-11-04T09:14:18.879620Z"
        },
        "id": "sOyC1KVjw3jr"
      },
      "outputs": [],
      "source": [
        "corpus = Corpus(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T09:18:33.425062Z",
          "iopub.status.busy": "2023-11-04T09:18:33.424648Z",
          "iopub.status.idle": "2023-11-04T09:18:33.433891Z",
          "shell.execute_reply": "2023-11-04T09:18:33.432520Z",
          "shell.execute_reply.started": "2023-11-04T09:18:33.425025Z"
        },
        "id": "CwVQ8vwzw3js"
      },
      "outputs": [],
      "source": [
        "def batchify(data, bsz):\n",
        "    nbatch = data.size(0) // bsz\n",
        "    data = data.narrow(0, 0, nbatch*bsz)\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(cuda)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T09:18:33.440753Z",
          "iopub.status.busy": "2023-11-04T09:18:33.440274Z",
          "iopub.status.idle": "2023-11-04T09:18:33.459962Z",
          "shell.execute_reply": "2023-11-04T09:18:33.458257Z",
          "shell.execute_reply.started": "2023-11-04T09:18:33.440712Z"
        },
        "id": "oqoW5uHpw3js"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_data = batchify(corpus.train, batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T09:18:33.465322Z",
          "iopub.status.busy": "2023-11-04T09:18:33.464940Z",
          "iopub.status.idle": "2023-11-04T09:18:33.481654Z",
          "shell.execute_reply": "2023-11-04T09:18:33.479882Z",
          "shell.execute_reply.started": "2023-11-04T09:18:33.465286Z"
        },
        "id": "_wLhwVJkw3js"
      },
      "outputs": [],
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.lstm = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "        self.init_weights()\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "        self.ntokens = ntoken\n",
        "        self.model_type = 'LSTM'\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        output, hidden = self.lstm(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output)\n",
        "        decoded = decoded.view(-1, self.ntokens)\n",
        "        return F.log_softmax(decoded, dim=1), hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "                weight.new_zeros(self.nlayers, bsz, self.nhid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T09:18:33.487649Z",
          "iopub.status.busy": "2023-11-04T09:18:33.487289Z",
          "iopub.status.idle": "2023-11-04T09:18:33.499039Z",
          "shell.execute_reply": "2023-11-04T09:18:33.497601Z",
          "shell.execute_reply.started": "2023-11-04T09:18:33.487610Z"
        },
        "id": "eY3-3yXqw3js"
      },
      "outputs": [],
      "source": [
        "class PoisitionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PoisitionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2)*(-math.log(10000.0)/d_model))\n",
        "        pe[:, 0::2] = torch.sin(position*div_term)\n",
        "        pe[:, 1::2] = torch.cos(position*div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T09:18:33.502810Z",
          "iopub.status.busy": "2023-11-04T09:18:33.502448Z",
          "iopub.status.idle": "2023-11-04T09:18:33.520909Z",
          "shell.execute_reply": "2023-11-04T09:18:33.519213Z",
          "shell.execute_reply.started": "2023-11-04T09:18:33.502777Z"
        },
        "id": "PCui5sbbw3js"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Transformer):\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__(d_model=ninp, nhead=nhead, dim_feedforward=nhid, num_decoder_layers=nlayers)\n",
        "        # self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PoisitionalEncoding(ninp, dropout)\n",
        "\n",
        "        self.input_emb = nn.Embedding(ntoken, ninp)\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "        self.ninp = ninp\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "        self.ntokens = ntoken\n",
        "        self.model_type = 'Transformer'\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz) == 1).transpose(0, 1))\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.input_emb.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src, has_mask=True):\n",
        "        if has_mask:\n",
        "            device = src.device\n",
        "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "                self.src_mask = mask\n",
        "        else:\n",
        "            self.src_mask = None\n",
        "\n",
        "        src = self.input_emb(src)*math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.encoder(src, mask=self.src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return F.log_softmax(output, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T09:21:12.520325Z",
          "iopub.status.busy": "2023-11-04T09:21:12.519821Z",
          "iopub.status.idle": "2023-11-04T09:21:14.464553Z",
          "shell.execute_reply": "2023-11-04T09:21:14.454285Z",
          "shell.execute_reply.started": "2023-11-04T09:21:12.520284Z"
        },
        "id": "KkkXddS4w3js",
        "outputId": "de83832b-29f4-4dc5-8ab0-22d732e6e7ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "ntokens = len(corpus.dictionary)\n",
        "transformer_model = TransformerModel(ntoken=ntokens, ninp=ninp, nhead=nhead, nhid=nhid, nlayers=nlayers, dropout=dropout).to(cuda)\n",
        "lstm_model = LSTMModel(ntoken=ntokens, ninp=ninp, nhid=nhid, nlayers=nlayers,dropout=dropout).to(cuda)\n",
        "\n",
        "criterion = nn.NLLLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T09:18:38.722765Z",
          "iopub.status.busy": "2023-11-04T09:18:38.722421Z",
          "iopub.status.idle": "2023-11-04T09:18:38.731457Z",
          "shell.execute_reply": "2023-11-04T09:18:38.729224Z",
          "shell.execute_reply.started": "2023-11-04T09:18:38.722732Z"
        },
        "id": "VvKhLMKtw3js"
      },
      "outputs": [],
      "source": [
        "def repackage_hidden(h):\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T09:18:38.745787Z",
          "iopub.status.busy": "2023-11-04T09:18:38.745408Z",
          "iopub.status.idle": "2023-11-04T09:18:38.753865Z",
          "shell.execute_reply": "2023-11-04T09:18:38.752080Z",
          "shell.execute_reply.started": "2023-11-04T09:18:38.745750Z"
        },
        "id": "RLcw5Didw3js"
      },
      "outputs": [],
      "source": [
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source)-1-i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T09:18:38.762730Z",
          "iopub.status.busy": "2023-11-04T09:18:38.762319Z",
          "iopub.status.idle": "2023-11-04T09:18:38.777203Z",
          "shell.execute_reply": "2023-11-04T09:18:38.775919Z",
          "shell.execute_reply.started": "2023-11-04T09:18:38.762689Z"
        },
        "id": "Hj8XKEszw3jt"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data_source):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    model_type = model.model_type\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    if model_type != 'Transformer':\n",
        "        hidden = model.init_hidden(eval_batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0)-1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            if model_type == 'Transformer':\n",
        "                output = model(data)\n",
        "                output = output.view(-1, ntokens)\n",
        "            else:\n",
        "                output, hidden = model(data, hidden)\n",
        "                hidden = repackage_hidden(hidden)\n",
        "            total_loss += len(data)*criterion(output, targets).item()\n",
        "    return total_loss/(len(data_source)-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T09:18:38.781605Z",
          "iopub.status.busy": "2023-11-04T09:18:38.781249Z",
          "iopub.status.idle": "2023-11-04T09:18:38.836997Z",
          "shell.execute_reply": "2023-11-04T09:18:38.834835Z",
          "shell.execute_reply.started": "2023-11-04T09:18:38.781548Z"
        },
        "id": "BYojT0uzw3jt"
      },
      "outputs": [],
      "source": [
        "def train(model, epoch, lr):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    if model.model_type != 'Transformer':\n",
        "        hidden = model.init_hidden(batch_size)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0)-1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        model.zero_grad()\n",
        "        if model.model_type == 'Transformer':\n",
        "            output = model(data)\n",
        "            output = output.view(-1, ntokens)\n",
        "        else:\n",
        "            output, hidden = model(data, hidden)\n",
        "            hidden = repackage_hidden(hidden)\n",
        "        loss = criterion(output, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        for p in model.parameters():\n",
        "            p.data.add_(p.grad, alpha=-lr)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch%log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss/log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                        epoch, batch, len(train_data)//bptt, elapsed*1000/log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T09:18:38.844201Z",
          "iopub.status.busy": "2023-11-04T09:18:38.843765Z",
          "iopub.status.idle": "2023-11-04T09:18:38.852425Z",
          "shell.execute_reply": "2023-11-04T09:18:38.850898Z",
          "shell.execute_reply.started": "2023-11-04T09:18:38.844154Z"
        },
        "id": "dl_zElMTw3jt"
      },
      "outputs": [],
      "source": [
        "def export_onnx(path, model, batch_size, seq_len):\n",
        "    model.eval()\n",
        "    x = torch.rand(seq_len, batch_size).to(device)\n",
        "    if model.model_type != \"Transformer\":\n",
        "        hidden = model.init_hidden(batch_size)\n",
        "        torch.onnx.export(model, (x, hidden), path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T09:18:38.857218Z",
          "iopub.status.busy": "2023-11-04T09:18:38.856890Z",
          "iopub.status.idle": "2023-11-04T09:18:38.868525Z",
          "shell.execute_reply": "2023-11-04T09:18:38.866950Z",
          "shell.execute_reply.started": "2023-11-04T09:18:38.857183Z"
        },
        "id": "mjA5PxGww3jt"
      },
      "outputs": [],
      "source": [
        "def run_training(model, epochs, learning_rate):\n",
        "    torch.cuda.empty_cache()\n",
        "    lr = learning_rate\n",
        "    best_val_loss = None\n",
        "    model_type = model.model_type\n",
        "    model_path = os.path.join(models_dir, (model_type + '.pt'))\n",
        "    for epoch in range(1, epochs+1):\n",
        "        torch.cuda.empty_cache()\n",
        "        epoch_start_time = time.time()\n",
        "        train(model, epoch, lr)\n",
        "        val_loss = evaluate(model, val_data)\n",
        "        print('-'*89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time()-epoch_start_time), val_loss, math.exp(val_loss)))\n",
        "        print('-'*89)\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(model_path, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            lr /= 4.0\n",
        "\n",
        "    test_loss = evaluate(model, test_data)\n",
        "    print('='*89)\n",
        "    print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(test_loss, math.exp(test_loss)))\n",
        "    print('='*89)\n",
        "    # export_onnx(os.path.join(models_dir, model_type), model, batch_size=1, seq_len=bptt)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "mem_rep_cuda = torch.cuda.memory_summary(device=cuda, abbreviated=False)\n",
        "print(mem_rep_cuda)"
      ],
      "metadata": {
        "id": "ZMK8xKde3LVe",
        "outputId": "41d3d63e-5911-437c-94df-619870f6a908",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 107244 KiB | 107244 KiB | 112047 KiB |   4803 KiB |\n",
            "|       from large pool | 104214 KiB | 104214 KiB | 104214 KiB |      0 KiB |\n",
            "|       from small pool |   3030 KiB |   7703 KiB |   7833 KiB |   4803 KiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 107244 KiB | 107244 KiB | 112047 KiB |   4803 KiB |\n",
            "|       from large pool | 104214 KiB | 104214 KiB | 104214 KiB |      0 KiB |\n",
            "|       from small pool |   3030 KiB |   7703 KiB |   7833 KiB |   4803 KiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 106407 KiB | 106407 KiB | 111207 KiB |   4800 KiB |\n",
            "|       from large pool | 103378 KiB | 103378 KiB | 103378 KiB |      0 KiB |\n",
            "|       from small pool |   3028 KiB |   7698 KiB |   7828 KiB |   4800 KiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   | 129024 KiB | 133120 KiB | 133120 KiB |   4096 KiB |\n",
            "|       from large pool | 124928 KiB | 124928 KiB | 124928 KiB |      0 KiB |\n",
            "|       from small pool |   4096 KiB |   8192 KiB |   8192 KiB |   4096 KiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  21780 KiB |  23492 KiB |  66160 KiB |  44380 KiB |\n",
            "|       from large pool |  20714 KiB |  20714 KiB |  55921 KiB |  35207 KiB |\n",
            "|       from small pool |   1066 KiB |   2778 KiB |  10239 KiB |   9173 KiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |      85    |      99    |     101    |      16    |\n",
            "|       from large pool |       9    |       9    |       9    |       0    |\n",
            "|       from small pool |      76    |      91    |      92    |      16    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |      85    |      99    |     101    |      16    |\n",
            "|       from large pool |       9    |       9    |       9    |       0    |\n",
            "|       from small pool |      76    |      91    |      92    |      16    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |       8    |      10    |      10    |       2    |\n",
            "|       from large pool |       6    |       6    |       6    |       0    |\n",
            "|       from small pool |       2    |       4    |       4    |       2    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       6    |      11    |      13    |       7    |\n",
            "|       from large pool |       4    |       4    |       5    |       1    |\n",
            "|       from small pool |       2    |       7    |       8    |       6    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T09:18:38.873781Z",
          "iopub.status.busy": "2023-11-04T09:18:38.873395Z",
          "iopub.status.idle": "2023-11-04T09:18:38.884769Z",
          "shell.execute_reply": "2023-11-04T09:18:38.882666Z",
          "shell.execute_reply.started": "2023-11-04T09:18:38.873746Z"
        },
        "id": "-S1uZKK6w3jt",
        "outputId": "6c49c191-5353-4abf-9ba6-5dcc09028f41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "132120576\n",
            "136314880\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.memory_reserved(device=cuda))\n",
        "print(torch.cuda.max_memory_reserved(device=cuda))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T09:20:13.120566Z",
          "iopub.status.busy": "2023-11-04T09:20:13.120095Z",
          "iopub.status.idle": "2023-11-04T09:20:14.667464Z",
          "shell.execute_reply": "2023-11-04T09:20:14.664166Z",
          "shell.execute_reply.started": "2023-11-04T09:20:13.120528Z"
        },
        "id": "4Beu4788w3jt",
        "outputId": "9b1cd03e-d341-4b74-aca6-8a7f6f73e75d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   250/  440 batches | ms/batch 31.08 | loss 59.33 | ppl 58202891542373762657681408.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 14.33s | valid loss 28.51 | valid ppl 2404605230324.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   250/  440 batches | ms/batch 31.12 | loss 58.52 | ppl 25944479316779617092632576.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 14.30s | valid loss 50.11 | valid ppl 5779969503360030081024.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   250/  440 batches | ms/batch 31.07 | loss 11.14 | ppl 68597.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 14.35s | valid loss 11.79 | valid ppl 131612.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   250/  440 batches | ms/batch 31.29 | loss  8.30 | ppl  4034.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 14.33s | valid loss  7.62 | valid ppl  2043.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   250/  440 batches | ms/batch 31.07 | loss  7.71 | ppl  2226.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 14.27s | valid loss  7.43 | valid ppl  1685.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   250/  440 batches | ms/batch 30.90 | loss  7.35 | ppl  1550.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 14.20s | valid loss  7.22 | valid ppl  1373.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   250/  440 batches | ms/batch 30.89 | loss  7.16 | ppl  1287.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 14.23s | valid loss  7.36 | valid ppl  1569.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   250/  440 batches | ms/batch 31.14 | loss  7.08 | ppl  1185.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 14.26s | valid loss  6.87 | valid ppl   964.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   250/  440 batches | ms/batch 30.90 | loss  7.07 | ppl  1174.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 14.21s | valid loss  6.87 | valid ppl   967.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   250/  440 batches | ms/batch 30.93 | loss  7.07 | ppl  1176.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 14.20s | valid loss  6.85 | valid ppl   943.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |   250/  440 batches | ms/batch 30.79 | loss  7.07 | ppl  1173.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 14.20s | valid loss  6.85 | valid ppl   943.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |   250/  440 batches | ms/batch 30.89 | loss  7.07 | ppl  1171.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 14.22s | valid loss  6.85 | valid ppl   942.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |   250/  440 batches | ms/batch 30.81 | loss  7.07 | ppl  1170.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 14.16s | valid loss  6.85 | valid ppl   942.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |   250/  440 batches | ms/batch 31.02 | loss  7.06 | ppl  1169.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 14.22s | valid loss  6.85 | valid ppl   941.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |   250/  440 batches | ms/batch 30.97 | loss  7.06 | ppl  1169.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 14.24s | valid loss  6.85 | valid ppl   941.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |   250/  440 batches | ms/batch 30.83 | loss  7.06 | ppl  1168.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 14.25s | valid loss  6.85 | valid ppl   940.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |   250/  440 batches | ms/batch 30.92 | loss  7.06 | ppl  1167.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 14.21s | valid loss  6.85 | valid ppl   940.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |   250/  440 batches | ms/batch 31.00 | loss  7.06 | ppl  1167.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 14.21s | valid loss  6.85 | valid ppl   940.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |   250/  440 batches | ms/batch 30.86 | loss  7.06 | ppl  1166.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 14.21s | valid loss  6.85 | valid ppl   939.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |   250/  440 batches | ms/batch 30.80 | loss  7.06 | ppl  1165.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 14.21s | valid loss  6.85 | valid ppl   939.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |   250/  440 batches | ms/batch 31.02 | loss  7.06 | ppl  1165.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 14.25s | valid loss  6.85 | valid ppl   939.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |   250/  440 batches | ms/batch 30.86 | loss  7.06 | ppl  1164.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 14.20s | valid loss  6.84 | valid ppl   938.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |   250/  440 batches | ms/batch 30.96 | loss  7.06 | ppl  1164.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 14.24s | valid loss  6.84 | valid ppl   938.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  24 |   250/  440 batches | ms/batch 30.90 | loss  7.06 | ppl  1164.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 14.19s | valid loss  6.84 | valid ppl   938.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |   250/  440 batches | ms/batch 30.78 | loss  7.06 | ppl  1163.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 14.16s | valid loss  6.84 | valid ppl   938.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  26 |   250/  440 batches | ms/batch 31.09 | loss  7.06 | ppl  1163.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 14.24s | valid loss  6.84 | valid ppl   938.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  27 |   250/  440 batches | ms/batch 30.94 | loss  7.06 | ppl  1162.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 14.22s | valid loss  6.84 | valid ppl   937.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  28 |   250/  440 batches | ms/batch 30.76 | loss  7.06 | ppl  1162.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 14.13s | valid loss  6.84 | valid ppl   937.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  29 |   250/  440 batches | ms/batch 30.78 | loss  7.06 | ppl  1162.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 14.18s | valid loss  6.84 | valid ppl   937.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  30 |   250/  440 batches | ms/batch 30.84 | loss  7.06 | ppl  1161.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 14.14s | valid loss  6.84 | valid ppl   937.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  31 |   250/  440 batches | ms/batch 30.94 | loss  7.06 | ppl  1161.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 14.18s | valid loss  6.84 | valid ppl   937.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  32 |   250/  440 batches | ms/batch 30.94 | loss  7.06 | ppl  1161.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 14.27s | valid loss  6.84 | valid ppl   937.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  33 |   250/  440 batches | ms/batch 30.78 | loss  7.06 | ppl  1160.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 14.19s | valid loss  6.84 | valid ppl   936.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  34 |   250/  440 batches | ms/batch 30.82 | loss  7.06 | ppl  1160.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 14.17s | valid loss  6.84 | valid ppl   936.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  35 |   250/  440 batches | ms/batch 30.72 | loss  7.06 | ppl  1160.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 14.15s | valid loss  6.84 | valid ppl   936.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  36 |   250/  440 batches | ms/batch 30.81 | loss  7.06 | ppl  1160.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 14.18s | valid loss  6.84 | valid ppl   936.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  37 |   250/  440 batches | ms/batch 30.73 | loss  7.06 | ppl  1159.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 14.16s | valid loss  6.84 | valid ppl   936.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  38 |   250/  440 batches | ms/batch 30.98 | loss  7.06 | ppl  1159.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 14.24s | valid loss  6.84 | valid ppl   936.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  39 |   250/  440 batches | ms/batch 30.94 | loss  7.06 | ppl  1159.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 14.22s | valid loss  6.84 | valid ppl   936.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  40 |   250/  440 batches | ms/batch 30.87 | loss  7.06 | ppl  1159.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 14.19s | valid loss  6.84 | valid ppl   936.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss  6.78 | test ppl   881.18\n",
            "=========================================================================================\n"
          ]
        }
      ],
      "source": [
        "run_training(transformer_model, epochs, learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-04T09:18:42.420643Z",
          "iopub.status.idle": "2023-11-04T09:18:42.421196Z",
          "shell.execute_reply": "2023-11-04T09:18:42.420956Z",
          "shell.execute_reply.started": "2023-11-04T09:18:42.420928Z"
        },
        "id": "HE_xylZCw3jt",
        "outputId": "35639824-e01f-473d-8418-4642c26797e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   250/  440 batches | ms/batch 57.45 | loss  7.68 | ppl  2164.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 25.86s | valid loss  7.27 | valid ppl  1434.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   250/  440 batches | ms/batch 56.15 | loss  7.22 | ppl  1360.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 25.51s | valid loss  7.12 | valid ppl  1234.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   250/  440 batches | ms/batch 56.14 | loss  7.04 | ppl  1146.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 25.53s | valid loss  6.36 | valid ppl   578.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   250/  440 batches | ms/batch 55.95 | loss  6.51 | ppl   669.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 25.46s | valid loss  6.21 | valid ppl   498.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   250/  440 batches | ms/batch 55.91 | loss  6.25 | ppl   517.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 25.48s | valid loss  5.98 | valid ppl   395.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   250/  440 batches | ms/batch 56.26 | loss  6.00 | ppl   403.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 25.59s | valid loss  5.85 | valid ppl   345.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   250/  440 batches | ms/batch 56.35 | loss  5.80 | ppl   331.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 25.61s | valid loss  5.66 | valid ppl   287.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   250/  440 batches | ms/batch 56.03 | loss  5.64 | ppl   280.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 25.48s | valid loss  5.60 | valid ppl   271.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   250/  440 batches | ms/batch 55.98 | loss  5.52 | ppl   249.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 25.49s | valid loss  5.53 | valid ppl   251.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   250/  440 batches | ms/batch 56.18 | loss  5.40 | ppl   221.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 25.58s | valid loss  5.43 | valid ppl   228.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |   250/  440 batches | ms/batch 56.02 | loss  5.31 | ppl   201.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 25.47s | valid loss  5.35 | valid ppl   210.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |   250/  440 batches | ms/batch 55.98 | loss  5.22 | ppl   185.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 25.46s | valid loss  5.33 | valid ppl   205.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |   250/  440 batches | ms/batch 56.08 | loss  5.15 | ppl   173.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 25.49s | valid loss  5.24 | valid ppl   188.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |   250/  440 batches | ms/batch 56.11 | loss  5.09 | ppl   162.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 25.50s | valid loss  5.25 | valid ppl   191.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |   250/  440 batches | ms/batch 56.08 | loss  5.01 | ppl   150.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 25.51s | valid loss  5.03 | valid ppl   152.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |   250/  440 batches | ms/batch 56.07 | loss  4.96 | ppl   142.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 25.50s | valid loss  5.02 | valid ppl   151.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |   250/  440 batches | ms/batch 55.98 | loss  4.94 | ppl   139.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 25.45s | valid loss  5.01 | valid ppl   149.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |   250/  440 batches | ms/batch 56.05 | loss  4.92 | ppl   136.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 25.49s | valid loss  5.00 | valid ppl   149.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |   250/  440 batches | ms/batch 56.10 | loss  4.90 | ppl   134.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 25.51s | valid loss  4.99 | valid ppl   147.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |   250/  440 batches | ms/batch 55.95 | loss  4.88 | ppl   131.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 25.45s | valid loss  4.98 | valid ppl   146.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |   250/  440 batches | ms/batch 56.10 | loss  4.87 | ppl   129.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 25.52s | valid loss  4.98 | valid ppl   145.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |   250/  440 batches | ms/batch 56.10 | loss  4.85 | ppl   127.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 25.49s | valid loss  4.97 | valid ppl   143.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |   250/  440 batches | ms/batch 55.99 | loss  4.83 | ppl   125.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 25.47s | valid loss  4.97 | valid ppl   144.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  24 |   250/  440 batches | ms/batch 56.00 | loss  4.83 | ppl   125.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 25.46s | valid loss  4.94 | valid ppl   139.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |   250/  440 batches | ms/batch 55.96 | loss  4.82 | ppl   123.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 25.46s | valid loss  4.94 | valid ppl   139.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  26 |   250/  440 batches | ms/batch 56.09 | loss  4.81 | ppl   122.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 25.50s | valid loss  4.93 | valid ppl   138.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  27 |   250/  440 batches | ms/batch 56.09 | loss  4.81 | ppl   122.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 25.50s | valid loss  4.93 | valid ppl   138.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  28 |   250/  440 batches | ms/batch 55.98 | loss  4.80 | ppl   121.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 25.47s | valid loss  4.93 | valid ppl   138.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  29 |   250/  440 batches | ms/batch 56.05 | loss  4.80 | ppl   120.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 25.49s | valid loss  4.93 | valid ppl   138.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  30 |   250/  440 batches | ms/batch 56.13 | loss  4.79 | ppl   120.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 25.52s | valid loss  4.93 | valid ppl   138.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  31 |   250/  440 batches | ms/batch 56.16 | loss  4.79 | ppl   120.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 25.52s | valid loss  4.93 | valid ppl   138.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  32 |   250/  440 batches | ms/batch 56.05 | loss  4.78 | ppl   119.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 25.50s | valid loss  4.92 | valid ppl   137.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  33 |   250/  440 batches | ms/batch 56.13 | loss  4.78 | ppl   118.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 25.51s | valid loss  4.92 | valid ppl   137.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  34 |   250/  440 batches | ms/batch 56.05 | loss  4.78 | ppl   118.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 25.50s | valid loss  4.92 | valid ppl   137.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  35 |   250/  440 batches | ms/batch 56.17 | loss  4.77 | ppl   117.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 25.53s | valid loss  4.92 | valid ppl   136.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  36 |   250/  440 batches | ms/batch 55.97 | loss  4.77 | ppl   117.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 25.45s | valid loss  4.92 | valid ppl   136.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  37 |   250/  440 batches | ms/batch 56.01 | loss  4.76 | ppl   117.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 25.47s | valid loss  4.92 | valid ppl   136.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  38 |   250/  440 batches | ms/batch 56.10 | loss  4.76 | ppl   116.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 25.51s | valid loss  4.91 | valid ppl   136.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  39 |   250/  440 batches | ms/batch 55.96 | loss  4.76 | ppl   116.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 25.49s | valid loss  4.91 | valid ppl   136.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  40 |   250/  440 batches | ms/batch 56.02 | loss  4.75 | ppl   115.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 25.49s | valid loss  4.91 | valid ppl   135.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss  4.85 | test ppl   128.01\n",
            "=========================================================================================\n"
          ]
        }
      ],
      "source": [
        "run_training(lstm_model, epochs, learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-04T09:18:42.426119Z",
          "iopub.status.idle": "2023-11-04T09:18:42.426690Z",
          "shell.execute_reply": "2023-11-04T09:18:42.426368Z",
          "shell.execute_reply.started": "2023-11-04T09:18:42.426340Z"
        },
        "id": "VAF-jQjcw3jt",
        "outputId": "911c1158-09ad-4c5a-e70f-5f561b097f74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  83,  427,   16, 1127,   26,    0], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "input = tokenise_prompt('The meaning of life is', corpus).to(cuda)\n",
        "input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-04T09:18:42.431731Z",
          "iopub.status.idle": "2023-11-04T09:18:42.432245Z",
          "shell.execute_reply": "2023-11-04T09:18:42.432010Z",
          "shell.execute_reply.started": "2023-11-04T09:18:42.431960Z"
        },
        "id": "Jrjt7DXuw3ju",
        "outputId": "b0c2c3d9-bba6-4c65-9e93-6ee4427d7691",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2308]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "input2 = torch.randint(ntokens, (1, 1), dtype=torch.long).to(cuda)\n",
        "input2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-04T09:18:42.437035Z",
          "iopub.status.idle": "2023-11-04T09:18:42.437539Z",
          "shell.execute_reply": "2023-11-04T09:18:42.437296Z",
          "shell.execute_reply.started": "2023-11-04T09:18:42.437270Z"
        },
        "id": "t5_chYqZw3ju"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, prompt, output_file, corpus=corpus, temp=1, device=cuda):\n",
        "    model.eval()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    if model.model_type != 'Transformer':\n",
        "        hidden = model.init_hidden(1)\n",
        "        # lstm_model.flatten_parameters()\n",
        "    model.to(cuda)\n",
        "    # input = tokenise_prompt(prompt, corpus).to(device)\n",
        "    input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(cuda)\n",
        "    with open(output_file, 'w') as outf:\n",
        "        with torch.no_grad():\n",
        "            for i in range(1000):\n",
        "                if model.model_type == 'Transformer':\n",
        "                    output = model(input, False)\n",
        "                    word_weights = output[-1].squeeze().div(temp).exp().cpu()\n",
        "                    word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "                    word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
        "                    input = torch.cat([input, word_tensor], 0)\n",
        "                else:\n",
        "                    output, hidden = model(input, hidden)\n",
        "                    word_weights = output.squeeze().div(temp).exp().cpu()\n",
        "                    word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "                    input.fill_(word_idx)\n",
        "\n",
        "                word = corpus.dictionary.idx2word[word_idx]\n",
        "\n",
        "                outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
        "                if i % 100 == 0:\n",
        "                    print('| Generated {}/{} words'.format(i, 1000))\n",
        "            print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-04T09:18:42.442615Z",
          "iopub.status.idle": "2023-11-04T09:18:42.443124Z",
          "shell.execute_reply": "2023-11-04T09:18:42.442903Z",
          "shell.execute_reply.started": "2023-11-04T09:18:42.442867Z"
        },
        "id": "kicIGU_Rw3ju"
      },
      "outputs": [],
      "source": [
        "prompt = \"the best place to buy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-04T09:18:42.448085Z",
          "iopub.status.idle": "2023-11-04T09:18:42.448572Z",
          "shell.execute_reply": "2023-11-04T09:18:42.448368Z",
          "shell.execute_reply.started": "2023-11-04T09:18:42.448344Z"
        },
        "id": "JbYUXXp7w3ju"
      },
      "outputs": [],
      "source": [
        "if transformer_model == None:\n",
        "    transformer_model = torch.load(os.path.join(models_dir, 'Transformer.pt'), map_location=device)\n",
        "if lstm_model == None:\n",
        "    lstm_model = torch.load(os.path.join(models_dir, 'Transformer.pt'), map_location=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-04T09:18:42.452939Z",
          "iopub.status.idle": "2023-11-04T09:18:42.453385Z",
          "shell.execute_reply": "2023-11-04T09:18:42.453180Z",
          "shell.execute_reply.started": "2023-11-04T09:18:42.453156Z"
        },
        "id": "wXmPp2yMw3ju",
        "outputId": "1b3dc3c3-49a6-4479-cfd6-77ca122d2586",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Generated 0/1000 words\n",
            "| Generated 100/1000 words\n",
            "| Generated 200/1000 words\n",
            "| Generated 300/1000 words\n",
            "| Generated 400/1000 words\n",
            "| Generated 500/1000 words\n",
            "| Generated 600/1000 words\n",
            "| Generated 700/1000 words\n",
            "| Generated 800/1000 words\n",
            "| Generated 900/1000 words\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "generate_text(transformer_model, prompt, './output/transformer_gen.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-04T09:18:42.458171Z",
          "iopub.status.idle": "2023-11-04T09:18:42.458662Z",
          "shell.execute_reply": "2023-11-04T09:18:42.458418Z",
          "shell.execute_reply.started": "2023-11-04T09:18:42.458393Z"
        },
        "id": "yHNvkLTnw3jv",
        "outputId": "64d54603-838a-4ab7-805a-0bd7cb5d3d83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Generated 0/1000 words\n",
            "| Generated 100/1000 words\n",
            "| Generated 200/1000 words\n",
            "| Generated 300/1000 words\n",
            "| Generated 400/1000 words\n",
            "| Generated 500/1000 words\n",
            "| Generated 600/1000 words\n",
            "| Generated 700/1000 words\n",
            "| Generated 800/1000 words\n",
            "| Generated 900/1000 words\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "generate_text(lstm_model, prompt, './output/lstm_gen.txt')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}