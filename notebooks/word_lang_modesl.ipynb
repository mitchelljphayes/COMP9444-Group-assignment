{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "from io import open\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "nhead = 16\n",
    "ninp = 256\n",
    "em_size = 256\n",
    "nhid = 256\n",
    "nlayers = 2\n",
    "dropout = 0.2\n",
    "bptt = 35\n",
    "batch_size = 20\n",
    "learning_rate = 1e-3\n",
    "clip = 0.25\n",
    "epochs = 3\n",
    "log_interval = 500\n",
    "onnx_export_dir = '../models/'\n",
    "data_dir = '../data/wikitext'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Taken from https://github.com/pytorch/examples/blob/main/word_language_model/data.py on 01/11/2023\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "        self.counter = 0\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if word not in self.idx2word:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = self.counter\n",
    "            self.counter += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise_prompt(input, corpus):\n",
    "    words = input.split() + ['<eos>']\n",
    "    ids = torch.LongTensor(len(words))\n",
    "    for i, word in enumerate(words):\n",
    "        ids[i] = corpus.dictionary.word2idx[word]\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Taken from https://github.com/pytorch/examples/blob/main/word_language_model/data.py on 01/11/2023\n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        assert os.path.exists(path)\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "            \n",
    "            ids = torch.LongTensor(tokens)\n",
    "            token = 0\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    words = line.split() + ['<eos>']\n",
    "                    for word in words:\n",
    "                        ids[token] = self.dictionary.word2idx[word]\n",
    "                        token += 1\n",
    "            \n",
    "            return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir('data_dir')\n",
    "    data = load_dataset('wikitext', 'wikitext-2-v1')\n",
    "    data.save_to_disk(os.path.join(data_dir, 'wikitext-2'))\n",
    "data = load_dataset('wikitext', 'wikitext-2-v1', data_dir=data_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 36718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting from HF dataset to plain text files can probably improve this\n",
    "train_data = '\\n'.join(data['train']['text'])\n",
    "valid_data = '\\n'.join(data['validation']['text'])\n",
    "test_data = '\\n'.join(data['test']['text'])\n",
    "with open(os.path.join(data_dir, 'train.txt'), 'w') as f:\n",
    "    f.write(train_data)\n",
    "with open(os.path.join(data_dir, 'valid.txt'), 'w') as f:\n",
    "    f.write(valid_data)\n",
    "with open(os.path.join(data_dir, 'test.txt'), 'w') as f:\n",
    "    f.write(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    nbatch = data.size(0) // bsz\n",
    "    data = data.narrow(0, 0, nbatch*bsz)\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_batch_size = 10\n",
    "train_data = batchify(corpus.train, 20)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.lstm = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "        self.init_weights()\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.ntokens = ntoken\n",
    "        self.model_type = 'LSTM' \n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.lstm(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output)\n",
    "        decoded = decoded.view(-1, self.ntokens)\n",
    "        return F.log_softmax(decoded, dim=1), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "                weight.new_zeros(self.nlayers, bsz, self.nhid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoisitionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PoisitionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2)*(-math.log(10000.0)/d_model))\n",
    "        pe[:, 0::2] = torch.sin(position*div_term)\n",
    "        pe[:, 1::2] = torch.cos(position*div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Transformer):\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__(d_model=ninp, nhead=nhead, dim_feedforward=nhid, num_decoder_layers=nlayers)\n",
    "        # self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PoisitionalEncoding(ninp, dropout)\n",
    "\n",
    "        self.input_emb = nn.Embedding(ntoken, ninp)\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "        self.ninp = ninp\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.ntokens = ntoken\n",
    "        self.model_type = 'Transformer'\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz) == 1).transpose(0, 1))\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.input_emb.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def forward(self, src, has_mask=True):\n",
    "        if has_mask:\n",
    "            device = src.device\n",
    "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "                self.src_mask = mask\n",
    "        else:\n",
    "            self.src_mask = None\n",
    "        \n",
    "        src = self.input_emb(src)*math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.encoder(src, mask=self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return F.log_softmax(output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mjp/.local/share/virtualenvs/group-assignment-f09OljWU/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "transformer_model = TransformerModel(ntoken=ntokens, ninp=ninp, nhead=nhead, nhid=nhid, nlayers=nlayers, dropout=dropout).to(device)\n",
    "lstm_model = LSTMModel(ntoken=ntokens, ninp=ninp, nhid=nhid, nlayers=nlayers,dropout=dropout).to(device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source)-1-i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_type, model, data_source):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    if model_type != 'Transformer':\n",
    "        hidden = model.init_hidden(eval_batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0)-1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            if model_type == 'Transformer':\n",
    "                output = model(data)\n",
    "                output = output.view(-1, ntokens)\n",
    "            else:\n",
    "                output, hidden = model(data, hidden)\n",
    "                hidden = repackage_hidden(hidden)\n",
    "            total_loss += len(data)*criterion(output, targets).item()\n",
    "    return total_loss/(len(data_source)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_type, model, epoch, lr):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    if model_type != 'Transformer':\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0)-1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        model.zero_grad()\n",
    "        if model_type == 'Transformer':\n",
    "            output = model(data)\n",
    "            output = output.view(-1, ntokens)\n",
    "        else:\n",
    "            output, hidden = model(data, hidden)\n",
    "            hidden = repackage_hidden(hidden)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(p.grad, alpha=-lr)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch%log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss/log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                        epoch, batch, len(train_data)//bptt, elapsed*1000/log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_onnx(path, model, batch_size, seq_len):\n",
    "    model.eval()\n",
    "    x = torch.rand(seq_len, batch_size).to(device)\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    torch.onnx.export(model, (x, hidden), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model_type, model, epochs, learning_rate):\n",
    "    lr = learning_rate\n",
    "    best_val_loss = None\n",
    "    try:\n",
    "        for epoch in range(1, epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            train(model_type, model, epoch, lr)\n",
    "            val_loss = evaluate(model_type, model, val_data)\n",
    "            print('-'*89)\n",
    "            print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                    'valid ppl {:8.2f}'.format(epoch, (time.time()-epoch_start_time), val_loss, math.exp(val_loss)))\n",
    "            print('-'*89)\n",
    "            if not best_val_loss or val_loss < best_val_loss:\n",
    "                with open((model_type + '.pt'), 'wb') as f:\n",
    "                    torch.save(model, f)\n",
    "                best_val_loss = val_loss\n",
    "            else:\n",
    "                lr /= 4.0\n",
    "    except KeyboardInterrupt:\n",
    "        print('-'*89)\n",
    "        print('Exiting from training early')\n",
    "    \n",
    "    with open((model_type + '.pt'), 'rb') as f:\n",
    "        model.torch.load(f)\n",
    "        if model_type == 'LSTM':\n",
    "            model.flatten_parameters()\n",
    "\n",
    "    test_loss = evaluate(model_type, model, test_data)\n",
    "    print('='*89)\n",
    "    print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(test_loss, math.exp(test_loss)))\n",
    "    print('='*89)\n",
    "    export_onnx(os.path.join(onnx_export_dir, model_type), model, batch_size=1, seq_len=bptt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_training('Transformer', transformer_model, epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_training('LSTM', lstm_model, epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  83,  427,   16, 1127,   26,    0])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = tokenise_prompt('The meaning of life is', corpus)\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, prompt, corpus=corpus, temp=0.4, device=device):\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    if model.model_type != 'Transformer':\n",
    "        hidden = model.init_hidden(1)\n",
    "        # lstm_model.flatten_parameters()\n",
    "    input = tokenise_prompt(prompt)\n",
    "    with open('generated.txt', 'w') as f:\n",
    "        with torch.no_grad():\n",
    "            for i in range(1000):\n",
    "                if model.model_type == 'Transformer':\n",
    "                    output = model(input, False)\n",
    "                    word_weights = output[-1].squeeze().div(temp).exp().cpu()\n",
    "                    word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "                    word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
    "                    input = torch.cat([input, word_tensor], 0)\n",
    "                else:\n",
    "                    output, hidden = model(input, hidden)\n",
    "                    word_weights = output.squeeze().div(temp).exp().cpu()\n",
    "                    word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "                    input.fill_(word_idx)\n",
    "                word = corpus.dictionary.idx2word[word_idx]\n",
    "                f.write(word + ('\\n' if i%20 == 19 else ' '))\n",
    "                if i % log_interval == 0:\n",
    "                    print('| Generated {}/{} words'.format(i, 1000))\n",
    "            print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'model_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/notebooks/lstm_lang_model.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/notebooks/lstm_lang_model.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m generate_text(\u001b[39m'\u001b[39;49m\u001b[39mTransformer.pt\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mTransformer\u001b[39;49m\u001b[39m'\u001b[39;49m, corpus, device)\n",
      "\u001b[1;32m/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/notebooks/lstm_lang_model.ipynb Cell 27\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/notebooks/lstm_lang_model.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_text\u001b[39m(model, prompt, corpus\u001b[39m=\u001b[39mcorpus, temp\u001b[39m=\u001b[39m\u001b[39m0.4\u001b[39m, device\u001b[39m=\u001b[39mdevice):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/notebooks/lstm_lang_model.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     ntokens \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(corpus\u001b[39m.\u001b[39mdictionary)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/notebooks/lstm_lang_model.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39;49mmodel_type \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mTransformer\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/notebooks/lstm_lang_model.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         hidden \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39minit_hidden(\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/notebooks/lstm_lang_model.ipynb#X40sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39m# lstm_model.flatten_parameters()\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'model_type'"
     ]
    }
   ],
   "source": [
    "generate_text('Transformer.pt', 'Transformer', corpus, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_text('LSTM.pt', 'LSTM', corpus, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "group-assignment-f09OljWU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
