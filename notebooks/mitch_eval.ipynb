{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import transformers\n",
    "import bert_score\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from bert_score import score, BERTScorer\n",
    "# from torchmetrics.functional.text.bert import bert_score\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "transformers.tokenization_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.configuration_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.modeling_utils.logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "bert_score.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams[\"xtick.major.size\"] = 0\n",
    "rcParams[\"xtick.minor.size\"] = 0\n",
    "rcParams[\"ytick.major.size\"] = 0\n",
    "rcParams[\"ytick.minor.size\"] = 0\n",
    "\n",
    "rcParams[\"axes.labelsize\"] = \"large\"\n",
    "rcParams[\"axes.axisbelow\"] = True\n",
    "rcParams[\"axes.grid\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = '..'\n",
    "# dataset_name = 'vblagoje/lfqa_support_docs'\n",
    "# dir_name = 'lfqa'\n",
    "dir_name = 'eval'\n",
    "dataset_name = 'mjphayes/machine_learning_questions'\n",
    "# dataset_name = 'wikitext'\n",
    "# dataset_variant = 'wikitext-2-raw-v1'\n",
    "# dir_name = 'wikitext'\n",
    "data_dir = os.path.join(project_root, 'data', dir_name)\n",
    "# model_checkpoint = 'gpt2'\n",
    "model_checkpoint = 'distilgpt2'\n",
    "# model_checkpoint = 'mjphayes/distilgpt2-finetuned-textbook_dataset'\n",
    "model_name = model_checkpoint.split('/')[-1]\n",
    "model_dir = os.path.join(project_root, 'models', model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    datasets = load_dataset(dataset_name, dataset_variant, cache_dir=data_dir)\n",
    "except:\n",
    "    datasets = load_dataset(dataset_name, cache_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transform(examples):\n",
    "#     return {\"question\": examples['question'], \"answer\": examples['answer']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = test_data.map(transform, num_proc=4, remove_columns=[\"input\", \"output\", \"meta\", \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, cache_dir=model_dir, use_fast=True, padding_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint, cache_dir=model_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(input, model=model):\n",
    "    prompt = f\"Question: {input}? \\n Answer:\"\n",
    "    model_inputs = tokenizer([prompt], return_tensors=\"pt\", padding='max_length', max_length=200)\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=300, do_sample=True)\n",
    "    bable = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return bable.split('Question: ')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 22\n",
    "test_answer = generation(test_data[num]['question'])\n",
    "test_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini_val = test_data.select(range(20))\n",
    "# print(len(mini_val))\n",
    "# mini_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "ideal_answers = []\n",
    "gen_answers = []\n",
    "for q_a in test_data:\n",
    "    question = q_a['question']\n",
    "    gen_answer = generation(question).split('Answer: ')[-1]\n",
    "    ideal = q_a['answer']\n",
    "    questions.append(question)\n",
    "    ideal_answers.append(ideal)\n",
    "    gen_answers.append(gen_answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Questions': questions,\n",
    "    'Ideal': ideal_answers,\n",
    "    'Generated':gen_answers,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal = ideal_answers\n",
    "generated = gen_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, R, F1 = score(gen_answers, ideal_answers, lang='en', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"System level F1 score: {F1.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(F1, bins=20)\n",
    "plt.xlabel(\"score\")\n",
    "plt.ylabel(\"counts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag_dataset = load_dataset('domnasrabadi/falcon7bRAG_eval')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = rag_dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'falcon7b_baseline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df = pd.read_csv('nx_7binstruct_baseline_output2.csv')\n",
    "df = pd.read_csv('../eval/distilgpt2_eval.csv')\n",
    "model_name = 'distilgpt2'\n",
    "# df = pd.read_csv('../eval/distilgpt2-finetuned-textbook_dataset_eval.csv')\n",
    "# model_name = 'distilgpt2-finetuned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(['Unnamed: 0', 'Unnamed: 0.1'], inplace=True, axis=1)\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = BERTScorer(lang=\"en\", rescale_with_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, R, F1 = scorer.score(ideal, generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated = df['model_answer'].tolist()\n",
    "# ideal = df['answer'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BERTScore_f1'] = F1\n",
    "df['BERTScore_precision'] = P\n",
    "df['BERTScore_recall'] = R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(os.path.join('../eval', f'falcon7b_rag_eval.csv'), index=False)\n",
    "df.to_csv(os.path.join('../eval', f'{model_name}_eval.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"System level F1 score: {F1.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(F1, bins=20)\n",
    "plt.xlabel(\"score\")\n",
    "plt.ylabel(\"counts\")\n",
    "plt.suptitle(\"BERTScore for distilgpt2\")\n",
    "plt.savefig('../eval/bert_score_distilgpt2.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use a pipeline as a high-level helper\n",
    "# from transformers import pipeline\n",
    "\n",
    "# pipe = pipeline(\"text-generation\", model=\"TheBloke/Llama-2-7B-Chat-GPTQ\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
