{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "import faiss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model we are using \n",
    "\n",
    "`llama-2-7b-chat.Q5_K_M.gguf`\n",
    "\n",
    "### Testing LLaMa without Langchain\n",
    "\n",
    "this is powered by the python bindings provided by `llama-cpp-python`.   \n",
    "2 of the most common calls/functions are the `init` and `__call__` methods e.g. `llama()`   \n",
    "\n",
    "Here are some args you should know for `init`:\n",
    "\n",
    "| init_arg | description | default |\n",
    "| --- | --- | --- |\n",
    "| model_path | Path to the model. | required |\n",
    "| n_ctx | Maximum context size. | 512 |\n",
    "| seed | Random seed. 0 for random. | 1337 |\n",
    "| embedding | Embedding mode only. | False |\n",
    "| n_threads | Number of threads to use.\t | auto-determined |\n",
    "| n_batch | Maximum number of prompt tokens to batch together | None |\n",
    "| verbose | Print verbose output to stderr. | True |\n",
    "  \n",
    "And for the `_call__` method :  \n",
    "\n",
    "| call_arg | description | default |\n",
    "| --- | --- | --- |\n",
    "| prompt | The prompt to generate text from. | required |\n",
    "| suffix | suffix to append to the generated text | none |\n",
    "| stop | list of strings to stop generation when encountered. | none |\n",
    "| max_tokens | The maximum number of tokens to generate. | 128 |\n",
    "| temperature | The temperature to use for sampling. | 0.8 |\n",
    "| logprobs | The number of logprobs to return.\t | none |\n",
    "| top_p | The top-p value to use for sampling. | 0.95 |\n",
    "| echo | Whether to echo the prompt. | False |   \n",
    "\n",
    "see the docs for more: https://abetlen.github.io/llama-cpp-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain: Checking for metal (mps) is working on mac\n",
    "\n",
    "this code below is from langchain llama cpp docs and confirms if metal works. \n",
    "\n",
    "\n",
    "```\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"llama.cpp/models/llama-2-7b-chat.Q5_K_M.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    callback_manager = callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")\n",
    "```\n",
    "\n",
    "The console log will show the following log to indicate Metal was enable properly.\n",
    "\n",
    "`ggml_metal_init: allocating`    \n",
    "`ggml_metal_init: using MPS`   \n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run this once otherwise you are wasting resources\n",
    "    # verbose = False -> suppresses all the extra timing + stats info it gives you\n",
    "\n",
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=\"./models/falcon\", verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm(\"Q: How old is Elon Musk? \\nA: \", \n",
    "             max_tokens=100, \n",
    "             stop=[\"Q:\", \"\\n\"], \n",
    "             temperature=0.5,\n",
    "             echo=True)\n",
    "             \n",
    "\n",
    "print(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using RAG - QA with PDFs in a Vector Database\n",
    "\n",
    "I tried multiple packages.   \n",
    "*`PyPDFLoader`* - formatting issues   \n",
    "*`UnstructuredPDFLoader`* - formatting issues    \n",
    "*`PDFMinerLoader`* - works but slow  \n",
    "**`PyMuPDFLoader`** - **best**  \n",
    "\n",
    "### Langchain LLaMa CPP options\n",
    "\n",
    "https://python.langchain.com/docs/integrations/llms/llamacpp   \n",
    "https://python.langchain.com/docs/integrations/text_embedding/llamacpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings() # llama embeddings dont work for some reason\n",
    "\n",
    "loader = PyMuPDFLoader(\"Deep Learning.pdf\")\n",
    "data = loader.load()\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = FAISS.from_documents(data, embeddings)\n",
    "#db.save_local(\"faiss_deeplearning_chap6\")  # only need to do this once\n",
    "#db = FAISS.load_local(\"faiss_deeplearning_chap6/\", embeddings=llama)\n",
    "vectordb.save_local(\"faiss_deeplearning_book\")  # only need to do this once\n",
    "vectordb = FAISS.load_local(\"faiss_deeplearning_book/\", embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "template = template = \"\"\"\n",
    "Question: {question}\"\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "\n",
    "llm = LlamaCpp(\n",
    "                #model_path=\"llama.cpp/models/llama-2-7b-chat.Q5_K_M.gguf\", \n",
    "                model_path = 'llama.cpp/models/tiiuae-falcon-7b-Q4_K_S.gguf',\n",
    "               verbose=False,\n",
    "               max_tokens = 4500,\n",
    "               n_ctx=6000,)\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = pd.read_parquet(\"../evals.parquet\")\n",
    "\n",
    "for i in evals[:5].iterrows():\n",
    "    print(i[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimised_query(query):\n",
    "    sim_search_prompt = f\"\"\"\n",
    "    CONTEXT: You are a helpful assistant that turns raw user queries into a version optimized for finding relevant documents. \n",
    "    Below is a User Query, please respond with an optimized version of their query and only the optimized query. Try to summarise\n",
    "    the query so it can easily be searched in a vector space for the most similar and appropriate context.\n",
    "    Do not repeat the optimised query in your response, just make it as concise as possible.\n",
    "\n",
    "    USER QUERY: {query} \n",
    "\n",
    "    OPTIMISED QUERY RESPONSE:  \n",
    "    \"\"\"\n",
    "    output = llm(sim_search_prompt, \n",
    "                max_tokens=100, \n",
    "                stop=[\".\\n\", \". \\n\"], \n",
    "                temperature=0.1,\n",
    "                echo=False)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimised_query(\"What is a layer in a neural network?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(query):    \n",
    "    search = vectordb.similarity_search(query, k=1)\n",
    "    sourcedocs = search[0].page_content\n",
    "    \n",
    "    prompt = f\"\"\" \n",
    "    Context: {sourcedocs}\n",
    "\n",
    "    Based on Context provide me answer for following question\n",
    "    Question: {query}\n",
    "\n",
    "    Tell me the information about the fact. The answer should be from context only\n",
    "    do not use general knowledge to answer the query. \n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    output = llm(prompt, \n",
    "                max_tokens=300, \n",
    "                stop=[\".\\n\", \". \\n\"], \n",
    "                temperature=0.1,\n",
    "                echo=False)\n",
    "    return output, sourcedocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_answer(\"What is a layer in a neural network?\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "evals = pd.read_parquet(\"../evals.parquet\")\n",
    "# print(evals.head(1)['question'].values[0])\n",
    "# print(evals.head(1)['answer'].values[0])\n",
    "\n",
    "evals_dict = [] # create a list of dictionaries for our output format\n",
    "\n",
    "for i in evals.iterrows():\n",
    "    eval_question = i[1][0]\n",
    "\n",
    "    # below we index to 0 for answer, you can also get source documents with index 1\n",
    "    llm_response = get_answer(eval_question)[0]\n",
    "\n",
    "    question_dict = {\n",
    "        'question':eval_question, \n",
    "        'llm_answer': llm_response\n",
    "    }\n",
    "    evals_dict.append(question_dict)\n",
    "\n",
    "    if i[0]%20 == 0:\n",
    "        print(f\"{i[0]} questions done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_responses_list = [i['llm_answer'] for i in evals_dict]\n",
    "print(len(llm_responses_list))\n",
    "\n",
    "evals_w_preds = evals.copy()\n",
    "evals_w_preds['llm_response'] = llm_responses_list\n",
    "evals_w_preds.to_parquet('FALCON_RAG_EVALS.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_search_prompt = f\"\"\"\n",
    "CONTEXT: You are a helpful assistant that turns raw user queries into a version optimized for finding relevant documents. \n",
    "Below is a User Query, please respond with an optimized version of their query and only the optimized query. \n",
    "Do not repeat the optimised query in your response, just make it as concise as possible.\n",
    "\n",
    "USER QUERY: {evals.head(1)['question'].values[0]} \n",
    "\n",
    "OPTIMISED QUERY RESPONSE:  \n",
    "\"\"\"\n",
    "output = llm(sim_search_prompt, \n",
    "             max_tokens=100, \n",
    "             stop=[\".\\n\", \". \\n\"], \n",
    "             temperature=0.1,\n",
    "             echo=False)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = vectordb.similarity_search(output, k=1)\n",
    "query = \"What is 'natural language processing' (NLP) in machine learning?\"\n",
    "\n",
    "template = '''Context: {context}\n",
    "Based on Context provide me answer for following question\n",
    "Question: {question}\n",
    "Tell me the information about the fact. The answer should be from context only\n",
    "do not use general knowledge to answer the query'''\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template= template)\n",
    "final_prompt = prompt.format(question=query, context=search)\n",
    "llm_chain.run(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = vectordb.similarity_search(output, k=1)\n",
    "query = \"What is 'natural language processing' (NLP) in machine learning?\"\n",
    "\n",
    "prompt = f\"\"\" \n",
    "Context: {search[0].page_content}\n",
    "\n",
    "Based on Context provide me answer for following question\n",
    "Question: {query}\n",
    "\n",
    "Tell me the information about the fact. The answer should be from context only\n",
    "do not use general knowledge to answer the query. \n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "output = llm(prompt, \n",
    "             max_tokens=300, \n",
    "             stop=[\".\\n\", \". \\n\"], \n",
    "             temperature=0.1,\n",
    "             echo=False)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation QA\n",
    "from langchain.chains import ConversationalRetrievalChain \n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(llm, vectordb.as_retriever(), return_source_documents = True)\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "query = \"What is deep learning?\"\n",
    "result = chain({\n",
    "    'question':query, \n",
    "    'chat_history':chat_history\n",
    "})\n",
    "\n",
    "print(result['answer'])\n",
    "print(result['source_documents'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first load faiss db \n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"\n",
    "### Human: I would like a summary of this document please.\n",
    "### Assistant: \n",
    "\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm, \n",
    "    retriever = vectordb.as_retriever()\n",
    ")\n",
    "\n",
    "result = qa_chain(template)\n",
    "result['result']"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
