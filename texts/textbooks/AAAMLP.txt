Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
  
Approaching (Almost) Any Machine 
Learning 
Problem
1 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
It would not have been possible for me to write this book without the  support of my family and friends. I would also like to thank the reviewers  who selflessly devoted their time in reviewing this book (names in  alphabetical order). 
Aakash Nain 
Aditya Soni 
Andreas Müller 
Andrey Lukyanenko 
Ayon Roy 
Bojan Tunguz 
Gilberto Titericz Jr. 
Konrad Banachewicz 
Luca Massaron 
Nabajeet Barman 
Parul Pandey 
Ram Ramrakhya 
Sanyam Bhutani 
Sudalai Rajkumar 
Tanishq Abraham 
Walter Reade 
Yuval Reina 
I hope I did not miss anyone.
2 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
Before you start, there are a few things that you must be aware of while going  through this book.  
This is not a traditional book. 
The book expects you to have basic knowledge of machine learning and deep  learning.  
Important terms are bold.  
Variable names and function/class names are italic.  
═════════════════════════════════════════════════════════════════════════ All the code is between these two lines 
═════════════════════════════════════════════════════════════════════════ Most of the times, the output is provided right after the code blocks. Figures are locally defined. For example, figure 1 is the first figure  
Code is very important in this book and there is a lot of it. You must go through  the code carefully and implement it on your own if you want to understand what’s  going on. 
Comments in Python begin with a hash (#). All the code in this book is explained  line-by-line only using comments. Thus, these comments must not be ignored. 
Bash commands start with $ or ❯. 
If you find a pirated copy of this book (print or e-book or pdf), contact me directly  with the details so that I can take necessary actions. 
If you didn’t code, you didn’t learn.
3 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
Table of Contents 
Setting up your working environment .....................................................5 Supervised vs unsupervised learning.......................................................7 Cross-validation....................................................................................14 Evaluation metrics................................................................................30 Arranging machine learning projects ...................................................73 Approaching categorical variables........................................................85 Feature engineering ...........................................................................142 Feature selection ................................................................................155 Hyperparameter optimization .............................................................167 Approaching image classification & segmentation..............................185 Approaching text classification/regression..........................................225 Approaching ensembling and stacking................................................272 Approaching reproducible code & model serving................................283
4 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
Setting up your working environment 
Before we begin with coding, it’s essential to get everything set-up on your  machine. Throughout this book, we will be using Ubuntu 18.04 and Python 3.7.6.  If you are a Windows user, you can install Ubuntu in multiple ways. On a virtual  machine, for example, Virtual Box which is provided by Oracle and is free software.  Alongside Windows as a dual boot system. I prefer dual boot as it is native. If you  are not an Ubuntu user, you might face problems with some of the bash scripts in  this book. To circumvent that you can install Ubuntu in a VM or go for Linux shell  on Windows. 
Setting up Python on any machine is quite easy with Anaconda. I particularly like  Miniconda, which is a minimal installer for conda. It is available for Linux, OSX  and Windows. Since Python 2 support ended at the end of 2019, we will be using  the Python 3 distribution. You should keep in mind that miniconda does not come  with all the packages as regular Anaconda. We will, thus, be installing packages as  we go. Installing miniconda is quite easy. 
The first thing that you need to do is download Miniconda3 to your system. 
$ cd ~/Downloads 
$ wget https://repo.anaconda.com/miniconda/... 
where the URL after wget command is the URL from miniconda3 webpage. For  64-bit Linux systems, the URL at the time of writing this book was: 
https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh Once you have downloaded miniconda3, you can run the following command: $ sh Miniconda3-latest-Linux-x86_64.sh 
Next, please read and follow the instructions on your screen. If you installed  everything correctly, you should be able to start the conda environment by typing  conda init the terminal. We will create a conda environment that we will be using  throughout this book. To create a conda environment, you can type: 
$ conda create -n environment_name python=3.7.6
5 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
This command will create a conda environment named environment_name which  can be activated using: 
$ conda activate environment_name 
And we are all set-up with the environment. Now it’s time to install some packages  that we would be using. A package can be installed in two different ways when you  are in a conda environment. You can either install the package from conda  repository or the official PyPi repository. 
$ conda/pip install package_name 
Note: It might be possible that some packages are not available in the conda repo.  Thus, installing using pip would be the most preferred way in this book. I have  already created a list of packages used while writing this book which is saved in the  environment.yml. You can find it in extra material available in my GitHub  repository. You can create the environment using the following command: 
$ conda env create -f environment.yml 
This command will create an environment called ml. To activate this environment  and start using it, you should run: 
$ conda activate ml 
And we are all set and ready to do some applied machine learning!  Always remember to be in the “ml” environment when coding along with this book. Let’s start with our real first chapter now.
6 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
Supervised vs unsupervised learning 
When dealing with machine learning problems, there are generally two types of  data (and machine learning models): 
• Supervised data: always has one or multiple targets associated with it. • Unsupervised data: does not have any target variable. 
A supervised problem is considerably easier to tackle than an unsupervised one. A  problem in which we are required to predict a value is known as a supervised  problem. For example, if the problem is to predict house prices given historical  house prices, with features like presence of a hospital, school or supermarket,  distance to nearest public transport, etc. is a supervised problem. Similarly, when  we are provided with images of cats and dogs, and we know beforehand which ones  are cats and which ones are dogs, and if the task is to create a model which predicts  whether a provided image is of a cat or a dog, the problem is considered to be  supervised. 
  

Figure 1: A supervised dataset. 
As we see in figure 1, every row of the data is associated with a target or label. The  columns are different features and rows represent different data points which are  usually called samples. The example shows ten samples with ten features and a  target variable which can be either a number or a category. If the target is  categorical, the problem becomes a classification problem. And if the target is a real
7 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
number, the problem is defined as a regression problem. Thus, supervised problems  can be divided into two sub-classes: 
• Classification: predicting a category, e.g. dog or cat. 
• Regression: predicting a value, e.g. house prices. 
It must be noted that sometimes we might use regression in a classification setting  depending on the metric used for evaluation. But we will come to that later. 
Another type of machine learning problem is the unsupervised type. Unsupervised  datasets do not have a target associated with them and in general, are more  challenging to deal with when compared to supervised problems. 
Let’s say you work in a financial firm which deals with credit card transactions.  There is a lot of data that comes in every second. The only problem is that it is  difficult to find humans who will mark each and every transaction either as a valid  or genuine transaction or a fraud. When we do not have any information about a  transaction being fraud or genuine, the problem becomes an unsupervised problem.  To tackle these kinds of problems we have to think about how many clusters can  data be divided into. Clustering is one of the approaches that you can use for  problems like this, but it must be noted that there are several other approaches  available that can be applied to unsupervised problems. For a fraud detection  problem, we can say that data can be divided into two classes (fraud or genuine). 
When we know the number of clusters, we can use a clustering algorithm for  unsupervised problems. In figure 2, the data is assumed to have two classes, dark  colour represents fraud, and light colour represents genuine transactions. These  classes, however, are not known to us before the clustering approach. After a  clustering algorithm is applied, we should be able to distinguish between the two  assumed targets. To make sense of unsupervised problems, we can also use  numerous decomposition techniques such as Principal Component Analysis  (PCA), t-distributed Stochastic Neighbour Embedding (t-SNE) etc. 
Supervised problems are easier to tackle in the sense that they can be evaluated  easily. We will read more about evaluation techniques in the following chapters.  However, it is challenging to assess the results of unsupervised algorithms and a lot  of human interference or heuristics are required. In this book, we will majorly be  focusing on supervised data and models, but it does not mean that we will be  ignoring the unsupervised data problems.
8 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur   
Figure 2: An unsupervised dataset. 
Most of the time, when people start with data science or machine learning, they  begin with very well-known datasets, for example, Titanic dataset, or Iris dataset which are supervised problems. In the Titanic dataset, you have to predict the  survival of people aboard Titanic based on factors like their ticket class, gender,  age, etc. Similarly, in the iris dataset, you have to predict the species of flower based  on factors like sepal width, petal length, sepal length and petal width. 
Unsupervised datasets may include datasets for customer segmentation. For  example, you have data for the customers visiting your e-commerce website or the  data for customers visiting a store or a mall, and you would like to segment them  or cluster them in different categories. Another example of unsupervised datasets  may include things like credit card fraud detection or just clustering several images. 
Most of the time, it’s also possible to convert a supervised dataset to unsupervised  to see how they look like when plotted. 
For example, let’s take a look at the dataset in figure 3. Figure 3 shows MNIST  dataset which is a very popular dataset of handwritten digits, and it is a supervised  problem in which you are given the images of the numbers and the correct label  associated with them. You have to build a model that can identify which digit is it  when provided only with the image.  
This dataset can easily be converted to an unsupervised setting for basic  visualization. 
9 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur   
Figure 3: MNIST dataset1 
If we do a t-Distributed Stochastic Neighbour Embedding (t-SNE) decomposition  of this dataset, we can see that we can separate the images to some extent just by  doing with two components on the image pixels. This is shown in figure 4. 
  

Figure 4: t-SNE visualization of the MNIST dataset. 3000 images were used. 
Let’s take a look at how this was done. First and foremost is importing all the  required libraries. 
1 Image source: By Josef Steppan - Own work, CC BY-SA 4.0, 
https://commons.wikimedia.org/w/index.php?curid=64810040
10 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
═════════════════════════════════════════════════════════════════════════ import matplotlib.pyplot as plt 
import numpy as np 
import pandas as pd 
import seaborn as sns 
from sklearn import datasets 
from sklearn import manifold 
%matplotlib inline 
═════════════════════════════════════════════════════════════════════════ 
We use matplotlib and seaborn for plotting, numpy to handle the numerical arrays,  pandas to create dataframes from the numerical arrays and scikit-learn (sklearn) to  get the data and perform t-SNE. 
After the imports, we need to either download the data and read it separately or use  sklearn’s built-in function that provides us with the MNIST dataset. 
═════════════════════════════════════════════════════════════════════════ data = datasets.fetch_openml( 
 'mnist_784',  
 version=1,  
 return_X_y=True 
) 
pixel_values, targets = data 
targets = targets.astype(int) 
═════════════════════════════════════════════════════════════════════════ 
In this part of the code, we have fetched the data using sklearn datasets, and we  have an array of pixel values and another array of targets. Since the targets are of string type, we convert them to integers. 
pixel_values is a 2-dimensional array of shape 70000x784. There are 70000  different images, each of size 28x28 pixels. Flattening 28x28 gives 784 data points. 
We can visualize the samples in this dataset by reshaping them to their original  shape and then plotting them using matplotlib. 
═════════════════════════════════════════════════════════════════════════ single_image = pixel_values[1, :].reshape(28, 28) 
plt.imshow(single_image, cmap='gray') 
═════════════════════════════════════════════════════════════════════════
11 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
This code will plot an image like the following: 
  

Figure 5: Plotting a single image from MNIST dataset. 
The most important step comes after we have grabbed the data. 
═════════════════════════════════════════════════════════════════════════ tsne = manifold.TSNE(n_components=2, random_state=42) 
transformed_data = tsne.fit_transform(pixel_values[:3000, :]) ═════════════════════════════════════════════════════════════════════════ 
This step creates the t-SNE transformation of the data. We use only two components  as we can visualize them well in a two-dimensional setting. The transformed_data,  in this case, is an array of shape 3000x2 (3000 rows and 2 columns). A data like  this can be converted to a pandas dataframe by calling pd.DataFrame on the array. 
═════════════════════════════════════════════════════════════════════════ tsne_df = pd.DataFrame( 
np.column_stack((transformed_data, targets[:3000])),  
columns=["x", "y", "targets"] 
) 
tsne_df.loc[:, "targets"] = tsne_df.targets.astype(int) 
═════════════════════════════════════════════════════════════════════════ 
Here we are creating a pandas dataframe from a numpy array. There are three  columns: x, y and targets. x and y are the two components from t-SNE  decomposition and targets is the actual number. This gives us a dataframe which  looks like the one shown in figure 6.
12 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur   
Figure 6: First 10 rows of pandas dataframe with t-SNE components and targets. And finally, we can plot it using seaborn and matplotlib. 
═════════════════════════════════════════════════════════════════════════ grid = sns.FacetGrid(tsne_df, hue="targets", size=8) 
grid.map(plt.scatter, "x", "y").add_legend() 
═════════════════════════════════════════════════════════════════════════ 
This is one way of visualizing unsupervised datasets. We can also do k-means  clustering on the same dataset and see how it performs in an unsupervised setting. One question that arises all the time is how to find the optimal number of clusters  in k-means clustering. Well, there is no right answer. You have to find the number  by cross-validation. Cross-validation will be discussed later in this book. Please  note that the above code was run in a jupyter notebook.  
In this book, we will use jupyter for simple things like the example above and for  plotting. For most of the stuff in this book, we will be using python scripts. You can  choose what you want to use since the results are going to be the same. 
MNIST is a supervised classification problem, and we converted it to an  unsupervised problem only to check if it gives any kind of good results and it is  apparent that we do get good results with decomposition with t-SNE. The results  would be even better if we use classification algorithms. What are they and how to  use them? Let’s look at them in the next chapters.
13 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
Cross-validation 
We did not build any models in the previous chapter. The reason for that is simple.  Before creating any kind of machine learning model, we must know what cross validation is and how to choose the best cross-validation depending on your  datasets. 
So, what is cross-validation, and why should we care about it? 
We can find multiple definitions as to what cross-validation is. Mine is a one-liner:  cross-validation is a step in the process of building a machine learning model which  helps us ensure that our models fit the data accurately and also ensures that we do  not overfit. But this leads to another term: overfitting. 
To explain overfitting, I think it’s best if we look at a dataset. There is a red wine quality dataset2 which is quite famous. This dataset has 11 different attributes that  decide the quality of red wine.  
These attributes include: 
• fixed acidity 
• volatile acidity 
• citric acid 
• residual sugar 
• chlorides 
• free sulfur dioxide 
• total sulfur dioxide 
• density 
• pH 
• sulphates 
• alcohol 
Based on these different attributes, we are required to predict the quality of red wine  which is a value between 0 and 10. 
2 P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis; Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553,  2009.
14 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
Let’s see how this data looks like. 
═════════════════════════════════════════════════════════════════════════ import pandas as pd 
df = pd.read_csv("winequality-red.csv") 
═════════════════════════════════════════════════════════════════════════ This dataset looks something like this: 
  
Figure 1: A snapshot of the red wine quality dataset. 
We can treat this problem either as a classification problem or as a regression  problem since wine quality is nothing but a real number between 0 and 10. For  simplicity, let’s choose classification. This dataset, however, consists of only six types of quality values. We will thus map all quality values from 0 to 5. 
═════════════════════════════════════════════════════════════════════════ # a mapping dictionary that maps the quality values from 0 to 5 quality_mapping = { 
 3: 0, 
 4: 1, 
 5: 2, 
 6: 3, 
 7: 4, 
 8: 5 
} 
# you can use the map function of pandas with 
# any dictionary to convert the values in a given 
# column to values in the dictionary 
df.loc[:, "quality"] = df.quality.map(quality_mapping) 
═════════════════════════════════════════════════════════════════════════
15 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
When we look at this data and consider it a classification problem, a lot of  algorithms come to our mind that we can apply to it, probably, we can use neural  networks. But it would be a bit of a stretch if we dive into neural networks from the  beginning. So, let’s start with something simple that we can visualize too: decision  trees. 
Before we begin to understand what overfitting is, let’s divide the data into two  parts. This dataset has 1599 samples. We keep 1000 samples for training and 599  as a separate set. 
Splitting can be done easily by the following chunk of code: 
═════════════════════════════════════════════════════════════════════════ # use sample with frac=1 to shuffle the dataframe 
# we reset the indices since they change after 
# shuffling the dataframe 
df = df.sample(frac=1).reset_index(drop=True) 
# top 1000 rows are selected 
# for training 
df_train = df.head(1000) 
# bottom 599 values are selected 
# for testing/validation 
df_test = df.tail(599) 
═════════════════════════════════════════════════════════════════════════ 
We will now train a decision tree model on the training set. For the decision tree  model, I am going to use scikit-learn. 
═════════════════════════════════════════════════════════════════════════ # import from scikit-learn 
from sklearn import tree 
from sklearn import metrics 
# initialize decision tree classifier class 
# with a max_depth of 3 
clf = tree.DecisionTreeClassifier(max_depth=3) 
# choose the columns you want to train on 
# these are the features for the model 
cols = ['fixed acidity',  
 'volatile acidity',  
 'citric acid',
16 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
 'residual sugar', 
 'chlorides', 
 'free sulfur dioxide', 
 'total sulfur dioxide', 
 'density', 
 'pH', 
 'sulphates', 
 'alcohol'] 
# train the model on the provided features 
# and mapped quality from before 
clf.fit(df_train[cols], df_train.quality) 
═════════════════════════════════════════════════════════════════════════ 
Note that I have used a max_depth of 3 for the decision tree classifier. I have left  all other parameters of this model to its default value. 
Now, we test the accuracy of this model on the training set and the test set: 
═════════════════════════════════════════════════════════════════════════ # generate predictions on the training set 
train_predictions = clf.predict(df_train[cols]) 
# generate predictions on the test set 
test_predictions = clf.predict(df_test[cols]) 
# calculate the accuracy of predictions on 
# training data set 
train_accuracy = metrics.accuracy_score( 
 df_train.quality, train_predictions 
) 
# calculate the accuracy of predictions on 
# test data set 
test_accuracy = metrics.accuracy_score( 
 df_test.quality, test_predictions 
) 
═════════════════════════════════════════════════════════════════════════ 
The training and test accuracies are found to be 58.9% and 54.25%. Now we  increase the max_depth to 7 and repeat the process. This gives training accuracy of  76.6% and test accuracy of 57.3%. Here, we have used accuracy, mainly because it  is the most straightforward metric. It might not be the best metric for this problem.  What about we calculate these accuracies for different values of max_depth and  make a plot?
17 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
═════════════════════════════════════════════════════════════════════════ # NOTE: this code is written in a jupyter notebook 
# import scikit-learn tree and metrics 
from sklearn import tree 
from sklearn import metrics 
# import matplotlib and seaborn 
# for plotting 
import matplotlib 
import matplotlib.pyplot as plt 
import seaborn as sns 
# this is our global size of label text 
# on the plots 
matplotlib.rc('xtick', labelsize=20)  
matplotlib.rc('ytick', labelsize=20)  
# This line ensures that the plot is displayed 
# inside the notebook 
%matplotlib inline 
# initialize lists to store accuracies 
# for training and test data 
# we start with 50% accuracy 
train_accuracies = [0.5] 
test_accuracies = [0.5] 
# iterate over a few depth values 
for depth in range(1, 25): 
 # init the model 
 clf = tree.DecisionTreeClassifier(max_depth=depth) 
 # columns/features for training 
 # note that, this can be done outside  
 # the loop 
 cols = [ 
 'fixed acidity',  
 'volatile acidity', 
 'citric acid',  
 'residual sugar', 
 'chlorides', 
 'free sulfur dioxide',  
 'total sulfur dioxide', 
 'density', 
 'pH',  
 'sulphates',
18 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
 'alcohol' 
 ] 
 # fit the model on given features 
 clf.fit(df_train[cols], df_train.quality) 
 # create training & test predictions 
 train_predictions = clf.predict(df_train[cols]) 
 test_predictions = clf.predict(df_test[cols]) 
 # calculate training & test accuracies 
 train_accuracy = metrics.accuracy_score( 
 df_train.quality, train_predictions 
 ) 
 test_accuracy = metrics.accuracy_score( 
 df_test.quality, test_predictions 
 ) 
  
 # append accuracies 
 train_accuracies.append(train_accuracy) 
 test_accuracies.append(test_accuracy) 
# create two plots using matplotlib 
# and seaborn 
plt.figure(figsize=(10, 5)) 
sns.set_style("whitegrid") 
plt.plot(train_accuracies, label="train accuracy") 
plt.plot(test_accuracies, label="test accuracy") 
plt.legend(loc="upper left", prop={'size': 15}) 
plt.xticks(range(0, 26, 5)) 
plt.xlabel("max_depth", size=20) 
plt.ylabel("accuracy", size=20) 
plt.show() 
═════════════════════════════════════════════════════════════════════════ This generates a plot, as shown in figure 2. 
We see that the best score for test data is obtained when max_depth has a value of  14. As we keep increasing the value of this parameter, test accuracy remains the  same or gets worse, but the training accuracy keeps increasing. It means that our  simple decision tree model keeps learning about the training data better and better  with an increase in max_depth, but the performance on test data does not improve  at all. 
19 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
This is called overfitting.  
The model fits perfectly on the training set and performs poorly when it comes to  the test set. This means that the model will learn the training data well but will not  generalize on unseen samples. In the dataset above, one can build a model with very  high max_depth which will have outstanding results on training data, but that kind  of model is not useful as it will not provide a similar result on the real-world samples  or live data. 
  

Figure 2: Training and test accuracies for different values of max_depth. 
One might argue that this approach isn’t overfitting as the accuracy of the test set  more or less remains the same. Another definition of overfitting would be when the  test loss increases as we keep improving training loss. This is very common when  it comes to neural networks. 
Whenever we train a neural network, we must monitor loss during the training time  for both training and test set. If we have a very large network for a dataset which is  quite small (i.e. very less number of samples), we will observe that the loss for both  training and test set will decrease as we keep training. However, at some point, test  loss will reach its minima, and after that, it will start increasing even though training  loss decreases further. We must stop training where the validation loss reaches its  minimum value.  
This is the most common explanation of overfitting.
20 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
Occam’s razor in simple words states that one should not try to complicate things  that can be solved in a much simpler manner. In other words, the simplest solutions  are the most generalizable solutions. In general, whenever your model does not  obey Occam’s razor, it is probably overfitting.   
Figure 3: Most general definition of overfitting. 
Now we can go back to cross-validation. 
While explaining about overfitting, I decided to divide the data into two parts. I  trained the model on one part and checked its performance on the other part. Well,  this is also a kind of cross-validation commonly known as a hold-out set. We use  this kind of (cross-) validation when we have a large amount of data and model  inference is a time-consuming process. 
There are many different ways one can do cross-validation, and it is the most critical step when it comes to building a good machine learning model which is  generalizable when it comes to unseen data. Choosing the right cross-validation depends on the dataset you are dealing with, and one’s choice of cross-validation  on one dataset may or may not apply to other datasets. However, there are a few  types of cross-validation techniques which are the most popular and widely used.  
These include: 
• k-fold cross-validation 
• stratified k-fold cross-validation
21 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
• hold-out based validation 
• leave-one-out cross-validation 
• group k-fold cross-validation 
Cross-validation is dividing training data into a few parts. We train the model on  some of these parts and test on the remaining parts. Take a look at figure 4. 
  

Figure 4: Splitting a dataset into training and validation sets 
Figure 4 & 5 say that when you get a dataset to build machine learning models, you  separate them into two different sets: training and validation. Many people also  split it into a third set and call it a test set. We will, however, be using only two  sets. As you can see, we divide the samples and the targets associated with them.  We can divide the data into k different sets which are exclusive of each other. This  is known as k-fold cross-validation. 
  
Figure 5: K-fold cross-validation
22 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
We can split any data into k-equal parts using KFold from scikit-learn. Each sample  is assigned a value from 0 to k-1 when using k-fold cross validation. 
═════════════════════════════════════════════════════════════════════════ # import pandas and model_selection module of scikit-learn import pandas as pd 
from sklearn import model_selection 
if __name__ == "__main__": 
# Training data is in a CSV file called train.csv 
df = pd.read_csv("train.csv") 
 # we create a new column called kfold and fill it with -1 df["kfold"] = -1 
# the next step is to randomize the rows of the data 
df = df.sample(frac=1).reset_index(drop=True) 
# initiate the kfold class from model_selection module 
kf = model_selection.KFold(n_splits=5) 
# fill the new kfold column 
for fold, (trn_, val_) in enumerate(kf.split(X=df)): 
df.loc[val_, 'kfold'] = fold 
# save the new csv with kfold column  
df.to_csv("train_folds.csv", index=False) 
═════════════════════════════════════════════════════════════════════════ 
You can use this process with almost all kinds of datasets. For example, when you  have images, you can create a CSV with image id, image location and image label  and use the process above. 
The next important type of cross-validation is stratified k-fold. If you have a  skewed dataset for binary classification with 90% positive samples and only 10%  negative samples, you don't want to use random k-fold cross-validation. Using  simple k-fold cross-validation for a dataset like this can result in folds with all  negative samples. In these cases, we prefer using stratified k-fold cross-validation.  Stratified k-fold cross-validation keeps the ratio of labels in each fold constant. So,  in each fold, you will have the same 90% positive and 10% negative samples. Thus,  whatever metric you choose to evaluate, it will give similar results across all folds.
23 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
It’s easy to modify the code for creating k-fold cross-validation to create stratified  k-folds. We are only changing from model_selection.KFold to  model_selection.StratifiedKFold and in the kf.split(...) function, we specify the  target column on which we want to stratify. We assume that our CSV dataset has a  column called “target” and it is a classification problem! 
═════════════════════════════════════════════════════════════════════════ # import pandas and model_selection module of scikit-learn import pandas as pd 
from sklearn import model_selection 
if __name__ == "__main__": 
# Training data is in a csv file called train.csv 
df = pd.read_csv("train.csv") 
# we create a new column called kfold and fill it with -1 df["kfold"] = -1 
# the next step is to randomize the rows of the data 
df = df.sample(frac=1).reset_index(drop=True) 
# fetch targets 
y = df.target.values 
# initiate the kfold class from model_selection module 
kf = model_selection.StratifiedKFold(n_splits=5) 
# fill the new kfold column 
for f, (t_, v_) in enumerate(kf.split(X=df, y=y)): 
df.loc[v_, 'kfold'] = f 
# save the new csv with kfold column 
df.to_csv("train_folds.csv", index=False) 
═════════════════════════════════════════════════════════════════════════ For the wine dataset, let’s look at the distribution of labels. 
═════════════════════════════════════════════════════════════════════════ b = sns.countplot(x='quality', data=df) 
b.set_xlabel("quality", fontsize=20) 
b.set_ylabel("count", fontsize=20) 
═════════════════════════════════════════════════════════════════════════ 
Note that we continue on the code above. So, we have converted the target values. Looking at figure 6 we can say that the quality is very much skewed. Some classes 
24 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
have a lot of samples, and some don’t have that many. If we do a simple k-fold, we  won’t have an equal distribution of targets in every fold. Thus, we choose stratified  k-fold in this case. 
  

Figure 6: Distribution of “quality” in wine dataset 
The rule is simple. If it’s a standard classification problem, choose stratified k-fold  blindly. 
But what should we do if we have a large amount of data? Suppose we have 1  million samples. A 5 fold cross-validation would mean training on 800k samples  and validating on 200k. Depending on which algorithm we choose, training and  even validation can be very expensive for a dataset which is of this size. In these  cases, we can opt for a hold-out based validation. 
The process for creating the hold-out remains the same as stratified k-fold. For a  dataset which has 1 million samples, we can create ten folds instead of 5 and keep  one of those folds as hold-out. This means we will have 100k samples in the hold out, and we will always calculate loss, accuracy and other metrics on this set and  train on 900k samples. 
Hold-out is also used very frequently with time-series data. Let’s assume the  problem we are provided with is predicting sales of a store for 2020, and you are  provided all the data from 2015-2019. In this case, you can select all the data for  2019 as a hold-out and train your model on all the data from 2015 to 2018.
25 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur   
Figure 7: Example of a time-series data 
In the example presented in figure 7, let’s say our job is to predict the sales from  time step 31 to 40. We can then keep 21 to 30 as hold-out and train our model from  step 0 to step 20. You should note that when you are predicting from 31 to 40, you  should include the data from 21 to 30 in your model; otherwise, performance will  be sub-par. 
In many cases, we have to deal with small datasets and creating big validation sets  means losing a lot of data for the model to learn. In those cases, we can opt for a  type of k-fold cross-validation where k=N, where N is the number of samples in the  dataset. This means that in all folds of training, we will be training on all data  samples except 1. The number of folds for this type of cross-validation is the same  as the number of samples that we have in the dataset.  
One should note that this type of cross-validation can be costly in terms of the time  it takes if the model is not fast enough, but since it’s only preferable to use this  cross-validation for small datasets, it doesn’t matter much. 
Now we can move to regression. The good thing about regression problems is that  we can use all the cross-validation techniques mentioned above for regression  problems except for stratified k-fold. That is we cannot use stratified k-fold directly,  but there are ways to change the problem a bit so that we can use stratified k-fold  for regression problems. Mostly, simple k-fold cross-validation works for any  regression problem. However, if you see that the distribution of targets is not  consistent, you can use stratified k-fold.
26 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
To use stratified k-fold for a regression problem, we have first to divide the target  into bins, and then we can use stratified k-fold in the same way as for classification  problems. There are several choices for selecting the appropriate number of bins. If  you have a lot of samples( > 10k, > 100k), then you don’t need to care about the  number of bins. Just divide the data into 10 or 20 bins. If you do not have a lot of  samples, you can use a simple rule like Sturge’s Rule to calculate the appropriate  number of bins. 
Sturge’s rule: 
Number of Bins = 1 + log2(N) 
Where N is the number of samples you have in your dataset. This function is plotted  in Figure 8. 
  

Figure 8: Plotting samples vs the number of bins by Sturge’s Rule 
Let’s make a sample regression dataset and try to apply stratified k-fold as shown  in the following python snippet. 
═════════════════════════════════════════════════════════════════════════ # stratified-kfold for regression 
import numpy as np 
import pandas as pd 
from sklearn import datasets 
from sklearn import model_selection
27 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
def create_folds(data): 
 # we create a new column called kfold and fill it with -1  data["kfold"] = -1 
  
 # the next step is to randomize the rows of the data  data = data.sample(frac=1).reset_index(drop=True) 
 # calculate the number of bins by Sturge's rule 
 # I take the floor of the value, you can also 
 # just round it 
 num_bins = int(np.floor(1 + np.log2(len(data)))) 
 # bin targets 
 data.loc[:, "bins"] = pd.cut( 
 data["target"], bins=num_bins, labels=False 
 ) 
  
 # initiate the kfold class from model_selection module  kf = model_selection.StratifiedKFold(n_splits=5) 
  
 # fill the new kfold column 
 # note that, instead of targets, we use bins! 
 for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):  data.loc[v_, 'kfold'] = f 
  
 # drop the bins column 
 data = data.drop("bins", axis=1) 
 # return dataframe with folds 
 return data 
if __name__ == "__main__": 
 # we create a sample dataset with 15000 samples  
 # and 100 features and 1 target 
 X, y = datasets.make_regression( 
 n_samples=15000, n_features=100, n_targets=1 
 ) 
 # create a dataframe out of our numpy arrays 
 df = pd.DataFrame( 
 X, 
 columns=[f"f_{i}" for i in range(X.shape[1])] 
 ) 
 df.loc[:, "target"] = y 
 # create folds 
 df = create_folds(df) 
═════════════════════════════════════════════════════════════════════════
28 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
Cross-validation is the first and most essential step when it comes to building  machine learning models. If you want to do feature engineering, split your data first.  If you're going to build models, split your data first. If you have a good cross validation scheme in which validation data is representative of training and real world data, you will be able to build a good machine learning model which is highly  generalizable.  
The types of cross-validation presented in this chapter can be applied to almost any  machine learning problem. Still, you must keep in mind that cross-validation also  depends a lot on the data and you might need to adopt new forms of cross-validation  depending on your problem and data.  
For example, let’s say we have a problem in which we would like to build a model  to detect skin cancer from skin images of patients. Our task is to build a binary  classifier which takes an input image and predicts the probability for it being benign  or malignant.  
In these kinds of datasets, you might have multiple images for the same patient in  the training dataset. So, to build a good cross-validation system here, you must have  stratified k-folds, but you must also make sure that patients in training data do not  appear in validation data. Fortunately, scikit-learn offers a type of cross-validation  known as GroupKFold. Here the patients can be considered as groups. But  unfortunately, there is no way to combine GroupKFold with StratifiedKFold in  scikit-learn. So you need to do that yourself. I’ll leave it as an exercise for the  reader.
29 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
Evaluation metrics 
When it comes to machine learning problems, you will encounter a lot of different  types of metrics in the real world. Sometimes, people even end up creating metrics  that suit the business problem. It’s out of the scope of this book to introduce and  explain each and every type of metric. Instead, we will see some of the most  common metrics that you can use when starting with your very first few projects. 
At the start of the book, we introduced supervised and unsupervised learning.  Although there are some kinds of metrics that you can use for unsupervised learning, we will only focus on supervised. The reason for this is because supervised  problems are in abundance compared to un-supervised, and evaluation of  unsupervised methods is quite subjective. 
If we talk about classification problems, the most common metrics used are: - Accuracy 
- Precision (P) 
- Recall (R) 
- F1 score (F1) 
- Area under the ROC (Receiver Operating Characteristic) curve or simply  AUC (AUC) 
- Log loss 
- Precision at k (P@k) 
- Average precision at k (AP@k) 
- Mean average precision at k (MAP@k) 
When it comes to regression, the most commonly used evaluation metrics are: - Mean absolute error (MAE) 
- Mean squared error (MSE) 
- Root mean squared error (RMSE) 
- Root mean squared logarithmic error (RMSLE) 
- Mean percentage error (MPE) 
- Mean absolute percentage error (MAPE) 
- R2 
Knowing about how the aforementioned metrics work is not the only thing we have  to understand. We must also know when to use which metrics, and that depends on 
30 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
what kind of data and targets you have. I think it’s more about the targets and less  about the data.  
To learn more about these metrics, let’s start with a simple problem. Suppose we  have a binary classification problem, i.e. a problem in which there are only two  targets. Let’s suppose it’s a problem of classifying chest x-ray images. There are  chest x-ray images with no problem, and some of the chest x-ray images have  collapsed lung which is also known as pneumothorax. So, our task is to build a  classifier that given a chest x-ray image can detect if it has pneumothorax.  
  

Figure 1: A lung image showing pneumothorax. Image is taken from SIIM-ACR Pneumothorax  Segmentation Competition3 
We also assume that we have an equal number of pneumothorax and non pneumothorax chest x-ray images; let’s say 100 each. Thus, we have 100 positive  samples and 100 negative samples with a total of 200 images. 
The first step is to divide the data described above into two equal sets of 100 images  each, i.e. training and validation set. In both the sets, we have 50 positive and 50  negative samples. 
3 https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation
31 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
When we have an equal number of positive and negative samples in a binary  classification metric, we generally use accuracy, precision, recall and f1. 
Accuracy: It is one of the most straightforward metrics used in machine learning.  It defines how accurate your model is. For the problem described above, if you  build a model that classifies 90 images accurately, your accuracy is 90% or 0.90. If  only 83 images are classified correctly, the accuracy of your model is 83% or 0.83.  Simple. 
Python code for calculating accuracy is also quite simple. 
═════════════════════════════════════════════════════════════════════════ def accuracy(y_true, y_pred): 
 """ 
 Function to calculate accuracy 
 :param y_true: list of true values 
 :param y_pred: list of predicted values 
 :return: accuracy score 
 """ 
 # initialize a simple counter for correct predictions  correct_counter = 0 
 # loop over all elements of y_true 
 # and y_pred "together" 
 for yt, yp in zip(y_true, y_pred): 
 if yt == yp: 
 # if prediction is equal to truth, increase the counter  correct_counter += 1 
 # return accuracy 
 # which is correct predictions over the number of samples  return correct_counter / len(y_true) 
═════════════════════════════════════════════════════════════════════════ We can also calculate accuracy using scikit-learn. 
═════════════════════════════════════════════════════════════════════════ In [X]: from sklearn import metrics 
 ...: l1 = [0,1,1,1,0,0,0,1] 
 ...: l2 = [0,1,0,1,0,1,0,0] 
 ...: metrics.accuracy_score(l1, l2) 
Out[X]: 0.625 
═════════════════════════════════════════════════════════════════════════
32 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
Now, let’s say we change the dataset a bit such that there are 180 chest x-ray images  which do not have pneumothorax and only 20 with pneumothorax. Even in this  case, we will create the training and validation sets with the same ratio of positive  to negative (pneumothorax to non- pneumothorax) targets. In each set, we have 90  non- pneumothorax and 10 pneumothorax images. If you say that all images in the  validation set are non-pneumothorax, what would your accuracy be? Let’s see; you classified 90% of the images correctly. So, your accuracy is 90%.  
But look at it one more time.  
You didn’t even build a model and got an accuracy of 90%. That seems kind of  useless. If we look carefully, we will see that the dataset is skewed, i.e., the number  of samples in one class outnumber the number of samples in other class by a lot. In  these kinds of cases, it is not advisable to use accuracy as an evaluation metric as it  is not representative of the data. So, you might get high accuracy, but your model  will probably not perform that well when it comes to real-world samples, and you  won’t be able to explain to your managers why. 
In these cases, it’s better to look at other metrics such as precision.  
Before learning about precision, we need to know a few terms. Here we have  assumed that chest x-ray images with pneumothorax are positive class (1) and  without pneumothorax are negative class (0). 
True positive (TP): Given an image, if your model predicts the image has  pneumothorax, and the actual target for that image has pneumothorax, it is  considered a true positive. 
True negative (TN): Given an image, if your model predicts that the image does not  have pneumothorax and the actual target says that it is a non-pneumothorax image,  it is considered a true negative. 
In simple words, if your model correctly predicts positive class, it is true positive,  and if your model accurately predicts negative class, it is a true negative. 
False positive (FP): Given an image, if your model predicts pneumothorax and the  actual target for that image is non- pneumothorax, it a false positive.
33 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
False negative (FN): Given an image, if your model predicts non-pneumothorax  and the actual target for that image is pneumothorax, it is a false negative. 
In simple words, if your model incorrectly (or falsely) predicts positive class, it is  a false positive. If your model incorrectly (or falsely) predicts negative class, it is a  false negative. 
Let’s look at implementations of these, one at a time. 
═════════════════════════════════════════════════════════════════════════ def true_positive(y_true, y_pred): 
 """ 
 Function to calculate True Positives 
 :param y_true: list of true values 
 :param y_pred: list of predicted values 
 :return: number of true positives 
 """ 
 # initialize 
 tp = 0 
 for yt, yp in zip(y_true, y_pred): 
 if yt == 1 and yp == 1: 
 tp += 1 
 return tp 
def true_negative(y_true, y_pred): 
 """ 
 Function to calculate True Negatives 
 :param y_true: list of true values 
 :param y_pred: list of predicted values 
 :return: number of true negatives 
 """ 
 # initialize 
 tn = 0 
 for yt, yp in zip(y_true, y_pred): 
 if yt == 0 and yp == 0: 
 tn += 1 
 return tn 
def false_positive(y_true, y_pred): 
 """ 
 Function to calculate False Positives 
 :param y_true: list of true values 
 :param y_pred: list of predicted values 
 :return: number of false positives 
 """ 
 # initialize
34 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
 fp = 0 
 for yt, yp in zip(y_true, y_pred): 
 if yt == 0 and yp == 1: 
 fp += 1 
 return fp 
def false_negative(y_true, y_pred): 
 """ 
 Function to calculate False Negatives 
 :param y_true: list of true values 
 :param y_pred: list of predicted values 
 :return: number of false negatives 
 """ 
 # initialize 
 fn = 0 
 for yt, yp in zip(y_true, y_pred): 
 if yt == 1 and yp == 0: 
 fn += 1 
 return fn 
═════════════════════════════════════════════════════════════════════════ 
The way I have implemented these here is quite simple and works only for binary  classification. Let’s check these functions. 
═════════════════════════════════════════════════════════════════════════ In [X]: l1 = [0,1,1,1,0,0,0,1] 
 ...: l2 = [0,1,0,1,0,1,0,0] 
In [X]: true_positive(l1, l2) 
Out[X]: 2 
In [X]: false_positive(l1, l2) 
Out[X]: 1 
In [X]: false_negative(l1, l2) 
Out[X]: 2 
In [X]: true_negative(l1, l2) 
Out[X]: 3 
═════════════════════════════════════════════════════════════════════════ If we have to define accuracy using the terms described above, we can write: Accuracy Score = (TP + TN) / (TP + TN + FP + FN)
35 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
We can now quickly implement accuracy score using TP, TN, FP and FN in python.  Let’s call it accuracy_v2. 
═════════════════════════════════════════════════════════════════════════ def accuracy_v2(y_true, y_pred): 
 """ 
 Function to calculate accuracy using tp/tn/fp/fn 
 :param y_true: list of true values 
 :param y_pred: list of predicted values 
 :return: accuracy score 
 """ 
 tp = true_positive(y_true, y_pred) 
 fp = false_positive(y_true, y_pred) 
 fn = false_negative(y_true, y_pred) 
 tn = true_negative(y_true, y_pred) 
 accuracy_score = (tp + tn) / (tp + tn + fp + fn) 
 return accuracy_score 
═════════════════════════════════════════════════════════════════════════ 
We can quickly check the correctness of this function by comparing it to our  previous implementation and scikit-learn version. 
═════════════════════════════════════════════════════════════════════════ In [X]: l1 = [0,1,1,1,0,0,0,1] 
 ...: l2 = [0,1,0,1,0,1,0,0] 
In [X]: accuracy(l1, l2) 
Out[X]: 0.625 
In [X]: accuracy_v2(l1, l2) 
Out[X]: 0.625 
In [X]: metrics.accuracy_score(l1, l2) 
Out[X]: 0.625 
═════════════════════════════════════════════════════════════════════════ Please note that in this code, metrics.accuracy_score comes from scikit-learn. 
Great. All values match. This means we have not made any mistakes in the  implementation. 
Now, we can move to other important metrics. 
First one is precision. Precision is defined as:
36 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
Precision = TP / (TP + FP) 
Let’s say we make a new model on the new skewed dataset and our model correctly  identified 80 non-pneumothorax out of 90 and 8 pneumothorax out of 10. Thus, we  identify 88 images out of 100 successfully. The accuracy is, therefore, 0.88 or 88%.  
But, out of these 100 samples, 10 non-pneumothorax images are misclassified as  having pneumothorax and 2 pneumothorax are misclassified as not having  pneumothorax. 
Thus, we have: 
- TP : 8 
- TN: 80 
- FP: 10 
- FN: 2 
So, our precision is 8 / (8 + 10) = 0.444. This means our model is correct 44.4%  times when it’s trying to identify positive samples (pneumothorax). 
Now, since we have implemented TP, TN, FP and FN, we can easily implement  precision in python. 
═════════════════════════════════════════════════════════════════════════ def precision(y_true, y_pred): 
 """ 
 Function to calculate precision 
 :param y_true: list of true values 
 :param y_pred: list of predicted values 
 :return: precision score 
 """ 
 tp = true_positive(y_true, y_pred) 
 fp = false_positive(y_true, y_pred) 
 precision = tp / (tp + fp) 
 return precision 
═════════════════════════════════════════════════════════════════════════ Let’s try this implementation of precision. 
═════════════════════════════════════════════════════════════════════════ In [X]: l1 = [0,1,1,1,0,0,0,1] 
 ...: l2 = [0,1,0,1,0,1,0,0]
37 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
In [X]: precision(l1, l2) 
Out[X]: 0.6666666666666666 
═════════════════════════════════════════════════════════════════════════ This seems fine. 
Next, we come to recall. Recall is defined as: 
Recall = TP / (TP + FN) 
In the above case recall is 8 / (8 + 2) = 0.80. This means our model identified 80%  of positive samples correctly. 
═════════════════════════════════════════════════════════════════════════ def recall(y_true, y_pred): 
 """ 
 Function to calculate recall 
 :param y_true: list of true values 
 :param y_pred: list of predicted values 
 :return: recall score 
 """ 
 tp = true_positive(y_true, y_pred) 
 fn = false_negative(y_true, y_pred) 
 recall = tp / (tp + fn) 
 return recall 
═════════════════════════════════════════════════════════════════════════ In the case of our two small lists, we should have a recall of 0.5. Let’s check. 
═════════════════════════════════════════════════════════════════════════ In [X]: l1 = [0,1,1,1,0,0,0,1] 
 ...: l2 = [0,1,0,1,0,1,0,0] 
In [X]: recall(l1, l2) 
Out[X]: 0.5 
═════════════════════════════════════════════════════════════════════════ And that matches our calculated value! 
For a “good” model, our precision and recall values should be high. We see that in  the above example, the recall value is quite high. However, precision is very low!  Our model produces quite a lot of false positives but less false negatives. Fewer false negatives are good in this type of problem because you don’t want to say that 
38 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
patients do not have pneumothorax when they do. That is going to be more harmful.  But we do have a lot of false positives, and that’s not good either. 
Most of the models predict a probability, and when we predict, we usually choose  this threshold to be 0.5. This threshold is not always ideal, and depending on this  threshold, your value of precision and recall can change drastically. If for every  threshold we choose, we calculate the precision and recall values, we can create a  plot between these sets of values. This plot or curve is known as the precision-recall  curve.  
Before looking into the precision-recall curve, let’s assume two lists. 
═════════════════════════════════════════════════════════════════════════ In [X]: y_true = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 
 ...: 1, 0, 0, 0, 0, 0, 0, 0, 1, 0] 
In [X]: y_pred = [0.02638412, 0.11114267, 0.31620708, 
 ...: 0.0490937, 0.0191491, 0.17554844, 
 ...: 0.15952202, 0.03819563, 0.11639273, 
 ...: 0.079377, 0.08584789, 0.39095342, 
 ...: 0.27259048, 0.03447096, 0.04644807, 
 ...: 0.03543574, 0.18521942, 0.05934905, 
 ...: 0.61977213, 0.33056815] 
═════════════════════════════════════════════════════════════════════════ 
So, y_true is our targets, and y_pred is the probability values for a sample being  assigned a value of 1. So, now, we look at probabilities in prediction instead of the  predicted value (which is most of the time calculated with a threshold at 0.5). 
═════════════════════════════════════════════════════════════════════════ precisions = [] 
recalls = [] 
# how we assumed these thresholds is a long story 
thresholds = [0.0490937 , 0.05934905, 0.079377,  
 0.08584789, 0.11114267, 0.11639273,  
 0.15952202, 0.17554844, 0.18521942,  
 0.27259048, 0.31620708, 0.33056815,  
 0.39095342, 0.61977213] 
# for every threshold, calculate predictions in binary 
# and append calculated precisions and recalls 
# to their respective lists 
for i in thresholds: 
 temp_prediction = [1 if x >= i else 0 for x in y_pred]
39 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
 p = precision(y_true, temp_prediction) 
 r = recall(y_true, temp_prediction) 
 precisions.append(p) 
 recalls.append(r) 
═════════════════════════════════════════════════════════════════════════ Now, we can plot these values of precisions and recalls. 
═════════════════════════════════════════════════════════════════════════ plt.figure(figsize=(7, 7)) 
plt.plot(recalls, precisions) 
plt.xlabel('Recall', fontsize=15) 
plt.ylabel('Precision', fontsize=15) 
═════════════════════════════════════════════════════════════════════════ 
Figure 2 shows the precision-recall curve we get this way. 

Figure 2: precision-recall curve 
This precision-recall curve looks very different from what you might have seen  on the internet. It’s because we had only 20 samples, and only 3 of them were  positive samples. But there’s nothing to worry. It’s the same old precision-recall  curve.
40 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
You will notice that it’s challenging to choose a value of threshold that gives both  good precision and recall values. If the threshold is too high, you have a smaller number of true positives and a high number of false negatives. This decreases your  recall; however, your precision score will be high. If you reduce the threshold too  low, false positives will increase a lot, and precision will be less. 
Both precision and recall range from 0 to 1 and a value closer to 1 is better. 
F1 score is a metric that combines both precision and recall. It is defined as a simple  weighted average (harmonic mean) of precision and recall. If we denote precision  using P and recall using R, we can represent the F1 score as: 
F1 = 2PR / (P + R) 
A little bit of mathematics will lead you to the following equation of F1 based on  TP, FP and FN 
F1 = 2TP / (2TP + FP + FN) 
A Python implementation is simple because we have already implemented these. 
═════════════════════════════════════════════════════════════════════════ def f1(y_true, y_pred): 
 """ 
 Function to calculate f1 score 
 :param y_true: list of true values 
 :param y_pred: list of predicted values 
 :return: f1 score 
 """ 
 p = precision(y_true, y_pred) 
 r = recall(y_true, y_pred) 
 score = 2 * p * r / (p + r) 
 return score 
═════════════════════════════════════════════════════════════════════════ Let’s see the results of this and compare it with scikit-learn. 
═════════════════════════════════════════════════════════════════════════ In [X]: y_true = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 
 ...: 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]
41 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
In [X]: y_pred = [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 
 ...: 1, 0, 0, 0, 0, 0, 0, 0, 1, 0] 
In [X]: f1(y_true, y_pred) 
Out[X]: 0.5714285714285715 
═════════════════════════════════════════════════════════════════════════ And from scikit learn for the same lists, we get: 
═════════════════════════════════════════════════════════════════════════ In [X]: from sklearn import metrics 
In [X]: metrics.f1_score(y_true, y_pred) 
Out[X]: 0.5714285714285715 
═════════════════════════════════════════════════════════════════════════ 
Instead of looking at precision and recall individually, you can also just look at F1 score. Same as for precision, recall and accuracy, F1 score also ranges from 0 to 1, and a perfect prediction model has an F1 of 1. When dealing with datasets that have  skewed targets, we should look at F1 (or precision and recall) instead of accuracy. 
Then there are other crucial terms that we should know about.  The first one is TPR or True Positive Rate, which is the same as recall. TPR = TP / (TP + FN) 
Even though it is same as recall, we will make a python function for it for further  use with this name. 
═════════════════════════════════════════════════════════════════════════ def tpr(y_true, y_pred): 
 """ 
 Function to calculate tpr 
 :param y_true: list of true values 
 :param y_pred: list of predicted values 
 :return: tpr/recall 
 """ 
 return recall(y_true, y_pred) 
═════════════════════════════════════════════════════════════════════════ TPR or recall is also known as sensitivity.
42 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
And FPR or False Positive Rate, which is defined as: 
FPR = FP / (TN + FP) 
═════════════════════════════════════════════════════════════════════════ def fpr(y_true, y_pred): 
 """ 
 Function to calculate fpr 
 :param y_true: list of true values 
 :param y_pred: list of predicted values 
 :return: fpr 
 """ 
 fp = false_positive(y_true, y_pred) 
 tn = true_negative(y_true, y_pred) 
 return fp / (tn + fp) 
═════════════════════════════════════════════════════════════════════════ And 1 - FPR is known as specificity or True Negative Rate or TNR. 
These are a lot of terms, but the most important ones out of these are only TPR and  FPR.  
Let’s assume that we have only 15 samples and their target values are binary: Actual targets : [0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1] 
We train a model like the random forest, and we can get the probability of when a  sample is positive. 
Predicted probabilities for 1: [0.1, 0.3, 0.2, 0.6, 0.8, 0.05, 0.9, 0.5, 0.3, 0.66, 0.3,  0.2, 0.85, 0.15, 0.99] 
For a typical threshold of >= 0.5, we can evaluate all the above values of precision,  recall/TPR, F1 and FPR. But we can do the same if we choose the value of the  threshold to be 0.4 or 0.6. In fact, we can choose any value between 0 and 1 and  calculate all the metrics described above. 
Let’s calculate only two values, though: TPR and FPR. 
═════════════════════════════════════════════════════════════════════════ # empty lists to store tpr  
# and fpr values
43 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
tpr_list = [] 
fpr_list = [] 
# actual targets 
y_true = [0, 0, 0, 0, 1, 0, 1,  
 0, 0, 1, 0, 1, 0, 0, 1] 
# predicted probabilities of a sample being 1 
y_pred = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05,  
 0.9, 0.5, 0.3, 0.66, 0.3, 0.2,  
 0.85, 0.15, 0.99] 
# handmade thresholds 
thresholds = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 
 0.6, 0.7, 0.8, 0.85, 0.9, 0.99, 1.0] 
# loop over all thresholds 
for thresh in thresholds:  
 # calculate predictions for a given threshold 
 temp_pred = [1 if x >= thresh else 0 for x in y_pred]  # calculate tpr 
 temp_tpr = tpr(y_true, temp_pred) 
 # calculate fpr 
 temp_fpr = fpr(y_true, temp_pred) 
 # append tpr and fpr to lists 
 tpr_list.append(temp_tpr) 
 fpr_list.append(temp_fpr) 
═════════════════════════════════════════════════════════════════════════ We can thus get a tpr and fpr value for each threshold. 

Figure 3: Table for threshold, TPR and FPR values
44 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
If we plot the table as shown in figure 3, i.e. if we have TPR on the y-axis and FPR  on the x-axis, we will get a curve as shown in figure 4. 
═════════════════════════════════════════════════════════════════════════ plt.figure(figsize=(7, 7)) 
plt.fill_between(fpr_list, tpr_list, alpha=0.4) 
plt.plot(fpr_list, tpr_list, lw=3) 
plt.xlim(0, 1.0) 
plt.ylim(0, 1.0) 
plt.xlabel('FPR', fontsize=15) 
plt.ylabel('TPR', fontsize=15) 
plt.show() 
═════════════════════════════════════════════════════════════════════════ 
Figure 4: Receiver operating characteristic (ROC) curve 
This curve is also known as the Receiver Operating Characteristic (ROC). And  if we calculate the area under this ROC curve, we are calculating another metric  which is used very often when you have a dataset which has skewed binary targets.  
This metric is known as the Area Under ROC Curve or Area Under Curve or  just simply AUC. There are many ways to calculate the area under the ROC curve.  For this particular purpose, we will stick to the fantastic implementation by scikit learn. 
═════════════════════════════════════════════════════════════════════════ In [X]: from sklearn import metrics
45 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
In [X]: y_true = [0, 0, 0, 0, 1, 0, 1, 
 ...: 0, 0, 1, 0, 1, 0, 0, 1] 
In [X]: y_pred = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05, 
 ...: 0.9, 0.5, 0.3, 0.66, 0.3, 0.2, 
 ...: 0.85, 0.15, 0.99] 
In [X]: metrics.roc_auc_score(y_true, y_pred) 
Out[X]: 0.8300000000000001 
═════════════════════════════════════════════════════════════════════════ AUC values range from 0 to 1. 
- AUC = 1 implies you have a perfect model. Most of the time, it means that  you made some mistake with validation and should revisit data processing  and validation pipeline of yours. If you didn’t make any mistakes, then  congratulations, you have the best model one can have for the dataset you  built it on. 
- AUC = 0 implies that your model is very bad (or very good!). Try inverting  the probabilities for the predictions, for example, if your probability for the  positive class is p, try substituting it with 1-p. This kind of AUC may also  mean that there is some problem with your validation or data processing. 
- AUC = 0.5 implies that your predictions are random. So, for any binary  classification problem, if I predict all targets as 0.5, I will get an AUC of  0.5. 
AUC values between 0 and 0.5 imply that your model is worse than random. Most  of the time, it’s because you inverted the classes. If you try to invert your  predictions, your AUC might become more than 0.5. AUC values closer to 1 are  considered good. 
But what does AUC say about our model?  
Suppose you get an AUC of 0.85 when you build a model to detect pneumothorax  from chest x-ray images. This means that if you select a random image from your  dataset with pneumothorax (positive sample) and another random image without  pneumothorax (negative sample), then the pneumothorax image will rank higher  than a non-pneumothorax image with a probability of 0.85.
46 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
After calculating probabilities and AUC, you would want to make predictions on  the test set. Depending on the problem and use-case, you might want to either have  probabilities or actual classes. If you want to have probabilities, it’s effortless. You  already have them. If you want to have classes, you need to select a threshold. In  the case of binary classification, you can do something like the following. 
Prediction = Probability >= Threshold 
Which means, that prediction is a new list which contains only binary variables. An  item in prediction is 1 if the probability is greater than or equal to a given threshold else the value is 0. 
And guess what, you can use the ROC curve to choose this threshold! The ROC  curve will tell you how the threshold impacts false positive rate and true positive  rate and thus, in turn, false positives and true positives. You should choose the  threshold that is best suited for your problem and datasets.  
For example, if you don’t want to have too many false positives, you should have a  high threshold value. This will, however, also give you a lot more false negatives.  Observe the trade-off and select the best threshold. Let’s see how these thresholds  impact true positive and false positive values. 
═════════════════════════════════════════════════════════════════════════ # empty lists to store true positive  
# and false positive values 
tp_list = [] 
fp_list = [] 
# actual targets 
y_true = [0, 0, 0, 0, 1, 0, 1,  
 0, 0, 1, 0, 1, 0, 0, 1] 
# predicted probabilities of a sample being 1 
y_pred = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05,  
 0.9, 0.5, 0.3, 0.66, 0.3, 0.2,  
 0.85, 0.15, 0.99] 
# some handmade thresholds 
thresholds = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 
 0.6, 0.7, 0.8, 0.85, 0.9, 0.99, 1.0] 
# loop over all thresholds 
for thresh in thresholds: 
47 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
 # calculate predictions for a given threshold 
 temp_pred = [1 if x >= thresh else 0 for x in y_pred]  # calculate tp 
 temp_tp = true_positive(y_true, temp_pred) 
 # calculate fp 
 temp_fp = false_positive(y_true, temp_pred) 
 # append tp and fp to lists 
 tp_list.append(temp_tp) 
 fp_list.append(temp_fp) 
═══════════════════════════════════════════════════════════════════════ Using this, we can create a table, as shown in Figure 5. 

Figure 5: TP and FP values for different thresholds 
Most of the time, the top-left value on ROC curve should give you a quite good  threshold, as shown in figure 6. 
Comparing the table and the ROC curve, we see that a threshold of around 0.6 is  quite good where we do not lose a lot of true positives and neither we have a lot of  false positives.
48 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
Figure 6: Select the best threshold from the leftmost top point in the ROC curve 
AUC is a widely used metric for skewed binary classification tasks in the industry, and a metric everyone should know about. Once you understand the idea behind  AUC, as explained in the paragraphs above, it is also easy to explain it to non technical people who would probably be assessing your models in the industry. 
Another important metric you should learn after learning AUC is log loss. In case  of a binary classification problem, we define log loss as: 
Log Loss = - 1.0 * ( target * log(prediction) + (1 - target) * log(1 - prediction) ) 
Where target is either 0 or 1 and prediction is a probability of a sample belonging  to class 1. 
For multiple samples in the dataset, the log-loss over all samples is a mere average  of all individual log losses. One thing to remember is that log loss penalizes quite  high for an incorrect or a far-off prediction, i.e. log loss punishes you for being very  sure and very wrong. 
═════════════════════════════════════════════════════════════════════════ import numpy as np 
def log_loss(y_true, y_proba):
49 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
 """ 
 Function to calculate log loss 
 :param y_true: list of true values 
 :param y_proba: list of probabilities for 1 
 :return: overall log loss 
 """ 
 # define an epsilon value 
 # this can also be an input 
 # this value is used to clip probabilities 
 epsilon = 1e-15 
 # initialize empty list to store 
 # individual losses 
 loss = [] 
 # loop over all true and predicted probability values  for yt, yp in zip(y_true, y_proba): 
 # adjust probability 
 # 0 gets converted to 1e-15 
 # 1 gets converted to 1-1e-15 
 # Why? Think about it! 
 yp = np.clip(yp, epsilon, 1 - epsilon) 
 # calculate loss for one sample 
 temp_loss = - 1.0 * ( 
 yt * np.log(yp)  
 + (1 - yt) * np.log(1 - yp) 
 ) 
 # add to loss list 
 loss.append(temp_loss) 
 # return mean loss over all samples 
 return np.mean(loss) 
═════════════════════════════════════════════════════════════════════════ Let’s test our implementation: 
═════════════════════════════════════════════════════════════════════════ In [X]: y_true = [0, 0, 0, 0, 1, 0, 1, 
 ...: 0, 0, 1, 0, 1, 0, 0, 1] 
In [X]: y_proba = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05, 
 ...: 0.9, 0.5, 0.3, 0.66, 0.3, 0.2, 
 ...: 0.85, 0.15, 0.99] 
In [X]: log_loss(y_true, y_proba) 
Out[X]: 0.49882711861432294 
═════════════════════════════════════════════════════════════════════════ We can compare this with scikit-learn:
50 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
═════════════════════════════════════════════════════════════════════════ In [X]: from sklearn import metrics 
In [X]: metrics.log_loss(y_true, y_proba) 
Out[X]: 0.49882711861432294 
═════════════════════════════════════════════════════════════════════════ 
Thus, our implementation is correct. Implementation of log loss is easy.  Interpretation may seem a bit difficult. You must remember that log loss penalizes  a lot more than other metrics.  
For example, if you are 51% sure about a sample belonging to class 1, log loss  would be: 
- 1.0 * ( 1 * log(0.51) + (1 - 1) * log(1 – 0.51) ) = 0.67 
And if you are 49% sure for a sample belonging to class 0, log loss would be: - 1.0 * ( 0 * log(0.49) + (1 - 0) * log(1 – 0.49) ) = 0.67 
So, even though we can choose a cut off at 0.5 and get perfect predictions, we will  still have a very high log loss. So, when dealing with log loss, you need to be very  careful; any non-confident prediction will have a very high log loss. 
Most of the metrics that we discussed until now can be converted to a multi-class  version. The idea is quite simple. Let’s take precision and recall. We can calculate  precision and recall for each class in a multi-class classification problem. 
There are three different ways to calculate this which might get confusing from time  to time. Let’s assume we are interested in precision first. We know that precision  depends on true positives and false positives. 
- Macro averaged precision: calculate precision for all classes individually  and then average them 
- Micro averaged precision: calculate class wise true positive and false  positive and then use that to calculate overall precision 
- Weighted precision: same as macro but in this case, it is weighted average  depending on the number of items in each class
51 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
This seems complicated but is easy to understand by python implementations. Let’s  see how macro-averaged precision is implemented. 
═════════════════════════════════════════════════════════════════════════ import numpy as np 
def macro_precision(y_true, y_pred): 
 """ 
 Function to calculate macro averaged precision 
 :param y_true: list of true values 
 :param y_pred: list of predicted values 
 :return: macro precision score 
 """ 
  
 # find the number of classes by taking 
 # length of unique values in true list 
 num_classes = len(np.unique(y_true)) 
  
 # initialize precision to 0 
 precision = 0 
  
 # loop over all classes 
 for class_ in range(num_classes): 
  
 # all classes except current are considered negative  temp_true = [1 if p == class_ else 0 for p in y_true]  temp_pred = [1 if p == class_ else 0 for p in y_pred]   
 # calculate true positive for current class 
 tp = true_positive(temp_true, temp_pred) 
  
 # calculate false positive for current class 
 fp = false_positive(temp_true, temp_pred) 
  
 # calculate precision for current class 
 temp_precision = tp / (tp + fp) 
  
 # keep adding precision for all classes 
 precision += temp_precision 
 # calculate and return average precision over all classes  precision /= num_classes 
 return precision 
═════════════════════════════════════════════════════════════════════════
52 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
You will notice that it wasn’t so difficult. Similarly, we have micro-averaged  precision score. 
═════════════════════════════════════════════════════════════════════════ import numpy as np 
def micro_precision(y_true, y_pred): 
 """ 
 Function to calculate micro averaged precision 
 :param y_true: list of true values 
 :param y_pred: list of predicted values 
 :return: micro precision score 
 """ 
  
 # find the number of classes by taking 
 # length of unique values in true list 
 num_classes = len(np.unique(y_true)) 
  
 # initialize tp and fp to 0 
 tp = 0 
 fp = 0 
  
 # loop over all classes 
 for class_ in range(num_classes): 
 # all classes except current are considered negative  temp_true = [1 if p == class_ else 0 for p in y_true]  temp_pred = [1 if p == class_ else 0 for p in y_pred]   
 # calculate true positive for current class 
 # and update overall tp 
 tp += true_positive(temp_true, temp_pred) 
  
 # calculate false positive for current class 
 # and update overall tp 
 fp += false_positive(temp_true, temp_pred) 
  
 # calculate and return overall precision 
 precision = tp / (tp + fp) 
 return precision 
═════════════════════════════════════════════════════════════════════════ This isn’t difficult, either. Then what is? Nothing. Machine learning is easy.  Now, let’s look at the implementation of weighted precision.
53 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
═════════════════════════════════════════════════════════════════════════ from collections import Counter 
import numpy as np 
def weighted_precision(y_true, y_pred): 
 """ 
 Function to calculate weighted averaged precision 
 :param y_true: list of true values 
 :param y_pred: list of predicted values 
 :return: weighted precision score 
 """ 
  
 # find the number of classes by taking 
 # length of unique values in true list 
 num_classes = len(np.unique(y_true)) 
  
 # create class:sample count dictionary 
 # it looks something like this: 
 # {0: 20, 1:15, 2:21} 
 class_counts = Counter(y_true) 
  
 # initialize precision to 0 
 precision = 0 
  
 # loop over all classes 
 for class_ in range(num_classes): 
 # all classes except current are considered negative  temp_true = [1 if p == class_ else 0 for p in y_true]  temp_pred = [1 if p == class_ else 0 for p in y_pred]   
 # calculate tp and fp for class 
 tp = true_positive(temp_true, temp_pred) 
 fp = false_positive(temp_true, temp_pred) 
  
 # calculate precision of class 
 temp_precision = tp / (tp + fp) 
  
 # multiply precision with count of samples in class  weighted_precision = class_counts[class_] * temp_precision   
 # add to overall precision 
 precision += weighted_precision 
 # calculate overall precision by dividing by 
 # total number of samples 
 overall_precision = precision / len(y_true)
54 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
 return overall_precision 
═════════════════════════════════════════════════════════════════════════ 
Let’s compare our implementations with scikit-learn to know if we implemented it  right. 
═════════════════════════════════════════════════════════════════════════ In [X]: from sklearn import metrics 
In [X]: y_true = [0, 1, 2, 0, 1, 2, 0, 2, 2] 
In [X]: y_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2] 
In [X]: macro_precision(y_true, y_pred) 
Out[X]: 0.3611111111111111 
In [X]: metrics.precision_score(y_true, y_pred, average="macro") Out[X]: 0.3611111111111111 
In [X]: micro_precision(y_true, y_pred) 
Out[X]: 0.4444444444444444 
In [X]: metrics.precision_score(y_true, y_pred, average="micro") Out[X]: 0.4444444444444444 
In [X]: weighted_precision(y_true, y_pred) 
Out[X]: 0.39814814814814814 
In [X]: metrics.precision_score(y_true, y_pred, average="weighted") Out[X]: 0.39814814814814814 
═════════════════════════════════════════════════════════════════════════ 
It seems like we implemented everything correctly. Please note that the  implementations shown here may not be the most efficient, but they are the easiest  to understand. 
Similarly, we can implement the recall metric for multi-class. Precision and recall  depend on true positive, false positive and false negative while F1 depends on  precision and recall.  
Implementation for recall is left as an exercise for the reader and one version of F1  for multi-class, i.e., weighted average is implemented here.
55 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
═════════════════════════════════════════════════════════════════════════ from collections import Counter 
import numpy as np 
def weighted_f1(y_true, y_pred): 
 """ 
 Function to calculate weighted f1 score 
 :param y_true: list of true values 
 :param y_proba: list of predicted values 
 :return: weighted f1 score 
 """ 
  
 # find the number of classes by taking 
 # length of unique values in true list 
 num_classes = len(np.unique(y_true)) 
  
 # create class:sample count dictionary 
 # it looks something like this: 
 # {0: 20, 1:15, 2:21} 
 class_counts = Counter(y_true) 
  
 # initialize f1 to 0 
 f1 = 0 
  
 # loop over all classes 
 for class_ in range(num_classes): 
 # all classes except current are considered negative  temp_true = [1 if p == class_ else 0 for p in y_true]  temp_pred = [1 if p == class_ else 0 for p in y_pred]   
 # calculate precision and recall for class 
 p = precision(temp_true, temp_pred) 
 r = recall(temp_true, temp_pred) 
  
 # calculate f1 of class 
 if p + r != 0: 
 temp_f1 = 2 * p * r / (p + r) 
 else: 
 temp_f1 = 0 
  
 # multiply f1 with count of samples in class 
 weighted_f1 = class_counts[class_] * temp_f1 
  
 # add to f1 precision 
 f1 += weighted_f1 
 
56 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
 # calculate overall F1 by dividing by 
 # total number of samples 
 overall_f1 = f1 / len(y_true) 
 return overall_f1 
═════════════════════════════════════════════════════════════════════════ 
Note that there are a few lines of code above which are new. And that’s why you  should read the code carefully. 
═════════════════════════════════════════════════════════════════════════ In [X]: from sklearn import metrics 
In [X]: y_true = [0, 1, 2, 0, 1, 2, 0, 2, 2] 
In [X]: y_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2] 
In [X]: weighted_f1(y_true, y_pred) 
Out[X]: 0.41269841269841273 
In [X]: metrics.f1_score(y_true, y_pred, average="weighted") Out[X]: 0.41269841269841273 
═════════════════════════════════════════════════════════════════════════ 
Thus, we have precision, recall and F1 implemented for multi-class problems. You  can similarly convert AUC and log loss to multi-class formats too. This format of  conversion is known as one-vs-all. I’m not going to implement them here as the  implementation is quite similar to what we have already discussed. 
In binary or multi-class classification, it is also quite popular to take a look at  confusion matrix. Don’t be confused; it’s quite easy. A confusion matrix is nothing  but a table of TP, FP, TN and FN. Using the confusion matrix, you can quickly see  how many samples were misclassified and how many were classified correctly.  
One might argue that the confusion matrix should be covered quite early in this  chapter, but I chose not to do it. If you understand TP, FP, TN, FN, precision, recall  and AUC, it becomes quite easy to understand and interpret confusion matrix. Let’s  see what confusion matrix looks like for a binary classification problem in figure 7. 
We see that the confusion matrix is made up of TP, FP, FN and TN. These are the  only values we need to calculate precision, recall, F1 score and AUC. Sometimes,  people also prefer calling FP as Type-I error and FN as Type-II error.
57 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
Figure 7: Confusion matrix for a binary classification task 
We can also expand the binary confusion matrix to a multi-class confusion matrix.  How would that look like? If we have N classes, it will be a matrix of size NxN.  For every class, we calculate the total number of samples that went to the class in  concern and other classes. This can be best understood by an example. 
Suppose we have the following actual classes: 
[0, 1, 2, 0, 1, 2, 0, 2, 2] 
And our predictions are: 
[0, 2, 1, 0, 2, 1, 0, 0, 2] 
Then our confusion matrix will look as shown in figure 8. 
What does figure 8 tell us?  
Let’s look at class 0. We see that there are 3 instances of class 0 in the actual target.  However, in prediction, we have 3 instances that belong to class 0 and 1 instance  that belongs to class 1. Ideally, for class 0 in the actual label, predicted labels 1 and  2 shouldn’t have any instance. Let’s see class 2. In actual labels, this count adds up  to 4 while in predicted it adds up to 3. Only 1 instance has a perfect prediction for  class 2 and 2 instances go to class 1.  
A perfect confusion matrix should only be filled diagonally from left to right. 
58 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
Figure 8: Confusion matrix for a multi-class problem 
Confusion matrix gives an easy way to calculate different metrics that we have  discussed before. Scikit-learn offers an easy and straightforward way to generate a  confusion matrix. Please note that the confusion matrix that I have shown in figure  8 is a transpose of scikit-learn’s confusion matrix and an original version can be  plotted by the following code. 
═════════════════════════════════════════════════════════════════════════ import matplotlib.pyplot as plt 
import seaborn as sns 
from sklearn import metrics 
# some targets 
y_true = [0, 1, 2, 0, 1, 2, 0, 2, 2] 
#some predictions 
y_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2] 
# get confusion matrix from sklearn 
cm = metrics.confusion_matrix(y_true, y_pred) 
# plot using matplotlib and seaborn 
plt.figure(figsize=(10, 10)) 
cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0,  as_cmap=True) 
sns.set(font_scale=2.5) 
sns.heatmap(cm, annot=True, cmap=cmap, cbar=False)
59 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
plt.ylabel('Actual Labels', fontsize=20) 
plt.xlabel('Predicted Labels', fontsize=20) 
═════════════════════════════════════════════════════════════════════════ 
So, until now, we have tackled metrics for binary and multi-class classification.  Then comes another type of classification problem called multi-label  classification. In multi-label classification, each sample can have one or more  classes associated with it. One simple example of this type of problem would be a  task in which you are asked to predict different objects in a given image. 

Figure 9: Different objects in an image4 
Figure 9 shows an example image from a well-known dataset. Note that this  dataset’s objective is something different but let’s not go there. Let’s assume that  the aim is only to predict if an object is present in an image or not. For figure 9, we  have a chair, flower-pot, window, but we don’t have other objects such as computer,  bed, tv, etc. So, one image can have multiple targets associated with it. This type of  problem is the multi-label classification problem. 
The metrics for this type of classification problem are a bit different. Some suitable  and most common metrics are: 
- Precision at k (P@k) 
- Average precision at k (AP@k) 
4 https://www.flickr.com/photos/krakluski/2950388100 License: CC BY 2.0
60 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
- Mean average precision at k (MAP@k) 
- Log loss 
Let’s start with precision at k or P@k. One must not confuse this precision with  the precision discussed earlier. If you have a list of original classes for a given  sample and list of predicted classes for the same, precision is defined as the number  of hits in the predicted list considering only top-k predictions, divided by k. 
If that’s confusing, it will become apparent with python code. 
═════════════════════════════════════════════════════════════════════════ def pk(y_true, y_pred, k): 
 """ 
 This function calculates precision at k  
 for a single sample 
 :param y_true: list of values, actual classes 
 :param y_pred: list of values, predicted classes 
 :param k: the value for k 
 :return: precision at a given value k 
 """ 
 # if k is 0, return 0. we should never have this 
 # as k is always >= 1 
 if k == 0: 
 return 0 
 # we are interested only in top-k predictions 
 y_pred = y_pred[:k] 
 # convert predictions to set 
 pred_set = set(y_pred) 
 # convert actual values to set 
 true_set = set(y_true) 
 # find common values 
 common_values = pred_set.intersection(true_set) 
 # return length of common values over k 
 return len(common_values) / len(y_pred[:k]) 
═════════════════════════════════════════════════════════════════════════ With code, everything becomes much easier to understand. 
Now, we have average precision at k or AP@k. AP@k is calculated using P@k.  For example, if we have to calculate AP@3, we calculate P@1, P@2 and P@3 and  then divide the sum by 3.  
Let’s see its implementation.
61 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
═════════════════════════════════════════════════════════════════════════ def apk(y_true, y_pred, k): 
 """ 
 This function calculates average precision at k  
 for a single sample 
 :param y_true: list of values, actual classes 
 :param y_pred: list of values, predicted classes 
 :return: average precision at a given value k 
 """ 
 # initialize p@k list of values 
 pk_values = [] 
 # loop over all k. from 1 to k + 1 
 for i in range(1, k + 1): 
 # calculate p@i and append to list 
 pk_values.append(pk(y_true, y_pred, i)) 
 # if we have no values in the list, return 0 
 if len(pk_values) == 0: 
 return 0 
 # else, we return the sum of list over length of list  return sum(pk_values) / len(pk_values) 
═════════════════════════════════════════════════════════════════════════ 
These two functions can be used to calculate average precision at k (AP@k) for two  given lists; let’s see how. 
In [X]: y_true = [ 
 ...: [1, 2, 3], 
 ...: [0, 2], 
 ...: [1], 
 ...: [2, 3], 
 ...: [1, 0], 
 ...: [] 
 ...: ] 
In [X]: y_pred = [ 
 ...: [0, 1, 2], 
 ...: [1], 
 ...: [0, 2, 3], 
 ...: [2, 3, 4, 0], 
 ...: [0, 1, 2], 
 ...: [0] 
 ...: ] 
In [X]: for i in range(len(y_true)): 
 ...: for j in range(1, 4): 
 ...: print(
62 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
 ...: f""" 
 ...: y_true={y_true[i]}, 
 ...: y_pred={y_pred[i]}, 
 ...: AP@{j}={apk(y_true[i], y_pred[i], k=j)}  ...: """ 
 ...: ) 
 ...: 
 y_true=[1, 2, 3], 
 y_pred=[0, 1, 2], 
 AP@1=0.0 
 y_true=[1, 2, 3], 
 y_pred=[0, 1, 2], 
 AP@2=0.25 
 y_true=[1, 2, 3], 
 y_pred=[0, 1, 2], 
 AP@3=0.38888888888888884 
 . 
 . 
═════════════════════════════════════════════════════════════════════════ 
Please note that I have omitted many values from the output, but you get the point. So, this is how we can calculate AP@k which is per sample. In machine learning,  we are interested in all samples, and that’s why we have mean average precision  at k or MAP@k. MAP@k is just an average of AP@k and can be calculated easily  by the following python code. 
═════════════════════════════════════════════════════════════════════════ def mapk(y_true, y_pred, k): 
 """ 
 This function calculates mean avg precision at k  
 for a single sample 
 :param y_true: list of values, actual classes 
 :param y_pred: list of values, predicted classes 
 :return: mean avg precision at a given value k 
 """ 
 # initialize empty list for apk values 
 apk_values = [] 
 # loop over all samples 
 for i in range(len(y_true)): 
 # store apk values for every sample 
 apk_values.append(
63 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
 apk(y_true[i], y_pred[i], k=k) 
 ) 
 # return mean of apk values list 
 return sum(apk_values) / len(apk_values) 
═════════════════════════════════════════════════════════════════════════ Now, we can calculate MAP@k for k=1, 2, 3 and 4 for the same list of lists. 
═════════════════════════════════════════════════════════════════════════ In [X]: y_true = [ 
 ...: [1, 2, 3], 
 ...: [0, 2], 
 ...: [1], 
 ...: [2, 3], 
 ...: [1, 0], 
 ...: [] 
 ...: ] 
In [X]: y_pred = [ 
 ...: [0, 1, 2], 
 ...: [1], 
 ...: [0, 2, 3], 
 ...: [2, 3, 4, 0], 
 ...: [0, 1, 2], 
 ...: [0] 
 ...: ] 
In [X]: mapk(y_true, y_pred, k=1) 
Out[X]: 0.3333333333333333 
In [X]: mapk(y_true, y_pred, k=2) 
Out[X]: 0.375 
In [X]: mapk(y_true, y_pred, k=3) 
Out[X]: 0.3611111111111111 
In [X]: mapk(y_true, y_pred, k=4) 
Out[X]: 0.34722222222222215 
═════════════════════════════════════════════════════════════════════════ P@k, AP@k and MAP@k all range from 0 to 1 with 1 being the best. 
Please note that sometimes you might see different implementations of P@k and  AP@k on the internet. For example, let’s take a look at one of these  implementations.
64 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
═════════════════════════════════════════════════════════════════════════ # taken from: 
# https://github.com/benhamner/Metrics/blob/ 
# master/Python/ml_metrics/average_precision.py 
import numpy as np 
def apk(actual, predicted, k=10): 
 """ 
 Computes the average precision at k. 
 This function computes the AP at k between two lists of  items. 
 Parameters 
 ---------- 
 actual : list 
 A list of elements to be predicted (order doesn't matter)  predicted : list 
 A list of predicted elements (order does matter)  k : int, optional 
 The maximum number of predicted elements 
 Returns 
 ------- 
 score : double 
 The average precision at k over the input lists  """ 
 if len(predicted)>k: 
 predicted = predicted[:k] 
 score = 0.0 
 num_hits = 0.0 
 for i,p in enumerate(predicted): 
 if p in actual and p not in predicted[:i]: 
 num_hits += 1.0 
 score += num_hits / (i+1.0) 
 if not actual: 
 return 0.0 
 return score / min(len(actual), k) 
═════════════════════════════════════════════════════════════════════════ 
This implementation is another version of AP@k where order matters and we weigh  the predictions. This implementation will have slightly different results from what  I have presented.
65 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
Now, we come to log loss for multi-label classification. This is quite easy. You  can convert the targets to binary format and then use a log loss for each column. In  the end, you can take the average of log loss in each column. This is also known as  mean column-wise log loss. Of course, there are other ways you can implement  this, and you should explore it as you come across it. 
We have now reached a stage where we can say that we now know all binary, multi class and multi-label classification metrics, and now we can move to regression  metrics. 
The most common metric in regression is error. Error is simple and very easy to  understand. 
Error = True Value – Predicted Value 
Absolute error is just absolute of the above. 
Absolute Error = Abs ( True Value – Predicted Value ) 
Then we have mean absolute error (MAE). It’s just mean of all absolute errors. 
═════════════════════════════════════════════════════════════════════════ import numpy as np 
def mean_absolute_error(y_true, y_pred): 
 """ 
 This function calculates mae 
 :param y_true: list of real numbers, true values 
 :param y_pred: list of real numbers, predicted values  :return: mean absolute error 
 """ 
 # initialize error at 0 
 error = 0 
 # loop over all samples in the true and predicted list  for yt, yp in zip(y_true, y_pred): 
 # calculate absolute error  
 # and add to error 
 error += np.abs(yt - yp) 
 # return mean error 
 return error / len(y_true) 
═════════════════════════════════════════════════════════════════════════
66 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
Similarly, we have squared error and mean squared error (MSE). Squared Error = ( True Value – Predicted Value )2 
And mean squared error (MSE) can be implemented as follows. 
═════════════════════════════════════════════════════════════════════════ def mean_squared_error(y_true, y_pred): 
 """ 
 This function calculates mse 
 :param y_true: list of real numbers, true values 
 :param y_pred: list of real numbers, predicted values  :return: mean squared error 
 """ 
 # initialize error at 0 
 error = 0 
 # loop over all samples in the true and predicted list  for yt, yp in zip(y_true, y_pred): 
 # calculate squared error  
 # and add to error 
 error += (yt - yp) ** 2 
 # return mean error 
 return error / len(y_true) 
═════════════════════════════════════════════════════════════════════════ 
MSE and RMSE (root mean squared error) are the most popular metrics used in  evaluating regression models. 
RMSE = SQRT ( MSE ) 
Another type of error in same class is squared logarithmic error. Some people  call it SLE, and when we take mean of this error across all samples, it is known as  MSLE (mean squared logarithmic error) and implemented as follows. 
═════════════════════════════════════════════════════════════════════════ import numpy as np 
def mean_squared_log_error(y_true, y_pred): 
 """ 
 This function calculates msle 
 :param y_true: list of real numbers, true values 
 :param y_pred: list of real numbers, predicted values  :return: mean squared logarithmic error
67 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
 """ 
 # initialize error at 0 
 error = 0 
 # loop over all samples in true and predicted list 
 for yt, yp in zip(y_true, y_pred): 
 # calculate squared log error  
 # and add to error 
 error += (np.log(1 + yt) - np.log(1 + yp)) ** 2 
 # return mean error 
 return error / len(y_true) 
═════════════════════════════════════════════════════════════════════════ 
Root mean squared logarithmic error is just a square root of this. It is also known  as RMSLE.  
Then we have the percentage error: 
Percentage Error = ( ( True Value – Predicted Value ) / True Value ) * 100 Same can be converted to mean percentage error for all samples. 
═════════════════════════════════════════════════════════════════════════ def mean_percentage_error(y_true, y_pred): 
 """ 
 This function calculates mpe 
 :param y_true: list of real numbers, true values 
 :param y_pred: list of real numbers, predicted values  :return: mean percentage error 
 """ 
 # initialize error at 0 
 error = 0 
 # loop over all samples in true and predicted list 
 for yt, yp in zip(y_true, y_pred): 
 # calculate percentage error  
 # and add to error 
 error += (yt - yp) / yt 
 # return mean percentage error 
 return error / len(y_true) 
═════════════════════════════════════════════════════════════════════════ 
And an absolute version of the same (and more common version) is known as mean  absolute percentage error or MAPE.
68 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
═════════════════════════════════════════════════════════════════════════ import numpy as np 
def mean_abs_percentage_error(y_true, y_pred): 
 """ 
 This function calculates MAPE 
 :param y_true: list of real numbers, true values 
 :param y_pred: list of real numbers, predicted values  :return: mean absolute percentage error 
 """ 
 # initialize error at 0 
 error = 0 
 # loop over all samples in true and predicted list 
 for yt, yp in zip(y_true, y_pred): 
 # calculate percentage error  
 # and add to error 
 error += np.abs(yt - yp) / yt 
 # return mean percentage error 
 return error / len(y_true) 
═════════════════════════════════════════════════════════════════════════ 
The best thing about regression is that there are only a few most popular metrics  that can be applied to almost every regression problem. And it is much easier to  understand when we compare it to classification metrics.  
Let’s talk about another regression metric known as R2 (R-squared), also known  as the coefficient of determination.  
In simple words, R-squared says how good your model fits the data. R-squared  closer to 1.0 says that the model fits the data quite well, whereas closer 0 means  that model isn’t that good. R-squared can also be negative when the model just  makes absurd predictions.  
The formula for R-squared is shown in figure 10, but as always a python  implementation makes things more clear. 

Figure 10: Formula for R-squared
69 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
═════════════════════════════════════════════════════════════════════════ import numpy as np 
def r2(y_true, y_pred): 
 """ 
 This function calculates r-squared score 
 :param y_true: list of real numbers, true values 
 :param y_pred: list of real numbers, predicted values  :return: r2 score 
 """ 
  
 # calculate the mean value of true values 
 mean_true_value = np.mean(y_true) 
  
 # initialize numerator with 0 
 numerator = 0 
 # initialize denominator with 0 
 denominator = 0 
  
 # loop over all true and predicted values 
 for yt, yp in zip(y_true, y_pred): 
 # update numerator 
 numerator += (yt - yp) ** 2 
 # update denominator 
 denominator += (yt - mean_true_value) ** 2 
 # calculate the ratio 
 ratio = numerator / denominator 
 # return 1 - ratio 
 return 1 – ratio 
═════════════════════════════════════════════════════════════════════════ 
There are many more evaluation metrics, and this list is never-ending. I can write a  book which is only about different evaluation metrics. Maybe I will. For now, these  evaluations metrics will fit almost every problem you want to attempt. Please note  that I have implemented these metrics in the most straightforward manner, and that  means they are not efficient enough. You can make most of them in a very efficient  way by properly using numpy. For example, take a look at the implementation of  mean absolute error without any loops. 
═════════════════════════════════════════════════════════════════════════ import numpy as np 
def mae_np(y_true, y_pred): 
 return np.mean(np.abs(y_true - y_pred)) 
═════════════════════════════════════════════════════════════════════════
70 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
I could have implemented all the metrics this way but to learn it’s better to look at  low-level implementation. Once you learn the low-level implementation in pure  python, and without using a lot of numpy, you can easily convert it to numpy and  make it much faster. 
Then, there are some advanced metrics. 
One of them which is quite widely used is quadratic weighted kappa, also known  as QWK. It is also known as Cohen’s kappa. QWK measures the “agreement”  between two “ratings”. The ratings can be any real numbers in 0 to N. And  predictions are also in the same range. An agreement can be defined as how close  these ratings are to each other. So, it’s suitable for a classification problem with N  different categories/classes. If the agreement is high, the score is closer towards 1.0.  In the case of low agreement, the score is close to 0. Cohen’s kappa has a good  implementation in scikit-learn, and detailed discussion of this metric is beyond the  scope of this book. 
═════════════════════════════════════════════════════════════════════════ In [X]: from sklearn import metrics 
In [X]: y_true = [1, 2, 3, 1, 2, 3, 1, 2, 3] 
In [X]: y_pred = [2, 1, 3, 1, 2, 3, 3, 1, 2] 
In [X]: metrics.cohen_kappa_score(y_true, y_pred, weights="quadratic") Out[X]: 0.33333333333333337 
In [X]: metrics.accuracy_score(y_true, y_pred) 
Out[X]: 0.4444444444444444 
═════════════════════════════════════════════════════════════════════════ 
You can see that even though accuracy is high, QWK is less. A QWK greater than  0.85 is considered to be very good! 
An important metric is Matthew’s Correlation Coefficient (MCC). MCC ranges  from -1 to 1. 1 is perfect prediction, -1 is imperfect prediction, and 0 is random  prediction. The formula for MCC is quite simple. 
 TP * TN - FP * FN 
MCC = ─────────────────────────────────────  [ (TP + FP) * (FN + TN) * (FP + TN) * (TP + FN) ] ^ (0.5)
71 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
We see that MCC takes into consideration TP, FP, TN and FN and thus can be used  for problems where classes are skewed. You can quickly implement it in python by  using what we have already implemented. 
═════════════════════════════════════════════════════════════════════════ def mcc(y_true, y_pred): 
 """ 
 This function calculates Matthew's Correlation Coefficient  for binary classification. 
 :param y_true: list of true values 
 :param y_pred: list of predicted values 
 :return: mcc score 
 """ 
 tp = true_positive(y_true, y_pred) 
 tn = true_negative(y_true, y_pred) 
 fp = false_positive(y_true, y_pred) 
 fn = false_negative(y_true, y_pred) 
 numerator = (tp * tn) - (fp * fn) 
 denominator = ( 
 (tp + fp) * 
 (fn + tn) * 
 (fp + tn) * 
 (tp + fn) 
 ) 
 denominator = denominator ** 0.5 
 return numerator/denominator 
═════════════════════════════════════════════════════════════════════════ 
These are the metrics that can help you get started and will apply to almost every  machine learning problem. 
One thing to keep in mind is that to evaluate un-supervised methods, for example,  some kind of clustering, it’s better to create or manually label the test set and keep  it separate from everything that is going on in your modelling part. When you are  done with clustering, you can evaluate the performance on the test set simply by  using any of the supervised learning metrics. 
Once we understand what metric to use for a given problem, we can start looking  more deeply into our models for improvements.
72 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
Arranging machine learning projects 
Finally, we are at a stage where we can start building our very first machine learning  models.  
Or are we? 
Before we start, we must take care of a few things. Please remember that we will  work in an IDE/text editor rather than jupyter notebooks. You can also work in  jupyter notebooks, and it’s totally up to you. However, I will be using jupyter only  for things like data exploration and for plotting charts and graphs. We will build the  classification framework in such a way that most problems will become plug n’  play. You will be able to train a model without making too many changes to the  code, and when you improve your models, you will be able to track them using git. 
Let’s look at the structure of the files first of all. For any project that you are doing,  create a new folder. For this example, I am calling the project “project”. 
The inside of the project folder should look something like the following. . 
├── input 
│ ├── train.csv 
│ └── test.csv 
├── src 
│ ├── create_folds.py 
│ ├── train.py 
│ ├── inference.py 
│ ├── models.py 
│ ├── config.py 
│ └── model_dispatcher.py 
├── models 
│ ├── model_rf.bin 
│ └── model_et.bin 
├── notebooks 
│ ├── exploration.ipynb 
│ └── check_data.ipynb 
├── README.md 
└── LICENSE
73 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
Let’s see what these folders and file are about. 
input/: This folder consists of all the input files and data for your machine learning  project. If you are working on NLP projects, you can keep your embeddings here.  If you are working on image projects, all images go to a subfolder inside this folder. 
src/: We will keep all the python scripts associated with the project here. If I talk  about a python script, i.e. any *.py file, it is stored in the src folder. 
models/: This folder keeps all the trained models. 
notebooks/: All jupyter notebooks (i.e. any *.ipynb file) are stored in the notebooks  folder. 
README.md: This is a markdown file where you can describe your project and  write instructions on how to train the model or to serve this in a production  environment. 
LICENSE: This is a simple text file that consists of a license for the project, such as  MIT, Apache, etc. Going into details of the licenses is beyond the scope of this  book. 
Let’s assume you are building a model to classify MNIST dataset (a dataset that has  been used in almost every machine learning book). If you remember, we touched  MNIST dataset in cross-validation chapter too. So, I am not going to explain how  this dataset looks like. There are many different formats of MNIST dataset available  online, but we will be using the CSV format of the dataset. 
In this format of the dataset, each row of the CSV consists of the label of the image  and 784 pixel values ranging from 0 to 255. The dataset consists of 60000 images  in this format. 
We can use pandas to read this data format easily. 
Please note that even though Figure 1 shows all pixel values as zeros, it is not the  case.
74 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 

Figure 1: MNIST dataset in CSV format 
Let’s take a look at the counts of the label column in this dataset. 
Figure 2: Counts of label in MNIST dataset 
We don’t need much more exploration for this dataset. We already know what we  have, and there is no need to make plots on different pixel values. From figure 2, it  is quite clear that the distribution of labels is quite good and even. We can thus use  accuracy/F1 as metrics. This is the first step when approaching a machine learning  problem: decide the metric! 
Now, we can code a little bit. We need to create the src/ folder and some python  scripts. 
Please note that the training CSV file is located in the input/ folder and is called  mnist_train.csv. 
How should these files look like for such a project? 
The first script that one should create is create_folds.py. 
75 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
This will create a new file in the input/ folder called mnist_train_folds.csv, and it’s  the same as mnist_train.csv. The only differences are that this CSV is shuffled and  has a new column called kfold.  
Once we have decided what kind of evaluation metric we want to use and have  created the folds, we are good to go with creating a basic model. This is done in  train.py. 
═════════════════════════════════════════════════════════════════════════ # src/train.py 
import joblib 
import pandas as pd 
from sklearn import metrics 
from sklearn import tree 
def run(fold): 
 # read the training data with folds 
 df = pd.read_csv("../input/mnist_train_folds.csv") 
 # training data is where kfold is not equal to provided fold  # also, note that we reset the index 
 df_train = df[df.kfold != fold].reset_index(drop=True) 
 # validation data is where kfold is equal to provided fold  df_valid = df[df.kfold == fold].reset_index(drop=True) 
 # drop the label column from dataframe and convert it to  # a numpy array by using .values. 
 # target is label column in the dataframe 
 x_train = df_train.drop("label", axis=1).values 
 y_train = df_train.label.values 
 # similarly, for validation, we have 
 x_valid = df_valid.drop("label", axis=1).values 
 y_valid = df_valid.label.values 
 # initialize simple decision tree classifier from sklearn  clf = tree.DecisionTreeClassifier() 
 # fit the model on training data 
 clf.fit(x_train, y_train) 
 # create predictions for validation samples 
 preds = clf.predict(x_valid)
76 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
 # calculate & print accuracy 
 accuracy = metrics.accuracy_score(y_valid, preds) 
 print(f"Fold={fold}, Accuracy={accuracy}") 
 # save the model 
 joblib.dump(clf, f"../models/dt_{fold}.bin") 
if __name__ == "__main__": 
 run(fold=0) 
 run(fold=1) 
 run(fold=2) 
 run(fold=3) 
 run(fold=4) 
═════════════════════════════════════════════════════════════════════════ You can run this script by calling python train.py in the console. 
═════════════════════════════════════════════════════════════════════════ ❯ python train.py 
Fold=0, Accuracy=0.8680833333333333 
Fold=1, Accuracy=0.8685 
Fold=2, Accuracy=0.8674166666666666 
Fold=3, Accuracy=0.8703333333333333 
Fold=4, Accuracy=0.8699166666666667 
═════════════════════════════════════════════════════════════════════════ 
When you look at the training script, you will see that there are still a few more  things that are hardcoded, for example, the fold numbers, the training file and the  output folder. 
We can thus create a config file with all this information: config.py. 
═════════════════════════════════════════════════════════════════════════ # config.py 
TRAINING_FILE = "../input/mnist_train_folds.csv" 
MODEL_OUTPUT = "../models/" 
═════════════════════════════════════════════════════════════════════════ 
And we make some changes to our training script too. The training file utilizes the  config file now. Thus making it easier to change data or the model output.
77 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
═════════════════════════════════════════════════════════════════════════ # train.py 
import os 
import config 
import joblib 
import pandas as pd 
from sklearn import metrics 
from sklearn import tree 
def run(fold): 
 # read the training data with folds 
 df = pd.read_csv(config.TRAINING_FILE) 
 # training data is where kfold is not equal to provided fold  # also, note that we reset the index 
 df_train = df[df.kfold != fold].reset_index(drop=True) 
 # validation data is where kfold is equal to provided fold  df_valid = df[df.kfold == fold].reset_index(drop=True) 
 # drop the label column from dataframe and convert it to  # a numpy array by using .values. 
 # target is label column in the dataframe 
 x_train = df_train.drop("label", axis=1).values 
 y_train = df_train.label.values 
 # similarly, for validation, we have 
 x_valid = df_valid.drop("label", axis=1).values 
 y_valid = df_valid.label.values 
 # initialize simple decision tree classifier from sklearn  clf = tree.DecisionTreeClassifier() 
 # fir the model on training data 
 clf.fit(x_train, y_train) 
 # create predictions for validation samples 
 preds = clf.predict(x_valid) 
 # calculate & print accuracy 
 accuracy = metrics.accuracy_score(y_valid, preds) 
 print(f"Fold={fold}, Accuracy={accuracy}") 
 # save the model
78 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
 joblib.dump( 
 clf,  
 os.path.join(config.MODEL_OUTPUT, f"dt_{fold}.bin")  ) 
if __name__ == "__main__": 
 run(fold=0) 
 run(fold=1) 
 run(fold=2) 
 run(fold=3) 
 run(fold=4) 
═════════════════════════════════════════════════════════════════════════ 
Please note that I am not showing the difference between this training script and the  one before. Please take a careful look at both of them and find the differences  yourself. There aren’t many of them. 
There is still one more thing related to the training script that can be improved. As  you can see, we call the run function multiple times for every fold. Sometimes it’s  not advisable to run multiple folds in the same script as the memory consumption  may keep increasing, and your program may crash. To take care of this problem,  we can pass arguments to the training script. I like doing it using argparse. 
═════════════════════════════════════════════════════════════════════════ # train.py 
import argparse 
. 
. 
. 
if __name__ == "__main__": 
 # initialize ArgumentParser class of argparse 
 parser = argparse.ArgumentParser() 
 # add the different arguments you need and their type  # currently, we only need fold 
 parser.add_argument( 
 "--fold", 
 type=int 
 ) 
 # read the arguments from the command line 
 args = parser.parse_args() 
 # run the fold specified by command line arguments
79 
Approaching (Almost) Any Machine Learning Problem – Abhishek Thakur 
 run(fold=args.fold) 
═════════════════════════════════════════════════════════════════════════ Now, we can run the python script again, but only for a given fold. 
═════════════════════════════════════════════════════════════════════════ ❯ python train.py --fold 0 
Fold=0, Accuracy=0.8656666666666667 
═════════════════════════════════════════════════════════════════════════ 
If you see carefully, our fold 0 score was a bit different before. This is because of  the randomness in the model. We will come to handling randomness in later  chapters. 
Now, if you want, you can create a shell script with different commands for  different folds and run them all together, as shown below. 
═════════════════════════════════════════════════════════════════════════ #!/bin/sh 
python train.py --fold 0 
python train.py --fold 1 
python train.py --fold 2 
python train.py --fold 3 
python train.py --fold 4 
═════════════════════════════════════════════════════════════════════════ And you can run this by the following command. 
═════════════════════════════════════════════════════════════════════════ ❯ sh run.sh 
Fold=0, Accuracy=0.8675 
Fold=1, Accuracy=0.8693333333333333 
Fold=2, Accuracy=0.8683333333333333 
Fold=3, Accuracy=0.8704166666666666 
Fold=4, Accuracy=0.8685 
═════════════════════════════════════════════════════════════════════════ 
We have made quite some progress now, but if we look at our training script, we  still are limited by a few things, for example, the model. The model is hardcoded in  the training script, and the only way to change it is to modify the script. So, we will  create a new python script called model_dispatcher.py. model_dispatcher.py, as the  name suggests, will dispatch our models to our training script.
80