{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chains.openai_functions import create_structured_output_runnable\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain import OpenAI, PromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from pypdf import PdfReader\n",
    "import openai\n",
    "import glob\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "from langchain.chains.openai_functions import (\n",
    "    create_openai_fn_chain,\n",
    "    create_structured_output_chain,\n",
    "    create_openai_fn_runnable,\n",
    "    create_structured_output_runnable,\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# %load_ext dotenv\n",
    "# %dotenv ./.env\n",
    "config = load_dotenv('.env')\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "model = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read(startPage, endPage):\n",
    "    global text\n",
    "    text = []\n",
    "    cleanText = \"\"\n",
    "    pdfFileObj = open('../pdfs/textbooks/deeplearningbook_2.pdf', 'rb')\n",
    "    pdfReader = PyPDF2.PdfReader(pdfFileObj)\n",
    "    while startPage <= endPage:\n",
    "        pageObj = pdfReader.pages[startPage]\n",
    "        text += pageObj.extract_text()\n",
    "        startPage += 1\n",
    "    pdfFileObj.close()\n",
    "    for myWord in text:\n",
    "        if myWord != '\\n':\n",
    "            cleanText += myWord\n",
    "    text = cleanText.split()\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CO', 'NTE', 'NT', 'SF', 'unc', 't', 'i', 'o', 'nsf', 'f', ':', 'A', 'B→Thefunctionwithdomainandrange', 'A', 'Bf', 'g', 'f', 'g', '◦Compositionofthefunctionsandf(;)xθAfunctionofxparametrized', 'byθ.', '(Sometimeswewrite', 'f(x)andomittheargumentθtolightennotation)log', 'x', 'x', 'Naturallogarithmofσ', 'x()Logisticsigmoid,11+exp()−', 'xζ', 'x', 'x', '()', 'log(1+exp(', 'Softplus,', '))||||x', 'p', 'Lpnormofx||||x', 'L2normofxx+Positivepartof,i.e.,', 'x', 'max(0)', ',', 'x1', 'c', 'o', 'ndi', 't', 'i', 'o', 'nis1iftheconditionistrue,0otherwiseSometimesweuseafunction', 'fwhoseargumentisascalarbutapplyittoavector,matrix,ortensor:', 'f(x),', 'f(X),or', 'f(', 'X).Thisdenotestheapplicationof', 'ftothearrayelement-wise.', 'Forexample,if', 'C=', 'σ(', 'X),then', 'C', 'i', ',', 'j', ',', 'k=', 'σ(', 'X', 'i', ',', 'j', ',', 'k)forallvalidvaluesof,and.', 'i', 'j', 'kD', 'at', 'aset', 's', 'and', 'D', 'i', 'st', 'r', 'i', 'but', 'i', 'o', 'n', 'sp', 'da', 't', 'aThedatageneratingdistributionˆ', 'p', 'da', 't', 'aTheempiricaldistributiondeﬁnedbythetrainingsetXAsetoftrainingexamplesx(', ')', 'iThe-thexample(input)fromadataset', 'iy(', ')', 'iory(', ')', 'iThetargetassociatedwithx(', ')', 'iforsupervisedlearn-ingXThe', 'm', 'n×matrixwithinputexamplex(', ')', 'iinrowX', 'i', ',', ':x', 'i', 'vC', 'h', 'a', 'p', 't', 'e', 'r', '1I', 'n', 't', 'ro', 'd', 'u', 'ct', 'i', 'onInventorshavelongdreamedofcreatingmachinesthatthink.ThisdesiredatesbacktoatleastthetimeofancientGreece.ThemythicalﬁguresPygmalion,Daedalus,andHephaestusmayallbeinterpretedaslegendaryinventors,andGalatea,Talos,andPandoramayallberegardedasartiﬁciallife(', ',', 'OvidandMartin2004Sparkes1996Tandy1997', ';,;,).Whenprogrammable', 'computerswereﬁrstconceived,peoplewonderedwhethersuchmachinesmightbecomeintelligent,overahundredyearsbeforeonewasbuilt(Lovelace1842,).Today,', 'ar', 't', 'i', 'ﬁc', 'i', 'al', 'i', 'n', 't', 'e', 'l', 'l', 'i', 'g', 'e', 'nc', 'e(AI)isathrivingﬁeldwithmanypracticalapplicationsandactiveresearchtopics.Welooktointelligentsoftwaretoautomateroutinelabor,understandspeechorimages,makediagnosesinmedicineandsupportbasicscientiﬁcresearch.Intheearlydaysofartiﬁcialintelligence,theﬁeldrapidlytackledandsolvedproblemsthatareintellectually', 'diﬃcultforhumanbeingsbutrelativelystraight-forwardforcomputers—problemsthatcanbedescribedbyalistofformal,math-ematicalrules.', 'Thetruechallengetoartiﬁcialintelligenceprovedtobesolvingthetasksthatareeasyforpeopletoperformbuthardforpeopletodescribeformally—probl', 'emsthatwesolveintuitively,thatfeelautomatic,likerecognizingspokenwordsorfacesinimages.Thisbookisaboutasolutiontothesemoreintuitiveproblems.Thissolutionistoallowcomputerstolearnfromexperienceandunderstandtheworldintermsofahierarchyofconcepts,witheachconceptdeﬁnedintermsofitsrelationtosimplerconcepts.Bygatheringknowledgefromexperience,thisapproachavoidstheneedforhumanoperatorstoformallyspecifyalloftheknowledgethatthecomputerneeds.Thehierarchyofconceptsallowsthecomputertolearncomplicatedconceptsbybuildingthemoutofsimplerones.Ifwedrawagraphshowinghowthese1']\n"
     ]
    }
   ],
   "source": [
    "Read(14,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_text(filename):\n",
    "    file = open(filename, 'rb')\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    text = ''\n",
    "    for page in range(len(reader.pages)):\n",
    "        page = reader.pages[page]\n",
    "        text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_text_from_dir(dir):\n",
    "    text = ''\n",
    "    for filename in glob.glob(os.path.join(dir, '*.pdf')):\n",
    "        text += extract_pdf_text(filename) + '\\n\\n\\n\\n\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = '../pdfs/textbooks'\n",
    "text = extract_pdf_text_from_dir(pdf_dir)\n",
    "with open('../data/text/text_books.txt', 'w') as f:\n",
    "    f.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_dir = '../data/pdfs/textbooks'\n",
    "# text_dir = '../output/textbooks'\n",
    "# for filename in glob.glob(os.path.join(pdf_dir, '*.pdf')):\n",
    "#     text_name = filename.split('/')[-1].split('.')[0]\n",
    "#     if ' ' in text_name:\n",
    "#         text_name = text_name.replace(' ', '_')\n",
    "#     output_name = os.path.join(text_dir, '.pdf')\n",
    "#     # dataset = create_dataset(\n",
    "#     #     model=model_name,\n",
    "#     #     tokenizer=model_name,\n",
    "#     #     file_path=filename,\n",
    "#     #     output_path=outout_file_path,\n",
    "#     #     load_in_4bit=True\n",
    "#     # )\n",
    "#     with open(output_name, 'w') as f:\n",
    "#       f.write(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = extract_pdf_text('../data/pdfs/exam.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep L ea r ni n g\n",
      "I a n G o o d f e l l o w\n",
      "Y o s h u a B e n g i o\n",
      "A a r o n C o u r v i l l eCon \n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1933637684.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    .\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.2, openai_api_key=OPENAI_API_KEY, model='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question(BaseModel):\n",
    "    \"\"\" An Exam question \"\"\"\n",
    "    question_id: int = Field(..., description=\"The question id\")\n",
    "    qustion_type: str = Field(..., description=\"The type of question, e.g. multiple choice, short answer, long answer\")\n",
    "    question: str = Field(..., description=\"The question text\")\n",
    "    choices: list[str] = Field(..., description=\"The choices for the question\")\n",
    "    answer: str = Field(..., description=\"The answer to the question\")\n",
    "    topic: str = Field(..., description=\"The topic of the question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Questions(BaseModel):\n",
    "    \"\"\" A collection of questions \"\"\"\n",
    "    questions: list[Question] = Field(..., description=\"The questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"\"\"{\n",
    "    \"properties\": {\n",
    "        \"question_id\": {\"type\": \"string\"},\n",
    "        \"question\": {\"type\": \"string\"},\n",
    "        \"choices\": {\"type\": \"array\", \"option\": {\"type\": \"string\"}},\n",
    "        \"answer\": {\"type\": \"string\"},\n",
    "        \"question_type\": {\"type\": \"string\"},\n",
    "        \"topic\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\"question_id\", \"question\", \"answer\", \"question_type\"]\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_question_answer_data(doc, llm):\n",
    "    # input = prompt.format_prompt(exam_text=doc, schema=schema)\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a world class algorithm for extracting questions and answers from exams, and putting them in a structured format.\",\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Use the following exam text to extract the questions and answers. \\n\\n {exam_text}\",\n",
    "            ),\n",
    "            (\n",
    "                \"human\", \"Tip:  Make sure to include the question id, question, answer, and question type in the output. Also make sure to use the correct format\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    chain = create_structured_output_chain(Questions, llm, prompt, verbose=True)\n",
    "    extract = chain.run(doc)\n",
    "    return extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = extract_question_answer_data(text, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "page_content='CS230\\nSolution: (iii)\\n(e)(1 point) Consider the model deﬁned in question (d) with parameters initialized with\\nzeros.W[1]denotes the weight matrix of the ﬁrst layer. You forward propagate a batch\\nof examples, and then backpropagate the gradients and update the parameters. Which\\nof the following statements is true?\\n(i) Entries of W[1]may be positive or negative\\n(ii) Entries of W[1]are all negative\\n(iii) Entries of W[1]are all positive\\n(iv) Entries of W[1]are all zeros\\nSolution: (i)\\n(f)(2 points) Consider the layers landl−1 in a fully connected neural network:\\nThe forward propagation equations for these layers are:\\nz[l−1]=W[l−1]a[l−2]+b[l−1]\\na[l−1]=g[l−1](z[l−1])\\nz[l]=W[l]a[l−1]+b[l]\\na[l]=g[l](z[l])\\nWhich of the following propositions is true? Xavier initialization ensures that :\\n(i)Var(W[l−1]) is the same as Var(W[l]).\\n(ii)Var(b[l]) is the same as Var(b[l−1]).\\n(iii)Var(a[l]) is the same as Var(a[l−1]), at the end of training.\\n(iv)Var(a[l]) is the same as Var(a[l−1]), at the beginning of training.\\nSolution: (iv)\\n4' metadata={'source': 'data/pdfs/exam.pdf', 'page': 3}\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader('data/pdfs/exam.pdf')\n",
    "docs = loader.load_and_split()\n",
    "print(len(docs))\n",
    "print(docs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = docs[3].page_content\n",
    "# text = \"\"\"\n",
    "# Question 3 (Loss Functions, 17 points + 3 bonus points)\n",
    "# Equipped with cutting-edge Deep Learning knowledge, you are working with a biology lab.\n",
    "# Specifically, you're asked to build a classifier that predicts the animal type from a given\n",
    "# image into four ( ny= 4) classes: dog, cat, iguana, mouse . There's always exactly one\n",
    "# animal per image. You decide to use cross-entropy loss to train your network. Recall that\n",
    "# the cross-entropy (CE) loss for a single example is defined as follows:\n",
    "# LCE(^y;y) =nyP\n",
    "# i=1yilog ^yi\n",
    "# where ^y= (^y1;:::; ^yny)>represents the predicted probability distribution over the classes\n",
    "# andy= (y1;:::;yny)>is the ground truth vector, which is zero everywhere except for the\n",
    "# correct class (e.g. y= (1;0;0;0)>fordog, andy= (0;0;1;0)>foriguana ).\n",
    "# (a)(2 points) Suppose you're given an example image of an iguana. If the model correctly\n",
    "# predicts the resulting probability distribution as ^ y= (0:25;0:25;0:3;0:2)>, what is the\n",
    "# value of the cross-entropy loss? You can give an answer in terms of logarithms.\n",
    "# Solution:log 0:3\n",
    "# (b)(2 points) After some training, the model now incorrectly predicts mouse with distri-\n",
    "# butionh0:0;0:0;0:4;0:6ifor the same image. What is the new value of the cross-entropy\n",
    "# loss for this example?\n",
    "# Solution:log 0:4\n",
    "# (c)(2 points) Suprisingly, the model achieves lower loss for a misprediction than for a\n",
    "# correct prediction. Explain what implementation choices led to this phenomenon.\n",
    "# Solution: This is because our objective is to minimize CE-loss, rather than to\n",
    "# directly maximize accuracy. While CE-loss is a reasonable proxy to accuracy, there\n",
    "# is no guarantee that a lower CE loss will lead to higher accuracy.\n",
    "# (d)(2 points) Given your observation from question (c), you decide to train your neural\n",
    "# network with the accuracy as the objective instead of the cross-entropy loss. Is this a\n",
    "# good idea? Give one reason. Note that the accuracy of a model is defined as\n",
    "# Accuracy =(Number of correctly-classified examples)\n",
    "# (Total number of examples)\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = PromptTemplate.from_template(\n",
    "#     \"Read the following exam paper and extract the information requested in the schema for each question \\n\\n{exam_text}\\n\\n\\n\\n\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain = create_structured_output(schema, llm, prompt)\n",
    "# input = prompt.format_prompt(exam_text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = runnable.invoke(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions_json = extract_question_answer_data(docs[3], schema, llm)\n",
    "# with open('data/exams.json', 'w') as f:\n",
    "#     f.write(questions_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = {\n",
    "    \"title\": \"Person\",\n",
    "    \"description\": \"Identifying information about a person.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\"title\": \"Name\", \"description\": \"The person's name\", \"type\": \"string\"},\n",
    "        \"age\": {\"title\": \"Age\", \"description\": \"The person's age\", \"type\": \"integer\"},\n",
    "        \"fav_food\": {\n",
    "            \"title\": \"Fav Food\",\n",
    "            \"description\": \"The person's favorite food\",\n",
    "            \"type\": \"string\",\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"name\", \"age\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\n",
    "#             \"system\",\n",
    "#             \"You are a world class algorithm for extracting information in structured formats.\",\n",
    "#         ),\n",
    "#         (\n",
    "#             \"human\",\n",
    "#             \"Use the given format to extract information from the following input: {input}\",\n",
    "#         ),\n",
    "#         (\"human\", \"Tip: Make sure to answer in the correct format\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# runnable = create_structured_output_chain_runnable(json_schema, llm, prompt)\n",
    "# runnable.invoke({\"input\": \"Sally is 13\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aiprocessor(page_no, text, json_schema, llm=llm):\n",
    "    print(f\"\\n\\n..AI processing page {page_no}\")\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are a world class algorithm for extracting questions and answers from exams, and putting them in a structured format.\n",
    "- User input is messy raw text extracted from a PDF page by PyPDF2.\n",
    "- The goal is to identify each question and extract the details cleanly as json.\n",
    "- Make sure you get every question and answer, and use the correct format.\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"use the following schema to extract the questions and answers. \\n\\n\" + json_schema\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"raw pdf text; extract and format into json:\" + text\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    api_params = {\"model\": llm, \"messages\": messages, \"stream\": True}\n",
    "    try:\n",
    "        api_response = openai.ChatCompletion.create(**api_params)\n",
    "        reply = \"\"\n",
    "        for delta in api_response:\n",
    "            if not delta['choices'][0]['finish_reason']:\n",
    "                word = delta['choices'][0]['delta']['content']\n",
    "                reply += word\n",
    "                print(word, end =\"\")       \n",
    "        return reply\n",
    "    except Exception as err:\n",
    "        error_message = f\"API Error page {page_no}: {str(err)}\"\n",
    "        print(error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "..AI processing page 0\n",
      "{\n",
      "    \"question_id\": \"1\",\n",
      "    \"question\": \"Multiple Choice\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"14\",\n",
      "    \"question_type\": \"Multiple Choice\",\n",
      "    \"topic\": \"\"\n",
      "}\n",
      "\n",
      "..AI processing page 1\n",
      "{\n",
      "  \"question_id\": \"1\",\n",
      "  \"question\": \"Which of the following is true about max-pooling?\",\n",
      "  \"choices\": [\n",
      "    \"It allows a neuron in a network to have information about features in a larger part of the image, compared to a neuron at the same depth in a network without max pooling.\",\n",
      "    \"It increases the number of parameters when compared to a similar network without max pooling.\",\n",
      "    \"It increases the sensitivity of the network towards the position of features within an image.\"\n",
      "  ],\n",
      "  \"answer\": \"i\",\n",
      "  \"question_type\": \"Multiple Choice\",\n",
      "  \"topic\": \"CS230\"\n",
      "}\n",
      "{\n",
      "  \"question_id\": \"2\",\n",
      "  \"question\": \"In order to backpropagate through a max-pool layer, you need to pass information about the positions of the max values from the forward pass.\",\n",
      "  \"choices\": [\n",
      "    \"True\",\n",
      "    \"False\"\n",
      "  ],\n",
      "  \"answer\": \"i\",\n",
      "  \"question_type\": \"Multiple Choice\",\n",
      "  \"topic\": \"CS230\"\n",
      "}\n",
      "{\n",
      "  \"question_id\": \"3\",\n",
      "  \"question\": \"Consider a simple convolutional neural network with one convolutional layer. Which of the following statements is true about this network? (Check all that apply.)\",\n",
      "  \"choices\": [\n",
      "    \"It is scale invariant.\",\n",
      "    \"It is rotation invariant.\",\n",
      "    \"It is translation invariant.\",\n",
      "    \"All of the above.\"\n",
      "  ],\n",
      "  \"answer\": \"iii\",\n",
      "  \"question_type\": \"Multiple Choice\",\n",
      "  \"topic\": \"CS230\"\n",
      "}\n",
      "{\n",
      "  \"question_id\": \"4\",\n",
      "  \"question\": \"You are training a Generative Adversarial Network to generate nice iguana images, with mini-batch gradient descent. The generator cost J(G) is extremely low, but the generator is not generating meaningful output images. What could be the reason? (Circle all that apply.)\",\n",
      "  \"choices\": [],\n",
      "  \"answer\": \"\",\n",
      "  \"question_type\": \"\",\n",
      "  \"topic\": \"CS230\"\n",
      "}\n",
      "\n",
      "..AI processing page 2\n",
      "Here is the extracted and formatted json for the questions and answers from the provided text:\n",
      "```\n",
      "[\n",
      "  {\n",
      "    \"question_id\": \"i\",\n",
      "    \"question\": \"The discriminator has poor performance.\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"None of the above.\",\n",
      "    \"question_type\": \"\",\n",
      "    \"topic\": \"CS230\"\n",
      "  },\n",
      "  {\n",
      "    \"question_id\": \"ii\",\n",
      "    \"question\": \"Your generator is overfitting.\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"True\",\n",
      "    \"question_type\": \"\",\n",
      "    \"topic\": \"CS230\"\n",
      "  },\n",
      "  {\n",
      "    \"question_id\": \"iii\",\n",
      "    \"question\": \"Your optimizer is stuck in a saddle point.\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"None of the above.\",\n",
      "    \"question_type\": \"\",\n",
      "    \"topic\": \"CS230\"\n",
      "  },\n",
      "  {\n",
      "    \"question_id\": \"iv\",\n",
      "    \"question\": \"Mini-batch gradient descent is a better optimizer than full-batch gradient descent to avoid getting stuck in saddle points.\",\n",
      "    \"choices\": [\n",
      "      \"True\",\n",
      "      \"False\"\n",
      "    ],\n",
      "    \"answer\": \"True\",\n",
      "    \"question_type\": \"\",\n",
      "    \"topic\": \"CS230\"\n",
      "  },\n",
      "  {\n",
      "    \"question_id\": \"e\",\n",
      "    \"question\": \"Using \\\"neural style transfer\\\" (as seen in class), you want to generate an RGB image of the Great Wall of China that looks like it was painted by Picasso. The size of your image is 100x100x3 and you are using a pretrained network with 1,000,000 parameters. At every iteration of gradient descent, how many updates do you perform?\",\n",
      "    \"choices\": [\n",
      "      \"10,000\",\n",
      "      \"30,000\",\n",
      "      \"1,000,000\",\n",
      "      \"1,030,000\"\n",
      "    ],\n",
      "    \"answer\": \"30,000\",\n",
      "    \"question_type\": \"2 points\",\n",
      "    \"topic\": \"CS230\"\n",
      "  },\n",
      "  {\n",
      "    \"question_id\": \"f\",\n",
      "    \"question\": \"You are building a model to predict the presence (labeled 1) or absence (labeled 0) of a tumor in a brain scan. The goal is to ultimately deploy the model to help doctors in hospitals. Which of these two metrics would you choose to use?\",\n",
      "    \"choices\": [\n",
      "      \"Precision = True positive examples / Total predicted positive examples\",\n",
      "      \"Recall = True positive examples / Total positive examples.\"\n",
      "    ],\n",
      "    \"answer\": \"Recall = True positive examples / Total positive examples.\",\n",
      "    \"question_type\": \"2 points\",\n",
      "    \"topic\": \"CS230\"\n",
      "  }\n",
      "]\n",
      "```\n",
      "Please note that the answer format for the questions without options is assumed to be a single string, while the questions with options have the answers as one of the options. The question types for questions (e) and (f) are specified as \"2 points\".\n",
      "\n",
      "..AI processing page 3\n",
      "{\n",
      "    \"question_id\": \"4\",\n",
      "    \"question\": \"What should we do to increase recall?\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"(ii) Increase recall, because we don't want false negatives.\",\n",
      "    \"question_type\": \"\",\n",
      "    \"topic\": \"CS230\"\n",
      "}\n",
      "\n",
      "..AI processing page 4\n",
      "{\n",
      "    \"question_id\": \"CS230-Q2\",\n",
      "    \"question\": \"Consider an input image of shape 500x500x3. You flatten this image and use a fully connected layer with 100 hidden units. What is the shape of the weight matrix of this layer?\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"Weight matrix which is 750,000x100, giving 75 million.\",\n",
      "    \"question_type\": \"Short Answer\",\n",
      "    \"topic\": \"\"\n",
      "},\n",
      "{\n",
      "    \"question_id\": \"CS230-Q2\",\n",
      "    \"question\": \"Consider an input image of shape 500x500x3. You flatten this image and use a fully connected layer with 100 hidden units. What is the shape of the corresponding bias vector?\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"100x1\",\n",
      "    \"question_type\": \"Short Answer\",\n",
      "    \"topic\": \"\"\n",
      "},\n",
      "{\n",
      "    \"question_id\": \"CS230-Q2\",\n",
      "    \"question\": \"Consider an input image of shape 500x500x3. You run this image in a convolutional layer with 10 filters, of kernel size 5x5. How many parameters does this layer have?\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"5x5x3x10 and a bias value for each of the 10 filters, giving 760 parameters.\",\n",
      "    \"question_type\": \"Short Answer\",\n",
      "    \"topic\": \"\"\n",
      "},\n",
      "{\n",
      "    \"question_id\": \"CS230-Q2\",\n",
      "    \"question\": \"You forward propagate an input x in your neural network. The output probability is y^. Explain briefly what y^ represents.\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"The derivative represents how much the output changes when the input is changed. In other words, how much the input has influenced the output.\",\n",
      "    \"question_type\": \"Short Answer\",\n",
      "    \"topic\": \"\"\n",
      "},\n",
      "{\n",
      "    \"question_id\": \"CS230-Q2\",\n",
      "    \"question\": \"Why is it necessary to include non-linearities in a neural network?\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"Without nonlinear activation functions, each layer simply performs a linear mapping of the input to the output of the layer. Because linear functions are closed under composition, this is equivalent to having a single (linear) layer. Thus, no matter how many such layers exist, the network can only learn linear functions.\",\n",
      "    \"question_type\": \"Short Answer\",\n",
      "    \"topic\": \"\"\n",
      "},\n",
      "{\n",
      "    \"question_id\": \"CS230-Q2\",\n",
      "    \"question\": \"The universal approximation theorem states that a neural network with a single hidden layer can approximate any continuous function (with some assumptions on the activation). Give one reason why you would use deep networks with multiple layers.\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"\",\n",
      "    \"question_type\": \"Short Answer\",\n",
      "    \"topic\": \"\"\n",
      "}\n",
      "\n",
      "..AI processing page 5\n",
      "{\n",
      "    \"question_id\": \"f\",\n",
      "    \"question\": \"Consider a dataset fx(1);x(2);:::;x(m)gwhere each example x(i)contains a sequence of 100 numbers: x(i)= (x(i)<1>;x(i)<2>;:::;x(i)<100>) You have a very accurate (but not perfect) model that predicts x<t+1>from (x<1>;:::;x<t>). Givenx<1>, you want to generate ( x<2>;:::;x<100>) by repeatedly running your model. Your method is:\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"1. Predict ^x<2>fromx<1>\\n2. Predict ^x<3>from (x<1>;^x<2>)\\n3. ...\\n4. Predict ^x<100>from (x<1>;^x<2>;:::;^x<99>)\",\n",
      "    \"question_type\": \"text\",\n",
      "    \"topic\": \"CS230\"\n",
      "}\n",
      "{\n",
      "    \"question_id\": \"g\",\n",
      "    \"question\": \"You want to use the \fgure below to explain the concept of early stopping to a friend. Fill-in the blanks. (1) and (2) describe the axes. (3) and (4) describe values on the vertical and horizontal axis. (5) and (6) describe the curves. Be precise.\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"(1) and (2) describe the axes: (1) = number of iterations/train steps, (2) = loss function value/accuracy\\n(3) and (4) describe values on the vertical and horizontal axis: (3) = loss function value/error, (4) = number of iterations/train steps\\n(5) and (6) describe the curves: (5) = training loss/accuracy curve, (6) = validation loss/accuracy curve\",\n",
      "    \"question_type\": \"text\",\n",
      "    \"topic\": \"CS230\"\n",
      "}\n",
      "\n",
      "..AI processing page 6\n",
      "{\n",
      "    \"question_id\": \"CS230-1\",\n",
      "    \"question\": \"Look at the grayscale image at the top of the collection of images below. Deduce what type of convolutional filter was used to get each of the lower images. Explain briefly and include the values of these filters. The filters have a shape of (3,3).\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"\",\n",
      "    \"question_type\": \"open-ended\",\n",
      "    \"topic\": \"Convolutional Filters\"\n",
      "}\n",
      "\n",
      "{\n",
      "    \"question_id\": \"CS230-2\",\n",
      "    \"question\": \"What are the different metrics used to evaluate the performance of a machine learning model during training?\",\n",
      "    \"choices\": [\n",
      "        \"Loss value (or error)\",\n",
      "        \"Number of iterations\",\n",
      "        \"Best validation loss value (or best validation error)\",\n",
      "        \"Optimal stopping point (or early stopping point)\",\n",
      "        \"Validation loss (or validation error)\",\n",
      "        \"Training loss (or training error)\"\n",
      "    ],\n",
      "    \"answer\": \"\",\n",
      "    \"question_type\": \"multiple-choice\",\n",
      "    \"topic\": \"Model Evaluation\"\n",
      "}\n",
      "\n",
      "..AI processing page 7\n",
      "{\n",
      "    \"question_id\": \"1\",\n",
      "    \"question\": \"When the input is 2-dimensional, you can plot the decision boundary of your neural network and clearly see if there is overfitting. How do you check overfitting if the input is 10-dimensional?\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"Compute cost function in the dev and training set. If there is a significant difference, then you have a variance problem.\",\n",
      "    \"question_type\": \"i\",\n",
      "    \"topic\": \"CS230\"\n",
      "}\n",
      "{\n",
      "    \"question_id\": \"2\",\n",
      "    \"question\": \"What is the advantage of using Inverted Dropout compared to its older version (\\Normal\\\" dropout)? Mention the difference of implementation.\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"Implementation difference: Add line of code, a[L] == keepprobThe expected value of the activation doesn't decrease with this extra line of code, so the activations do not need to be rescaled at test time.\",\n",
      "    \"question_type\": \"j\",\n",
      "    \"topic\": \"CS230\"\n",
      "}\n",
      "{\n",
      "    \"question_id\": \"3\",\n",
      "    \"question\": \"Give a method to fight exploding gradient in fully-connected neural networks.\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"Gradient clipping.\",\n",
      "    \"question_type\": \"k\",\n",
      "    \"topic\": \"CS230\"\n",
      "}\n",
      "{\n",
      "    \"question_id\": \"4\",\n",
      "    \"question\": \"You are using an Adam optimizer. Show why the bias correction naturally disappears when the numbers of steps to compute the exponential moving averages gets large.\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"Vt-1 * \ft -> Vt when t->1\",\n",
      "    \"question_type\": \"l\",\n",
      "    \"topic\": \"CS230\"\n",
      "}\n",
      "\n",
      "..AI processing page 8\n",
      "[\n",
      "  {\n",
      "    \"question_id\": \"(i)\",\n",
      "    \"question\": \"You would like to detect the 3x3 cross in Image 1. Hand-engineer a filter to be convolved over Image 1. Given the output of this \\\"convolved\\\" operation, the algorithm (?) should give you the position of the cross.\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"0\",\n",
      "    \"question_type\": \"multiple choice\",\n",
      "    \"topic\": \"CS230\"\n",
      "  },\n",
      "  {\n",
      "    \"question_id\": \"(ii)\",\n",
      "    \"question\": \"You would like to detect the 3x3 red cross in Image 2 but not the blue cross. Hand-engineer a filter to be convolved over Image 2. Given the output of this \\\"convolved\\\" operation, the algorithm (?) should give you the position of the red cross.\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"0\",\n",
      "    \"question_type\": \"multiple choice\",\n",
      "    \"topic\": \"CS230\"\n",
      "  }\n",
      "]\n",
      "\n",
      "..AI processing page 9\n",
      "{\n",
      "    \"question_id\": \"3\",\n",
      "    \"question\": \"Suppose you're given an example image of an iguana. If the model correctly predicts the resulting probability distribution as ^ y= (0:25;0:25;0:3;0:2)>, what is the value of the cross-entropy loss? You can give an answer in terms of logarithms.\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"log 0:3\",\n",
      "    \"question_type\": \"(a)\",\n",
      "    \"topic\": \"Loss Functions\"\n",
      "}\n",
      "\n",
      "{\n",
      "    \"question_id\": \"3\",\n",
      "    \"question\": \"After some training, the model now incorrectly predicts mouse with distributionh0:0;0:0;0:4;0:6ifor the same image. What is the new value of the cross-entropy loss for this example?\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"log 0:4\",\n",
      "    \"question_type\": \"(b)\",\n",
      "    \"topic\": \"Loss Functions\"\n",
      "}\n",
      "\n",
      "{\n",
      "    \"question_id\": \"3\",\n",
      "    \"question\": \"Suprisingly, the model achieves lower loss for a misprediction than for a correct prediction. Explain what implementation choices led to this phenomenon.\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"This is because our objective is to minimize CE-loss, rather than to directly maximize accuracy. While CE-loss is a reasonable proxy to accuracy, there is no guarantee that a lower CE loss will lead to higher accuracy.\",\n",
      "    \"question_type\": \"(c)\",\n",
      "    \"topic\": \"Loss Functions\"\n",
      "}\n",
      "\n",
      "{\n",
      "    \"question_id\": \"3\",\n",
      "    \"question\": \"Given your observation from question (c), you decide to train your neural network with the accuracy as the objective instead of the cross-entropy loss. Is this a good idea? Give one reason. Note that the accuracy of a model is de\fned as Accuracy =(Number of correctly-classi\fed examples) (Total number of examples)\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"No, it is not a good idea because the accuracy may not be a differentiable objective for optimization. Cross-entropy loss provides gradients that can be used for backpropagation and updating the weights of the neural network.\",\n",
      "    \"question_type\": \"(d)\",\n",
      "    \"topic\": \"Loss Functions\"\n",
      "}\n",
      "\n",
      "..AI processing page 10\n",
      "{\n",
      "    \"question_id\": \"CS230_1\",\n",
      "    \"question\": \"After tuning the model architecture, why does the cross-entropy loss never reach zero if softmax activation is used?\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"The cross-entropy loss can never be zero if a softmax activation is used because the term nyP i=1 ezi is never equal to ezc, so the loss will never reach zero, although it will get very close to zero at the end of training.\",\n",
      "    \"question_type\": \"explanation\",\n",
      "    \"topic\": \"Neural Networks\"\n",
      "}\n",
      "\n",
      "{\n",
      "    \"question_id\": \"CS230_2\",\n",
      "    \"question\": \"How can new images be labeled when each example can simultaneously belong to multiple classes?\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"New images can be labeled using multi-hot encoding, where each class is represented by a binary element in a vector. For example, (1;0;0;1) would indicate that the image belongs to the first and fourth classes.\",\n",
      "    \"question_type\": \"explanation\",\n",
      "    \"topic\": \"Image Classification\"\n",
      "}\n",
      "\n",
      "{\n",
      "    \"question_id\": \"CS230_3\",\n",
      "    \"question\": \"Why is it problematic to retrain a new model with the same architecture (softmax output activation with cross-entropy loss) for chimeras as well as normal animals?\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"Retraining a new model with the same architecture is problematic because the softmax output activation assumes that each example belongs to a single class. However, chimeras can have multiple classes associated with them, making the softmax activation unsuitable for this task.\",\n",
      "    \"question_type\": \"explanation\",\n",
      "    \"topic\": \"Neural Networks\"\n",
      "}\n",
      "\n",
      "..AI processing page 11\n",
      "{\n",
      "    \"question_id\": \"CS230_q1\",\n",
      "    \"question\": \"Propose a different activation function for the last layer and a loss function that are better suited for this multi-class labeling task.\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"We can formulate this as n independent logistic regression tasks, each trying to predict whether the example belongs to the corresponding class or not. Then the loss can simply be the average of n logistic losses over all classes.\",\n",
      "    \"question_type\": \"short-answer\",\n",
      "    \"topic\": \"activation function, loss function\"\n",
      "}\n",
      "\n",
      "{\n",
      "    \"question_id\": \"CS230_q2\",\n",
      "    \"question\": \"Prove the following lower bound on the cross-entropy loss for an example with L correct classes.\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"The lower bound is L log(L).\",\n",
      "    \"question_type\": \"short-answer\",\n",
      "    \"topic\": \"cross-entropy loss\"\n",
      "}\n",
      "\n",
      "..AI processing page 12\n",
      "{\n",
      "    \"question_id\": \"CS230_4\",\n",
      "z(i \"question\": \"In this question, you will explore the importance of good balance between positive and negative labeled examples within a mini-batch, especially when your network includes batch normalization layers. Recall the batch normalization layer takes values z= (z(1);:::;z(m)) as input and computes znorm= (z(1) norm;:::;z(m) norm) according to: z(i) norm=z(i)\u0000\u0016p (\u001b2) +\u000fwhere\u0016=1 mmX i=1z(i)(\u001b2) =1 mmX i=1(z(i)\u0000\u0016)2 It then applies a second transformation to get ez= (ez(1);:::;ez(m)) using learned parameters and \f: ez(i)= ) norm+ \u000e\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"(a)(1 point) Explain the purpose of \u000fin the expression for z(i) norm.\",\n",
      "    \"question_type\": \"multiple_choice\",\n",
      "    \"topic\": \"Batch Norm\"\n",
      "},\n",
      "{\n",
      "    \"question_id\": \"CS230_4_solution\",\n",
      "    \"question\": \"\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"It prevents division by 0 for features with variance 0.\",\n",
      "    \"question_type\": \"solution\",\n",
      "    \"topic\": \"Batch Norm\"\n",
      "},\n",
      "{\n",
      "    \"question_id\": \"CS230_5\",\n",
      "    \"question\": \"You want to use a neural network with batch normalization layers to build a classifier that can distinguish a hot dog from not a hot dog. (For the rest of this question, assume \u000f= 0.)\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"\",\n",
      "    \"question_type\": \"multiple_choice\",\n",
      "    \"topic\": \"Batch Norm\"\n",
      "}\n",
      "\n",
      "..AI processing page 13\n",
      "{\n",
      "    \"question_id\": \"CS230_b_2\",\n",
      "    \"question\": \"What is z_norm? Express your answer as a matrix with shape (3, 4).\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"14\",\n",
      "    \"question_type\": \"multiple_choice\",\n",
      "    \"topic\": \"CS230\"\n",
      "}\n",
      "\n",
      "..AI processing page 14\n",
      "[\n",
      "    {\n",
      "        \"question_id\": \"0\",\n",
      "        \"question\": \"(c)(2 points) Suppose = (1;1;1) and\f= (0;\\u000010;10). What is ez? Express your answer as a matrix with shape (3, 4).\",\n",
      "        \"choices\": [],\n",
      "        \"answer\": \"BBB@\\u00001 1 1\\u00001\\n\\u00001 1 1\\u00001\\n\\u00001 1 1\\u000011\\nCCCA\",\n",
      "        \"question_type\": \"(c)\",\n",
      "        \"topic\": \"CS230\"\n",
      "    },\n",
      "    {\n",
      "        \"question_id\": \"1\",\n",
      "        \"question\": \"(d)(2 points) Give 2 bene\fts of using a batch normalization layer.\",\n",
      "        \"choices\": [],\n",
      "        \"answer\": \"Solution: We accept any of these: (i) accelerates learning by reducing covariate\\nshift, decoupling dependence of layers, and/or allowing for higher learning rates/\\ndeeper networks, (ii) accelerates learning by normalizing contours of output dis-\\ndistribution to be more uniform across dimensions, (iii) Regularizes by using batch\\nstatistics as noisy estimates of the mean and variance for normalization (reducing\\nlikelihood of over\ftting), (iv) mitigates poor weights initialization and/or vari-\\nability in scale of weights, (v) mitigates vanishing/exploding gradient problems,\\n(vi) constrains output of each layer to relevant regions of an activation function,\\nand/or stabilizes optimization process, (vii) mitigates linear discrepancies between\\nbatches, (viii) improves expressiveness of the model by including additional learned\\nparameters, \\rnand\f, producing improved loss.\\nPartial credit was awarded to responses which cited multiple bene\fts from the\\nsame category.\\nSome responses that were intentionally not accepted: (i) symmetry-breaking, (ii)\\nadds noise (without mentioning regularization), (iii) normalizes input distribution\\n(without mentioning accelerated training), (iv) adds stability (without mentioning\\noptimization process), (v) faster (without mentioning learning or training) (vi)\\nreduces variance (without mentioning of what)\",\n",
      "        \"question_type\": \"(d)\",\n",
      "        \"topic\": \"CS230\"\n",
      "    },\n",
      "    {\n",
      "        \"question_id\": \"2\",\n",
      "        \"question\": \"(e)(2 points) Explain what would go wrong if the batch normalization layer only applied\\nthe \frst transformation ( znorm).\",\n",
      "        \"choices\": [],\n",
      "        \"answer\": \"15\",\n",
      "        \"question_type\": \"(e)\",\n",
      "        \"topic\": \"CS230\"\n",
      "    }\n",
      "]\n",
      "\n",
      "..AI processing page 15\n",
      "[\n",
      "  {\n",
      "    \"question_id\": \"f\",\n",
      "    \"question\": \"Why is this approach preferred over using the mini-batch statistics during training and at test time?\",\n",
      "    \"choices\": [\n",
      "      \"Moving averages of the mean and variance produce a normalization that's more consistent with the transformation the network used to learn during training than the mini-batch statistics.\",\n",
      "      \"Moving averages of the mean and variance produce a consistent normalization for an example, that's independent of the other examples in the batch.\"\n",
      "    ],\n",
      "    \"answer\": \"Moving averages of the mean and variance produce a normalization that's more consistent with the transformation the network used to learn during training than the mini-batch statistics.\",\n",
      "    \"question_type\": \"multiple_choice\",\n",
      "    \"topic\": \"CS230\"\n",
      "  }\n",
      "]\n",
      "\n",
      "..AI processing page 16\n",
      "{\n",
      "    \"question_id\": \"g\",\n",
      "    \"question\": \"Explain why inclusion of the batch normalization layer causes a discrepancy between training error and testing error when mini-batches are constructed with poor mixing.\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"During training, batches on average get normalized according to a statistic drawn from each of the distributions; however, at test time, batches get normalized according to the mean of both of the distributions, which never occurred during training. This leads to a discrepancy between training error and testing error.\",\n",
      "    \"question_type\": \"Short answer\",\n",
      "    \"topic\": \"Batch normalization\"\n",
      "}\n",
      "\n",
      "..AI processing page 17\n",
      "{\n",
      "    \"question_id\": \"18\",\n",
      "    \"question\": \"Rubric +1 for mentioning the moving averages are different from the mini-batch statistics. +1 for describing how the moving averages are different from the mini-batch statistics (as in the figure above or using the example numbers provided). +1 for describing this is caused by the way we constructed the mini-batches. +1 for describing how this causes a disparity in training/testing.\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"\",\n",
      "    \"question_type\": \"\",\n",
      "    \"topic\": \"\"\n",
      "}\n",
      "\n",
      "..AI processing page 18\n",
      "{\n",
      "    \"question_id\": \"5\",\n",
      "    \"question\": \"In this question, you will implement a fully-connected network. The architecture is LINEAR\\n!RELU!DROPOUT!BATCHNORM . This is a dummy architecture that has been\\nmade up for this exercise.\\nThe code below implements the forward propagation, but some parts are missing. You will\\nneed to \fll the parts between the tags (START CODE HERE) and (END CODE HERE)\\nbased on the given instructions . Note that we are using only numpy (not tensor\\now),\\nand the relu function has been imported for you.\\n\\nimport numpy as np\\nfrom utils import relu\\ndef forward_propagation_with_dropout_and_batch_norm(X, parameters, keep_prob = 0.5):\\n\\\"\\\"\\\"\\nImplements the forward propagation: LINEAR -> RELU -> DROPOUT -> BATCHNORM.\\nArguments:\\nX -- input data of shape (n_x, m)\\nparameters -- python dictionary containing your parameters:\\nW -- weight matrix of shape (n_y, n_x)\\nb -- bias vector of shape (n_y, 1)\\nkeep_prob -- probability of keeping a neuron active during drop-out, scalar\\ngamma -- shifting parameter of the batch normalization layer, scalar\\nbeta -- scaling parameter of the batch normalization layer, scalar\\nepsilon -- stability parameter of the batch normalization layer, scalar\\nReturns:\\nA2 -- output of the forward propagation\\ncache -- tuple, information stored for computing the backward propagation\\n\\\"\\\"\\\"\\n# retrieve parameters\\nW = parameters[\\\"W\\\"]\\nb = parameters[\\\"b\\\"]\\ngamma = parameters[\\\"gamma\\\"]\\nbeta = parameters[\\\"beta\\\"]\\nepsilon = parameters[\\\"epsilon\\\"]\\n### START CODE HERE ###\\n# LINEAR\\nZ1 =\\n# RELU\\nA1 =\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"Z1 and A1\",\n",
      "    \"question_type\": \"coding\",\n",
      "    \"topic\": \"Numpy\"\n",
      "}\n",
      "\n",
      "..AI processing page 19\n",
      "{\n",
      "    \"question_id\": \"CS230-1\",\n",
      "    \"question\": \"What is the purpose of dropout in neural networks?\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"To prevent overfitting by randomly dropping out neurons during training\",\n",
      "    \"question_type\": \"Multiple Choice\",\n",
      "    \"topic\": \"DROPOUT\"\n",
      "}\n",
      "{\n",
      "    \"question_id\": \"CS230-2\",\n",
      "    \"question\": \"What is the purpose of batch normalization in neural networks?\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"To normalize the activations of a previous layer and improve training speed\",\n",
      "    \"question_type\": \"Multiple Choice\",\n",
      "    \"topic\": \"BATCHNORM\"\n",
      "}\n",
      "\n",
      "..AI processing page 20\n",
      "{\n",
      "  \"question_id\": \"1\",\n",
      "  \"question\": \"What is the purpose of the epsilon parameter in the batch normalization layer in CS230?\",\n",
      "  \"choices\": [],\n",
      "  \"answer\": \"The epsilon parameter is used for stability during normalization.\",\n",
      "  \"question_type\": \"multiple_choice\",\n",
      "  \"topic\": \"CS230\"\n",
      "},\n",
      "{\n",
      "  \"question_id\": \"2\",\n",
      "  \"question\": \"What are the returns of the forward propagation in CS230?\",\n",
      "  \"choices\": [],\n",
      "  \"answer\": \"The returns of the forward propagation are A2 (output) and cache (information for backward propagation).\",\n",
      "  \"question_type\": \"multiple_choice\",\n",
      "  \"topic\": \"CS230\"\n",
      "},\n",
      "{\n",
      "  \"question_id\": \"3\",\n",
      "  \"question\": \"What is the purpose of the dropout step in the forward propagation in CS230?\",\n",
      "  \"choices\": [],\n",
      "  \"answer\": \"The purpose of the dropout step is to randomly shut down neurons to prevent overfitting.\",\n",
      "  \"question_type\": \"multiple_choice\",\n",
      "  \"topic\": \"CS230\"\n",
      "},\n",
      "{\n",
      "  \"question_id\": \"4\",\n",
      "  \"question\": \"What are the steps involved in the batch normalization process in CS230?\",\n",
      "  \"choices\": [],\n",
      "  \"answer\": \"The steps involved in the batch normalization are: 1. Compute the vector of means. 2. Compute the vector of variances. 3. Normalize the activations using the means and variances. 4. Scale and shift the normalized activations.\",\n",
      "  \"question_type\": \"multiple_choice\",\n",
      "  \"topic\": \"CS230\"\n",
      "},\n",
      "{\n",
      "  \"question_id\": \"5\",\n",
      "  \"question\": \"What is the purpose of the gamma and beta parameters in the batch normalization process in CS230?\",\n",
      "  \"choices\": [],\n",
      "  \"answer\": \"The purpose of the gamma and beta parameters is to scale and shift the normalized activations.\",\n",
      "  \"question_type\": \"multiple_choice\",\n",
      "  \"topic\": \"CS230\"\n",
      "}\n",
      "}\n",
      "\n",
      "..AI processing page 21\n",
      "{\n",
      "    \"question_id\": \"A2\",\n",
      "    \"question\": \"What is cache?\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"Cache is a smaller, faster memory component that stores copies of data from frequently used main memory locations.\",\n",
      "    \"question_type\": \"definition\",\n",
      "    \"topic\": \"CS230\"\n",
      "}\n",
      "\n",
      "..AI processing page 22\n",
      "{\n",
      "    \"question_id\": \"6\",\n",
      "    \"question\": \"Who let the cars in?\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"\",\n",
      "    \"question_type\": \"short_answer\",\n",
      "    \"topic\": \"CS230\"\n",
      "}\n",
      "\n",
      "{\n",
      "    \"question_id\": \"6(a)\",\n",
      "    \"question\": \"You pick a pair of (different) unseen images of the same car and compute the loss L. The loss is extremely low, but your model performs poorly when deployed. What could be a possible problem with your approach?\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"Using pairs causes all encodings to be clustered tightly in the embedding space. Cannot differentiate between different objects. Having the same encoding for all objects gives minimum MSE.\",\n",
      "    \"question_type\": \"short_answer\",\n",
      "    \"topic\": \"CS230\"\n",
      "}\n",
      "\n",
      "..AI processing page 23\n",
      "{\n",
      "    \"question_id\": \"CS230(b)\",\n",
      "    \"question\": \"Explain how you would you use this newly trained network to decide whether a car in front of your mansion should be allowed to enter.\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"Encode all examples in the database D using the newly trained network. Encode the image of the car waiting in front of the mansion. Use the Nearest Neighbors algorithm with a thresholding to decide if this car is in the database or not.\",\n",
      "    \"question_type\": \"explanation\",\n",
      "    \"topic\": \"CS230\"\n",
      "}\n",
      "\n",
      "..AI processing page 24\n",
      "{\n",
      "    \"question_id\": \"7\",\n",
      "    \"question\": \"An X-neuron, as opposed to a neuron, takes vectors as input, and outputs vectors. There are three stages in the forward propagation for an X-neuron.\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"25\",\n",
      "    \"question_type\": \"X-Network\",\n",
      "    \"topic\": \"\"\n",
      "}\n",
      "\n",
      "..AI processing page 25\n",
      "{\n",
      "    \"question_id\": \"CS230-1\",\n",
      "    \"question\": \"Given that all X-neurons in your network output a vector of shape ( D;1), what is the dimension of W[2] for a single X-neuron in layer 2 of your X-network?\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"DxD\",\n",
      "    \"question_type\": \"multiple_choice\",\n",
      "    \"topic\": \"CS230\"\n",
      "}\n",
      "\n",
      "{\n",
      "    \"question_id\": \"CS230-2\",\n",
      "    \"question\": \"Given that all X-neurons in your network output a vector of shape ( D;1), what is the dimension of W[1] for a single X-neuron in layer 1 of your network?\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"Dxnx\",\n",
      "    \"question_type\": \"multiple_choice\",\n",
      "    \"topic\": \"CS230\"\n",
      "}\n",
      "\n",
      "{\n",
      "    \"question_id\": \"CS230-3\",\n",
      "    \"question\": \"For the jth X-neuron of the 2nd layer, how many weights (c[l]ij) does it have in its stage 2?\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"26\",\n",
      "    \"question_type\": \"multiple_choice\",\n",
      "    \"topic\": \"CS230\"\n",
      "}\n",
      "\n",
      "..AI processing page 26\n",
      "{\n",
      "    \"question_id\": \"1\",\n",
      "    \"question\": \"How would you vectorize this sum as a matrix product?\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"Stack all ^ujjiin matrix that is D\u0002n[1]multiply with vector c[2]i which isn[1]\u00021\",\n",
      "    \"question_type\": \"Multiple Choice\",\n",
      "    \"topic\": \"CS230\"\n",
      "}\n",
      "{\n",
      "    \"question_id\": \"2\",\n",
      "    \"question\": \"What is the dimension of the output of stage 2 of the jthX-neuron?\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"D\u00021.\",\n",
      "    \"question_type\": \"Multiple Choice\",\n",
      "    \"topic\": \"CS230\"\n",
      "}\n",
      "{\n",
      "    \"question_id\": \"3\",\n",
      "    \"question\": \"What is the dimension of this gradient?\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"Dimension (D, 1)\",\n",
      "    \"question_type\": \"Multiple Choice\",\n",
      "    \"topic\": \"CS230\"\n",
      "}\n",
      "{\n",
      "    \"question_id\": \"4\",\n",
      "    \"question\": \"Given this derivative, write the derivative@L\\n@s[2]\\n1in terms of@L\\n@a[2]\\n1and\\na[2]\\n1?\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"@L\\n@a[2]\\n1Ja[2]\\n1J(1\u0000a[2]\\n1)\",\n",
      "    \"question_type\": \"Multiple Choice\",\n",
      "    \"topic\": \"CS230\"\n",
      "}\n",
      "{\n",
      "    \"question_id\": \"5\",\n",
      "    \"question\": \"Using the derivative calculated in question (d), write down the gradient\\n@L\\n@W[2]\\n1i.\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"c1i\u0012\\n@L\\n@a[2]\\n1J(1\u0000a[2]\\n1)Ja[2]\\n1)\u0013\\na[1]T\\ni\",\n",
      "    \"question_type\": \"Multiple Choice\",\n",
      "    \"topic\": \"CS230\"\n",
      "}\n",
      "{\n",
      "    \"question_id\": \"6\",\n",
      "    \"question\": \"You decide to use your X-network to predict if an image contains a falafel or not. The falafel is a type of food, and this is a binary classi\fcation application. Given an image, you run it through your X-network, and use the output activation a[3]1 to make a prediction. You do this by using the L2 norm of the output as a measure of probability of the presence of a falafel. You de\fne the loss function for one training example as follows:L=ymax(0;m+\u0000jja[3]1jj)2+\u0015(1\u0000y) max(0;jja[3]1jj\u0000m\u0000)2You have set \u0015= 1,m\u0000= 0:1 andm+= 0:9 (they are hyperparameters). Here, yis the label which tells you whether the image truly has a falafel ( y= 1) or not ( y= 0).\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"\",\n",
      "    \"question_type\": \"\",\n",
      "    \"topic\": \"CS230\"\n",
      "}\n",
      "{\n",
      "    \"question_id\": \"7\",\n",
      "    \"question\": \"What is the value of y?\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"The label which tells you whether the image truly has a falafel (y= 1) or not (y= 0)\",\n",
      "    \"question_type\": \"Multiple Choice\",\n",
      "    \"topic\": \"CS230\"\n",
      "}\n",
      "\n",
      "..AI processing page 27\n",
      "{\n",
      "    \"question_id\": \"f\",\n",
      "    \"question\": \"Draw the graph of Lagainstjja[3] assuming the picture contains a falafel.\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"Solution not provided\",\n",
      "    \"question_type\": \"graph\",\n",
      "    \"topic\": \"CS230\"\n",
      "}\n",
      "{\n",
      "    \"question_id\": \"g(i)\",\n",
      "    \"question\": \"What is the maximum value the loss can have?\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"0.12\",\n",
      "    \"question_type\": \"calculation\",\n",
      "    \"topic\": \"CS230\"\n",
      "}\n",
      "{\n",
      "    \"question_id\": \"g(ii)\",\n",
      "    \"question\": \"What is the minimum value of the loss and when is this minimum reached?\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"The minimum value is 0 and it is reached when jja[3] 1jj is less than 0.1.\",\n",
      "    \"question_type\": \"text\",\n",
      "    \"topic\": \"CS230\"\n",
      "}\n",
      "\n",
      "..AI processing page 28\n",
      "[\n",
      "    {\n",
      "        \"question_id\": \"Q8\",\n",
      "        \"question\": \"Let's say you have a website with a lot of traffic. You would like to build a network that computes the probability of a user clicking on a given advertisement on your website. This is a supervised learning setting. What dataset do you need to train such a network?\",\n",
      "        \"choices\": [],\n",
      "        \"answer\": \"Pairs (~ x;y) such that: {~ x: Features of the user + advertisement {y: Binary label (0/1)\",\n",
      "        \"question_type\": \"Practical case study\",\n",
      "        \"topic\": \"Online advertising\"\n",
      "    },\n",
      "    {\n",
      "        \"question_id\": \"Q8b\",\n",
      "        \"question\": \"Choose an appropriate cost function for the problem and give the formula of the cost.\",\n",
      "        \"choices\": [],\n",
      "        \"answer\": \"Pm i=1(yilog(^yi) + (1-yi)log(1-^yi)) Summation over all training examples.\",\n",
      "        \"question_type\": \"Practical case study\",\n",
      "        \"topic\": \"Online advertising\"\n",
      "    },\n",
      "    {\n",
      "        \"question_id\": \"Q8c\",\n",
      "        \"question\": \"Your website sells sport footwear and you have already collected a dataset of 1 million examples from past visits. Your friend, who works in the high fashion industry, offers to let you use their dataset as it has similar descriptors. However, you are concerned about the impact of this different distribution dataset in the performance of your predictive system. Explain how you would use your friend's dataset.\",\n",
      "        \"choices\": [],\n",
      "        \"answer\": \"Training set. Reasons include: {Neural networks are very data intensive. Therefore, opportunities to increase the dataset should not be missed. {The new data in the training set could help the neural network to learn better lower level features (as you have more data) {Using the new data in the dev/test set would change the goal/target of the optimization process. Thus, the optimized system would not perform well with real world data.\",\n",
      "        \"question_type\": \"Practical case study\",\n",
      "        \"topic\": \"Online advertising\"\n",
      "    },\n",
      "    {\n",
      "        \"question_id\": \"Q8d\",\n",
      "        \"question\": \"How would you assess the impact of the new dataset?\",\n",
      "        \"choices\": [],\n",
      "        \"answer\": \"Split the training set in a new training set + train-dev set (made exclusively of old training examples). Measure the difference of accuracy between the new training set and train-dev set. If there is a significant difference, then the distribution mismatch between the old and new images is a problem.\",\n",
      "        \"question_type\": \"Practical case study\",\n",
      "        \"topic\": \"Online advertising\"\n",
      "    }\n",
      "]\n",
      "\n",
      "..AI processing page 29\n",
      "{\n",
      "    \"question_id\": \"(e)\",\n",
      "    \"question\": \"Initially, you decide to build a fully connected neural network for the problem. This baseline model would have L hidden layers, one input layer and an output layer. The number of neurons in the lth layer isn[l]. Write down the number of parameters of this model as a function of L and n[l] for l = 0, ..., L. Note: The input layer is considered to be layer number 0\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"PL\\ni=0(n[l+1] * (n[l] + n[l+1])) = PL\\ni=0(n[l+1] * (n[l] + 1))\",\n",
      "    \"question_type\": \"short answer\",\n",
      "    \"topic\": \"CS230\"\n",
      "}\n",
      "{\n",
      "    \"question_id\": \"(f)\",\n",
      "    \"question\": \"Based on the information provided in the graph below, what type of problem will you encounter as the number of hidden layer approaches 10? Mention possible solutions to this problem.\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"Overfitting problem. Possible solutions are: \\n- Regularization: Parameter norm penalties (L2/L1), Dropout. \\n- Reduce complexity of the neural network (decrease number of neurons and hidden layers) \\n- Increase the training dataset. Data Augmentation/GANs\",\n",
      "    \"question_type\": \"short answer\",\n",
      "    \"topic\": \"CS230\"\n",
      "}\n",
      "\n",
      "..AI processing page 30\n",
      "{\n",
      "    \"question_id\": \"CS230-31\",\n",
      "    \"question\": \"\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"\",\n",
      "    \"question_type\": \"\",\n",
      "    \"topic\": \"\"\n",
      "}\n",
      "\n",
      "..AI processing page 31\n",
      "{\n",
      "  \"question_id\": \"32\",\n",
      "  \"question\": \"What is the purpose of CS230?\",\n",
      "  \"choices\": [],\n",
      "  \"answer\": \"To extract and format raw PDF text into JSON\",\n",
      "  \"question_type\": \"multiple choice\",\n",
      "  \"topic\": \"CS230\"\n",
      "}\n",
      "\n",
      "..AI processing page 32\n",
      "{\n",
      "    \"question_id\": \"33\",\n",
      "    \"question\": \"What is the format of the extracted data after processing the raw PDF text?\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"json\",\n",
      "    \"question_type\": \"Multiple Choice\",\n",
      "    \"topic\": \"CS230\"\n",
      "}\n",
      "\n",
      "..AI processing page 33\n",
      "{\n",
      "    \"question_id\": \"34\",\n",
      "    \"question\": \"\",\n",
      "    \"choices\": [],\n",
      "    \"answer\": \"\",\n",
      "    \"question_type\": \"\",\n",
      "    \"topic\": \"CS230\"\n",
      "}AI-processed text saved to ../data/pdfs/CS230_Midterm_spring_2018.pdf-AI-all.txt\n"
     ]
    }
   ],
   "source": [
    "# Replace with your OpenAI API key and model\n",
    "# Create a list to store AI-processed text\n",
    "ai_processed_text_list = []\n",
    "\n",
    "# Open the PDF file in binary mode\n",
    "pdf_file = \"../data/pdfs/CS230_Midterm_spring_2018.pdf\"\n",
    "with open(pdf_file, 'rb') as pdf_file:\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "    # Iterate through each page and extract text\n",
    "    for page_num in range(len(pdf_reader.pages)):\n",
    "        page = pdf_reader.pages[page_num]\n",
    "        page_text = page.extract_text()\n",
    "\n",
    "        if len(page_text)>20:\n",
    "            # Dump unprocessed pages if desired\n",
    "            page_text_file = pdf_file.name + \"-extractedpage\" + str(page_num) + \".txt\"\n",
    "            with open(page_text_file, 'w', encoding='utf-8') as output_file:\n",
    "                output_file.write(page_text)\n",
    "\n",
    "            # Process with AI\n",
    "            ai_processed_text = aiprocessor(page_num, page_text, schema, model)\n",
    "\n",
    "            # Dump AI pages if desired\n",
    "            page_text_file = pdf_file.name + \"-AIpage\" + str(page_num) + \".txt\"\n",
    "            with open(page_text_file, 'w', encoding='utf-8') as output_file:\n",
    "                output_file.write(ai_processed_text)\n",
    "\n",
    "            # Append the AI-processed text to the list\n",
    "            ai_processed_text_list.append(ai_processed_text)\n",
    "\n",
    "# Combine all AI-processed text into a single string\n",
    "combined_text = \"\\n\".join(ai_processed_text_list)\n",
    "\n",
    "# Define the output text file name (same root name as the PDF)\n",
    "output_text_file = pdf_file.name + \"-AI-all.txt\"\n",
    "\n",
    "# Save the combined text into a .txt file\n",
    "with open(output_text_file, 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write(combined_text)\n",
    "\n",
    "print(f\"AI-processed text saved to {output_text_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_text = \"\\n\".join(ai_processed_text_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI-processed text saved to ../data/pdfs/exam.pdf-AI-all.txt\n"
     ]
    }
   ],
   "source": [
    "output_text_file = pdf_file.name + \"-AI-all.txt\"\n",
    "\n",
    "# Save the combined text into a .txt file\n",
    "with open(output_text_file, 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write(combined_text)\n",
    "\n",
    "print(f\"AI-processed text saved to {output_text_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
