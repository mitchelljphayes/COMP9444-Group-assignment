{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chains.openai_functions import create_structured_output_runnable\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain import OpenAI, PromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "import openai\n",
    "import PyPDF2\n",
    "import glob\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "from langchain.chains.openai_functions import (\n",
    "    create_openai_fn_chain,\n",
    "    create_structured_output_chain,\n",
    "    create_openai_fn_runnable,\n",
    "    create_structured_output_runnable,\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# %load_ext dotenv\n",
    "# %dotenv ./.env\n",
    "config = load_dotenv('.env')\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "model = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_text(filename):\n",
    "    file = open(filename, 'rb')\n",
    "    reader = PyPDF2.PdfReader(file)\n",
    "    text = ''\n",
    "    for page in range(len(reader.pages)):\n",
    "        page = reader.pages[page]\n",
    "        text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_text_from_dir(dir):\n",
    "    text = ''\n",
    "    for filename in glob.glob(os.path.join(dir, '*.pdf')):\n",
    "        text += extract_pdf_text(filename)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_pdf_text_from_dir(os.path.join('./data', 'pdfs'))\n",
    "with open('../data/texts/exams.txt', 'w') as f:\n",
    "    f.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_pdf_text('../data/pdfs/exam.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CS230: Deep Learning\n",
      "Fall Quarter 2018\n",
      "Stanford University\n",
      "Midterm Examination\n",
      "180 minutes\n",
      "Problem F\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.2, openai_api_key=OPENAI_API_KEY, model='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question(BaseModel):\n",
    "    \"\"\" An Exam question \"\"\"\n",
    "    question_id: int = Field(..., description=\"The question id\")\n",
    "    qustion_type: str = Field(..., description=\"The type of question, e.g. multiple choice, short answer, long answer\")\n",
    "    question: str = Field(..., description=\"The question text\")\n",
    "    choices: list[str] = Field(..., description=\"The choices for the question\")\n",
    "    answer: str = Field(..., description=\"The answer to the question\")\n",
    "    topic: str = Field(..., description=\"The topic of the question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Questions(BaseModel):\n",
    "    \"\"\" A collection of questions \"\"\"\n",
    "    questions: list[Question] = Field(..., description=\"The questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"question_id\": {\"type\": \"string\"},\n",
    "        \"question\": {\"type\": \"string\"},\n",
    "        \"choices\": {\"type\": \"array\", \"option\": {\"type\": \"string\"}},\n",
    "        \"answer\": {\"type\": \"string\"},\n",
    "        \"question_type\": {\"type\": \"string\"},\n",
    "        \"topic\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\"question_id\", \"question\", \"answer\", \"question_type\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_question_answer_data(doc, llm):\n",
    "    # input = prompt.format_prompt(exam_text=doc, schema=schema)\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a world class algorithm for extracting questions and answers from exams, and putting them in a structured format.\",\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Use the following exam text to extract the questions and answers. \\n\\n {exam_text}\",\n",
    "            ),\n",
    "            (\n",
    "                \"human\", \"Tip:  Make sure to include the question id, question, answer, and question type in the output. Also make sure to use the correct format\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    chain = create_structured_output_chain(Questions, llm, prompt, verbose=True)\n",
    "    extract = chain.run(doc)\n",
    "    return extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = extract_question_answer_data(text, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "page_content='CS230\\nSolution: (iii)\\n(e)(1 point) Consider the model deﬁned in question (d) with parameters initialized with\\nzeros.W[1]denotes the weight matrix of the ﬁrst layer. You forward propagate a batch\\nof examples, and then backpropagate the gradients and update the parameters. Which\\nof the following statements is true?\\n(i) Entries of W[1]may be positive or negative\\n(ii) Entries of W[1]are all negative\\n(iii) Entries of W[1]are all positive\\n(iv) Entries of W[1]are all zeros\\nSolution: (i)\\n(f)(2 points) Consider the layers landl−1 in a fully connected neural network:\\nThe forward propagation equations for these layers are:\\nz[l−1]=W[l−1]a[l−2]+b[l−1]\\na[l−1]=g[l−1](z[l−1])\\nz[l]=W[l]a[l−1]+b[l]\\na[l]=g[l](z[l])\\nWhich of the following propositions is true? Xavier initialization ensures that :\\n(i)Var(W[l−1]) is the same as Var(W[l]).\\n(ii)Var(b[l]) is the same as Var(b[l−1]).\\n(iii)Var(a[l]) is the same as Var(a[l−1]), at the end of training.\\n(iv)Var(a[l]) is the same as Var(a[l−1]), at the beginning of training.\\nSolution: (iv)\\n4' metadata={'source': 'data/pdfs/exam.pdf', 'page': 3}\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader('data/pdfs/exam.pdf')\n",
    "docs = loader.load_and_split()\n",
    "print(len(docs))\n",
    "print(docs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = docs[3].page_content\n",
    "# text = \"\"\"\n",
    "# Question 3 (Loss Functions, 17 points + 3 bonus points)\n",
    "# Equipped with cutting-edge Deep Learning knowledge, you are working with a biology lab.\n",
    "# Specifically, you're asked to build a classifier that predicts the animal type from a given\n",
    "# image into four ( ny= 4) classes: dog, cat, iguana, mouse . There's always exactly one\n",
    "# animal per image. You decide to use cross-entropy loss to train your network. Recall that\n",
    "# the cross-entropy (CE) loss for a single example is defined as follows:\n",
    "# LCE(^y;y) =nyP\n",
    "# i=1yilog ^yi\n",
    "# where ^y= (^y1;:::; ^yny)>represents the predicted probability distribution over the classes\n",
    "# andy= (y1;:::;yny)>is the ground truth vector, which is zero everywhere except for the\n",
    "# correct class (e.g. y= (1;0;0;0)>fordog, andy= (0;0;1;0)>foriguana ).\n",
    "# (a)(2 points) Suppose you're given an example image of an iguana. If the model correctly\n",
    "# predicts the resulting probability distribution as ^ y= (0:25;0:25;0:3;0:2)>, what is the\n",
    "# value of the cross-entropy loss? You can give an answer in terms of logarithms.\n",
    "# Solution:log 0:3\n",
    "# (b)(2 points) After some training, the model now incorrectly predicts mouse with distri-\n",
    "# butionh0:0;0:0;0:4;0:6ifor the same image. What is the new value of the cross-entropy\n",
    "# loss for this example?\n",
    "# Solution:log 0:4\n",
    "# (c)(2 points) Suprisingly, the model achieves lower loss for a misprediction than for a\n",
    "# correct prediction. Explain what implementation choices led to this phenomenon.\n",
    "# Solution: This is because our objective is to minimize CE-loss, rather than to\n",
    "# directly maximize accuracy. While CE-loss is a reasonable proxy to accuracy, there\n",
    "# is no guarantee that a lower CE loss will lead to higher accuracy.\n",
    "# (d)(2 points) Given your observation from question (c), you decide to train your neural\n",
    "# network with the accuracy as the objective instead of the cross-entropy loss. Is this a\n",
    "# good idea? Give one reason. Note that the accuracy of a model is defined as\n",
    "# Accuracy =(Number of correctly-classified examples)\n",
    "# (Total number of examples)\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = PromptTemplate.from_template(\n",
    "#     \"Read the following exam paper and extract the information requested in the schema for each question \\n\\n{exam_text}\\n\\n\\n\\n\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain = create_structured_output(schema, llm, prompt)\n",
    "# input = prompt.format_prompt(exam_text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = runnable.invoke(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions_json = extract_question_answer_data(docs[3], schema, llm)\n",
    "# with open('data/exams.json', 'w') as f:\n",
    "#     f.write(questions_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = {\n",
    "    \"title\": \"Person\",\n",
    "    \"description\": \"Identifying information about a person.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\"title\": \"Name\", \"description\": \"The person's name\", \"type\": \"string\"},\n",
    "        \"age\": {\"title\": \"Age\", \"description\": \"The person's age\", \"type\": \"integer\"},\n",
    "        \"fav_food\": {\n",
    "            \"title\": \"Fav Food\",\n",
    "            \"description\": \"The person's favorite food\",\n",
    "            \"type\": \"string\",\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"name\", \"age\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\n",
    "#             \"system\",\n",
    "#             \"You are a world class algorithm for extracting information in structured formats.\",\n",
    "#         ),\n",
    "#         (\n",
    "#             \"human\",\n",
    "#             \"Use the given format to extract information from the following input: {input}\",\n",
    "#         ),\n",
    "#         (\"human\", \"Tip: Make sure to answer in the correct format\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# runnable = create_structured_output_chain_runnable(json_schema, llm, prompt)\n",
    "# runnable.invoke({\"input\": \"Sally is 13\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aiprocessor(page_no, text, json_schema, llm=llm):\n",
    "    print(f\"\\n\\n..AI processing page {page_no}\")\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are a world class algorithm for extracting questions and answers from exams, and putting them in a structured format.\n",
    "- User input is messy raw text extracted from a PDF page by PyPDF2.\n",
    "- The goal is to identify each question and extract the details cleanly as json.\n",
    "- Make sure you get every question and answer, and use the correct format.\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"use the following schema to extract the questions and answers. \\n\\n {json_schema}\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"raw pdf text; extract and format tables: {text}\"\"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    api_params = {\"model\": llm, \"messages\": messages, \"stream\": True}\n",
    "    try:\n",
    "        api_response = openai.ChatCompletion.create(**api_params)\n",
    "        reply = \"\"\n",
    "        for delta in api_response:\n",
    "            if not delta['choices'][0]['finish_reason']:\n",
    "                word = delta['choices'][0]['delta']['content']\n",
    "                reply += word\n",
    "                print(word, end =\"\")       \n",
    "        return reply\n",
    "    except Exception as err:\n",
    "        error_message = f\"API Error page {page_no}: {str(err)}\"\n",
    "        print(error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "..AI processing page 0\n",
      "{\n",
      "  \"question\": \"Problem 1\",\n",
      "  \"points\": 10,\n",
      "  \"type\": \"Multiple Choice\"\n",
      "}\n",
      "\n",
      "..AI processing page 1\n",
      "[\n",
      "  {\n",
      "    \"question_number\": \"1\",\n",
      "    \"question_type\": \"Multiple Choice Questions\",\n",
      "    \"question_text\": \"Which of the following techniques does NOT prevent a model from overfitting?\",\n",
      "    \"choices\": [\n",
      "      \"Data augmentation\",\n",
      "      \"Dropout\",\n",
      "      \"Early stopping\",\n",
      "      \"None of the above\"\n",
      "    ],\n",
      "    \"correct_answer\": \"None of the above\",\n",
      "    \"score\": \"10\"\n",
      "  },\n",
      "  {\n",
      "    \"question_number\": \"2\",\n",
      "    \"question_type\": \"Multiple Choice Questions\",\n",
      "    \"question_text\": \"Consider the following data sets: Xtrain = (x(1);x(2);:::;x(mtrain)); Ytrain = (y(1);y(2);:::;y(mtrain)) Xtest = (x(1);x(2);:::;x(mtest)); Ytest = (y(1);y(2);:::;y(mtest)) You want to normalize your data before training your model. Which of the following propositions are true? (Circle all that apply.)\",\n",
      "    \"choices\": [\n",
      "      \"The normalizing mean and variance computed on the training set, and used to train the model, should be used to normalize test data.\",\n",
      "      \"Test data should be normalized with its own mean and variance before being fed to the network at test time because the test distribution might be different from the train distribution.\",\n",
      "      \"Normalizing the input impacts the landscape of the loss function.\",\n",
      "      \"In imaging, just like for structured data, normalization consists in subtracting the mean from the input and multiplying the result by the standard deviation.\"\n",
      "    ],\n",
      "    \"correct_answer\": [\n",
      "      \"The normalizing mean and variance computed on the training set, and used to train the model, should be used to normalize test data.\",\n",
      "      \"Normalizing the input impacts the landscape of the loss function.\"\n",
      "    ],\n",
      "    \"score\": \"3\"\n",
      "  }\n",
      "]\n",
      "\n",
      "..AI processing page 2\n",
      "{\n",
      "  \"questions\": [\n",
      "    {\n",
      "      \"question\": \"(c)(2 points) Which of the following is true, given the optimal learning rate?\",\n",
      "      \"options\": [\n",
      "        \"(i) Batch gradient descent is always guaranteed to converge to the global optimum of a loss function.\",\n",
      "        \"(ii) Stochastic gradient descent is always guaranteed to converge to the global optimum of a loss function.\",\n",
      "        \"(iii) For convex loss functions (i.e. with a bowl shape), batch gradient descent is guaranteed to eventually converge to the global optimum while stochastic gradient descent is not.\",\n",
      "        \"(iv) For convex loss functions (i.e. with a bowl shape), stochastic gradient descent is guaranteed to eventually converge to the global optimum while batch gradient descent is not.\",\n",
      "        \"(v) For convex loss functions (i.e. with a bowl shape), both stochastic gradient descent and batch gradient descent will eventually converge to the global optimum.\",\n",
      "        \"(vi) For convex loss functions (i.e. with a bowl shape), neither stochastic gradient descent nor batch gradient descent are guaranteed to converge to the global optimum.\"\n",
      "      ],\n",
      "      \"solution\": \"(iii)\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"(d)(1 point) You design the following 2-layer fully connected neural network. All activations are sigmoids and your optimizer is stochastic gradient descent. You initialize all the weights and biases to zero and forward propagate an input x2Rn\u00021 in the network. What is the output ^ y?\",\n",
      "      \"options\": [\n",
      "        \"(i) -1\",\n",
      "        \"(ii) 0\",\n",
      "        \"(iii) 0.5\",\n",
      "        \"(iv) 1\"\n",
      "      ],\n",
      "      \"solution\": \"No solution provided in the text.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "..AI processing page 3\n",
      "{\n",
      "  \"questions\": [\n",
      "    {\n",
      "      \"number\": \"(e)\",\n",
      "      \"points\": \"1\",\n",
      "      \"text\": \"Consider the model defined in question (d) with parameters initialized with zeros.W[1]denotes the weight matrix of the first layer. You forward propagate a batch of examples, and then backpropagate the gradients and update the parameters. Which of the following statements is true?\",\n",
      "      \"answers\": [\n",
      "        {\n",
      "          \"number\": \"(i)\",\n",
      "          \"text\": \"Entries of W[1] may be positive or negative\"\n",
      "        },\n",
      "        {\n",
      "          \"number\": \"(ii)\",\n",
      "          \"text\": \"Entries of W[1] are all negative\"\n",
      "        },\n",
      "        {\n",
      "          \"number\": \"(iii)\",\n",
      "          \"text\": \"Entries of W[1] are all positive\"\n",
      "        },\n",
      "        {\n",
      "          \"number\": \"(iv)\",\n",
      "          \"text\": \"Entries of W[1] are all zeros\"\n",
      "        }\n",
      "      ],\n",
      "      \"solution\": \"(iii)\"\n",
      "    },\n",
      "    {\n",
      "      \"number\": \"(f)\",\n",
      "      \"points\": \"2\",\n",
      "      \"text\": \"Consider the layers landl−1 in a fully connected neural network: The forward propagation equations for these layers are: z[l−1]=W[l−1]a[l−2]+b[l−1] a[l−1]=g[l−1](z[l−1])z[l]=W[l]a[l−1]+b[l] a[l]=g[l](z[l])\\nWhich of the following propositions is true? Xavier initialization ensures that :\",\n",
      "      \"answers\": [\n",
      "        {\n",
      "          \"number\": \"(i)\",\n",
      "          \"text\": \"Var(W[l−1]) is the same as Var(W[l]).\"\n",
      "        },\n",
      "        {\n",
      "          \"number\": \"(ii)\",\n",
      "          \"text\": \"Var(b[l]) is the same as Var(b[l−1]).\"\n",
      "        },\n",
      "        {\n",
      "          \"number\": \"(iii)\",\n",
      "          \"text\": \"Var(a[l]) is the same as Var(a[l−1]), at the end of training.\"\n",
      "        },\n",
      "        {\n",
      "          \"number\": \"(iv)\",\n",
      "          \"text\": \"Var(a[l]) is the same as Var(a[l−1]), at the beginning of training.\"\n",
      "        }\n",
      "      ],\n",
      "      \"solution\": \"(iv)\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "..AI processing page 4\n",
      "{\n",
      "  \"question_number\": \"2\",\n",
      "  \"question_type\": \"Short Answer\",\n",
      "  \"points\": \"35\",\n",
      "  \"question\": \"Please write concise answers.\",\n",
      "  \"answers\": [\n",
      "    {\n",
      "      \"part\": \"(a)\",\n",
      "      \"points\": \"2\",\n",
      "      \"answer\": \"You are training a logistic regression model. You initialize the parameters with 0's. Is this a good idea? Explain your answer.\",\n",
      "      \"solution\": \"There is no symmetry problem with this approach. In logistic regression, we have a = Wx + b where a is a scalar and W and x are both vectors. The derivative of the binary cross entropy loss with respect to a single dimension in the weight vector W[i] is a function of x[i], which is in general different than x[j] when i != j.\"\n",
      "    },\n",
      "    {\n",
      "      \"part\": \"(b)\",\n",
      "      \"points\": \"2\",\n",
      "      \"answer\": \"You design a fully connected neural network architecture where all activations are sigmoids. You initialize the weights with large positive numbers. Is this a good idea? Explain your answer.\",\n",
      "      \"solution\": \"Large W causes Wx to be large. When Wx is large, the gradient is small for sigmoid activation function. Hence, we will encounter the vanishing gradient problem.\"\n",
      "    },\n",
      "    {\n",
      "      \"part\": \"(c)\",\n",
      "      \"points\": \"2\",\n",
      "      \"answer\": \"You are given a dataset of 10x10 grayscale images. Your goal is to build a 5-class classifier. You have to adopt one of the following two options: • the input is flattened into a 100-dimensional vector, followed by a fully-connected layer with 5 neurons • the input is directly given to a convolutional layer with five 10x10 filters Explain which one you would choose and why.\",\n",
      "      \"solution\": \"The 2 approaches are the same. But the second one seems better in terms of computational costs (no need to flatten the input). We accept the answer 'the 2 approaches are the same'.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "..AI processing page 5\n",
      "{\n",
      "  \"questions\": [\n",
      "    {\n",
      "      \"question\": \"(d)(2 points) You are doing full batch gradient descent using the entire training set (not stochastic gradient descent). Is it necessary to shuffle the training data? Explain your answer.\",\n",
      "      \"answer\": \"Solution: It is not necessary. Each iteration of full batch gradient descent runs through the entire dataset and therefore order of the dataset does not matter.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"(e)(2 points) You would like to train a dog/cat image classifier using mini-batch gradient descent. You have already split your dataset into train, dev and test sets. The classes are balanced. You realize that within the training set, the images are ordered in such a way that all the dog images come first and all the cat images come after. A friend tells you: 'you absolutely need to shuffle your training set before the training procedure.' Is your friend right? Explain.\",\n",
      "      \"answer\": \"Solution: Yes, there is a problem. The optimization is much harder with mini-batch gradient descent because the loss function moves by a lot when going from the one type of image to another.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"(f)(2 points) You want to evaluate the classifier you trained in (e). Your test set (Xtest;Ytest) is such that the first m1 images are of dogs, and the remaining images are of cats. After shuffling Xtest and Ytest, you evaluate your model on it to obtain a classification accuracy a1%. You also evaluate your model on Xtest and Ytest without shuffling to obtain accuracy a2%. What is the relationship between a1 and a2 (>,\n",
      "\n",
      ",\n",
      "=,\n",
      "\n",
      ",\n",
      "\n",
      ")? Explain.\",\n",
      "      \"answer\": \"Solution: a1 = a2. When evaluating on the test set, the only form of calculation that you do is a single metric (e.g. accuracy) on the entire test set. The calculation of this metric on the entire test set does not depend on the ordering.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "..AI processing page 6\n",
      "{\n",
      "  \"questions\": [\n",
      "    {\n",
      "      \"number\": \"g\",\n",
      "      \"points\": \"2\",\n",
      "      \"question\": \"Data augmentation is often used to increase the amount of data you have. Should you apply data augmentation to the test set? Explain why.\",\n",
      "      \"solution\": \"Both answers are okay but need to be justified. If no, then explain that we want to test on real data only. If yes, then explain in which situation doing data augmentation on test set might make sense (e.g. as an ensemble approach in image classifiers).\"\n",
      "    },\n",
      "    {\n",
      "      \"number\": \"h\",\n",
      "      \"points\": \"2\",\n",
      "      \"question\": \"Weight sharing allows CNNs to deal with image data without using too many parameters. Does weight sharing increase the bias or the variance of a model?\",\n",
      "      \"solution\": \"Increases bias.\"\n",
      "    },\n",
      "    {\n",
      "      \"number\": \"i\",\n",
      "      \"points\": \"2\",\n",
      "      \"question\": \"You'd like to train a fully-connected neural network with 5 hidden layers, each with 10 hidden units. The input is 20-dimensional and the output is a scalar. What is the total number of trainable parameters in your network?\",\n",
      "      \"solution\": \"(20+1)*10 + (10+1)*10*4 + (10+1)*1\"\n",
      "    },\n",
      "    {\n",
      "      \"number\": \"j\",\n",
      "      \"points\": \"3\",\n",
      "      \"question\": \"Consider the figure below:\\nFigure 1: Input of shape (nH, nW, nC) = (10, 10, 1); There are five 4x4 convolutional filters with 'valid' padding and a stride of (2, 2)\\nWhat is the output shape after performing the convolution step in Figure 1? Write your answer in the following format: (nH, nW, nC).\",\n",
      "      \"solution\": \"(h=4, w=4, c=5)\"\n",
      "    },\n",
      "    {\n",
      "      \"number\": \"k\",\n",
      "      \"points\": \"2\",\n",
      "      \"question\": \"Recall that sigmoid(z) = 1 / (1 + e^(-z)) and tanh(z) = (e^z - e^(-z)) / (e^z + e^(-z)). Calculate d(sigmoid(z))/dz in terms of sigmoid(z) and d(tanh(z))/dz in terms of tanh(z).\",\n",
      "      \"solution\": \"Gradient for sigmoid: sigmoid(z) * (1 - sigmoid(z))\\nGradient for tanh: 1 - tanh^2(z)\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "..AI processing page 7\n",
      "[\n",
      "  {\n",
      "    \"question\": \"(l)(2 points) Assume that before training your neural network the setting is: The data is zero centered. All weights are initialized independently with mean 0 and variance 0.001. The biases are all initialized to 0. Learning rate is small and cannot be tuned. Using the result from (k), explain which activation function between tanh and sigmoid is likely to lead to a higher gradient during the first update.\",\n",
      "    \"answer\": \"Solution: tanh. During initialization, expected value of z is 0. Derivative of sigmoid w.r.t. z evaluated at zero = 0.5 * 0.5 = 0.25. Derivative of tanh w.r.t. z evaluated at zero = 1. tanh has higher gradient magnitude close to zero.\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"(m) You want to build a 10-class neural network classifier, Given a cat image, you want to classify which of the 10 cat breeds it belongs to.\",\n",
      "    \"answer\": \"(i)(2 points) What loss function do you use? Introduce the appropriate notation and write down the formula of the loss function. Solution: You would want to use the cross entropy loss given by L = -∑(y_i * log(^y_i)) (ii)(2 points) Assuming you train your network using mini-batch gradient descent with a batch size of 64, what cost function do you use? Introduce the appropriate notation and write down the formula of the cost function. Solution: If there are m training examples, J = (1/m) * ∑L(i) (iii)(3 points) One of your friends has trained a cat vs. non-cat classifier. It performs very well and you want to use transfer learning to build your own model. Explain what additional hyperparameters (due to the transfer learning) you will need to tune. Solution: The parameters you would need to choose are: 1) How many layers of the original network to keep. 2) How many new layers to introduce 3) How many of the layers of the original network would you want to keep frozen while fine-tuning.\"\n",
      "  }\n",
      "]\n",
      "\n",
      "..AI processing page 8\n",
      "{\n",
      "  \"questions\": [\n",
      "    {\n",
      "      \"number\": \"9\",\n",
      "      \"points\": \"3\",\n",
      "      \"question_text\": \"Approach 2 involves twice as many parameters as approach 1. Can approach 2 learn more complex models than approach 1?\"\n",
      "    },\n",
      "    {\n",
      "      \"number\": \"(n)\",\n",
      "      \"points\": \"3\",\n",
      "      \"question_text\": \"A binary classification problem could be solved with the two approaches described below:\"\n",
      "    },\n",
      "    {\n",
      "      \"number\": \"9\",\n",
      "      \"points\": \"3\",\n",
      "      \"question_text\": \"If yes, give the parameters ( Ws;bs) of a function that can be modeled by approach 2 but not by approach 1.\"\n",
      "    },\n",
      "    {\n",
      "      \"number\": \"9\",\n",
      "      \"points\": \"3\",\n",
      "      \"question_text\": \"If no, show that ( Ws;bs) can always be written in terms of (Wl;bl).\"\n",
      "    }\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    {\n",
      "      \"question_number\": \"9\",\n",
      "      \"answer_text\": \"No. For approach 1, the classifier is \u001b(w1x+b1)\u00150:5 which is equivalent to w1x+b1\u00150.\"\n",
      "    },\n",
      "    {\n",
      "      \"question_number\": \"9\",\n",
      "      \"answer_text\": \"Now, for approach 2, define w2 as the weights for the first output and w2 as the weights for the second output. Similarly, define b2 as the bias for the first output and b2 as the bias for the second output.\"\n",
      "    },\n",
      "    {\n",
      "      \"question_number\": \"9\",\n",
      "      \"answer_text\": \"For approach 2, the classifier is exp(w2x+b2) exp(w2x+b2)+exp(w2x+b2)\u0015exp(w2x+b2) exp(w2x+b2)+exp(w2x+b2).\"\n",
      "    },\n",
      "    {\n",
      "      \"question_number\": \"9\",\n",
      "      \"answer_text\": \"Since the denominator is positive and same on both sides, this is equivalent to exp(w2x+b2)\u0015exp(w2x+b2).\"\n",
      "    },\n",
      "    {\n",
      "      \"question_number\": \"9\",\n",
      "      \"answer_text\": \"Since exp is a monotonic increasing function, this is equivalent to w2x+b2\u0015 w2x+b2.\"\n",
      "    },\n",
      "    {\n",
      "      \"question_number\": \"9\",\n",
      "      \"answer_text\": \"This is equivalent to ( w2-w2)x+ (b2-b2)\u00150.\"\n",
      "    },\n",
      "    {\n",
      "      \"question_number\": \"9\",\n",
      "      \"answer_text\": \"Given any w2 and b2 in approach 2, we can get the exact same model by setting w1=w2-w2 and b1=b2-b2 in approach 1.\"\n",
      "    },\n",
      "    {\n",
      "      \"question_number\": \"9\",\n",
      "      \"answer_text\": \"Alternatively, one can argue based on degrees of freedom.\"\n",
      "    },\n",
      "    {\n",
      "      \"question_number\": \"9\",\n",
      "      \"answer_text\": \"Note that in this case, we are asking about model complexity, which is independent of loss function.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "..AI processing page 9\n",
      "{\n",
      "  \"question\": \"Question 3 (Attacks on Neural Networks, 15 points)\",\n",
      "  \"subquestions\": [\n",
      "    {\n",
      "      \"question\": \"(a)(2 points) Name a type of neural network attack that Alice cannot use against Bob's model, and explain why it cannot be used in this case.\",\n",
      "      \"answer\": \"White-box attacks\\nWhite box attacks require access to the weights of the model whereas black box attacks do not.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"(b)(3 points) How can Alice forge an image xiguana which looks like an iguana but will be wrongly classified as a plant by Bob's model? Give an iterative method and explicitly mention the loss function.\",\n",
      "      \"answer\": \"L=jj^y-yiguanajj+¬jjx-xplantjj\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"(c)(3 points) It is possible to add an invisible perturbation ∆ to an image x, such that ~x=x+∆ is misclassified by a model. Assuming you have access to the target model, explain how you would find ∆.\",\n",
      "      \"answer\": \"∆ can be chosen using a method such as the Fast Gradient Sign Based Method. It is an iterative method that requires access to the model (a white-box attack)\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"(d)(2 points) Given that you have obtained ∆, you notice that ||∆||<<1. Explain why even though the adversarial noise has a small magnitude, it can cause a large change in the output of a model.\",\n",
      "      \"answer\": \"Since the dimensionality of the images is very large, even though the noise is small, it can cause a large swing in the output. To illustrate consider ~x=x+∆. While passing this through a single layer, W~x=Wx+W∆=P ||W∆||. If ||∆|| is very large, this can have a significant contribution.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"(e)(3 points) Alice doesn't have access to Bob's network. How can she still generate an adversarial example using the method described above?\",\n",
      "      \"answer\": \"Alice can train her own substitute model using a similar architecture and train it on a labeled dataset. She can then use this substitute model to generate the adversarial example as described in part (b).\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "..AI processing page 10\n",
      "{\n",
      "  \"questions\": [\n",
      "    {\n",
      "      \"question_number\": \"CS230\",\n",
      "      \"question_text\": \"Solution: Adversarial examples are transferable, and so Alice can use Fast Gradient Sign Based Method on a di\u000berent model, built for the same task (cat vs. non-cat classi\fcation), and it is likely that this adversarial example will also be misclassi\fed by Bob's model.\"\n",
      "    },\n",
      "    {\n",
      "      \"question_number\": \"(f)\",\n",
      "      \"question_text\": \"(2 points) To defend himself against Alice's attacks, Bob is thinking of using dropout. Dropout randomly shuts down certain neurons of the network, and makes it more robust to changes in the input. Thus, Bob has the intuition that the network will be less vulnerable to adversarial examples. Is Bob correct? Explain why or why not.\"\n",
      "    }\n",
      "  ],\n",
      "  \"answers\": [\n",
      "    {\n",
      "      \"question_number\": \"CS230\",\n",
      "      \"answer_text\": \"No, dropout isn't used at test time while Alice will forge her adversarial examples by querying a network at test time.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "..AI processing page 11\n",
      "{\n",
      "  \"question\": \"Question 4 (Autonomous driving case study, 27 points)\",\n",
      "  \"text\": \"As the CEO of an autonomous driving company Potato Inc., you are trying to build a pipeline to predict a car's steering angle \u00122[\u00001;1] from an input image.\\n(a) After setting up a camera on the hood of your car, an expert driver in your team has collected the dataset Xin a city where most roads are straight. She always drives in the center of the lane.\\n\\n(i)(2 points) Describe two problems that can occur if you train a model on Xand test it on real-world data.\\n\\nSolution: System hasn't seen situations where you're not driving sensibly, or away from the centre of the lane. This can lead to an accumulation of errors if your model output slightly di\u000bers from the correct output at a given time step. Also, a majority of outputs from the steering will be 0, if the driver was always in the center of the lane, and most of the roads were straight.\\n\\n(ii)(3 points) Without collecting new data, how can you help your model gener-alize to real world data? Describe the details of how you would implement the approach.\\n\\nSolution: Data augmentation. Slight left and right translations of the images, with a corresponding change in steering angle. For example, if you translate an image to the right, add some small quantity \u000fto the steering angle (if positive implies steering to the left), so that you're now steering to the left.\\n\\n(b)(2 points) Give two reasons why you wouldn't use an end-to-end model to predict the steering direction ( \u0012) from the input image.\\n\\nSolution: 1 - If there's not enough data. 2 - If it is easier to collect data for submodules than end-to-end data. For instance, collecting high variance data while driving a car around with a camera on the hood is costly. Finding images with cars/pedestrians and labelling with bounding boxes might be easier, and helps train \\\"car detector\\\" and \\\"pedestrian detector\\\" submodules.\\n12\"\n",
      "}\n",
      "\n",
      "..AI processing page 12\n",
      "{\n",
      "  \"questions\": [\n",
      "    {\n",
      "      \"question\": \"(i)(3 points) What data do you need to train the submodules in the pipeline presented in Figure 2?\",\n",
      "      \"answer\": \"To train the Car Detector, we need to collect Xc(images from a camera hood of a car) and Yc(bounding box labels localizing the cars.)\\nTo train the Pedestrian Detector, we need to collect Xp(images from a camera hood of a car) and Yp(bounding box labels localizing the pedestrians.)\\nTo train the Path planner, we need to collect Xs(bounding boxes localizing the cars and the pedestrians) and Ys(steering angle.)\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"(ii)(3 points) Explain how you would collect this data.\",\n",
      "      \"answer\": \"(Xc;Yc) : Put a camera on the hood of a car, label the bounding boxes by hand. You can also download images from roads online or use online datasets such as COCO or PASCAL VOC.\\n(Xp;Yp) : Put a camera on the hood of a car, label the bounding boxes by hand. You can also download images from roads online or use online datasets such as COCO or PASCAL VOC.\\n(Xs;Ys) : Put a camera on the hood of a car, label the bounding boxes by hand.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "..AI processing page 13\n",
      "{\n",
      "  \"questions\": [\n",
      "    {\n",
      "      \"question\": \"(d)(2 points) Propose a metric to measure the performance of your pipeline model.\",\n",
      "      \"answer\": \"Sum of absolute deviations (i.e. L1 distance) between ground truth and predicted steering angle.\\nSum of squared errors (i.e. L2 distance) between ground truth and predicted steering angle.\\n(not expected) You can also design metrics for submodules. For CandPit might be mAP (/mean IoU)\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"(e) Assume that you have designed a metric that scores your pipeline between 0% (bad) and 100% (good.) On unseen data from the real world, your entire pipeline gets a score of 54%.\\n(i)(2 points) Define Bayes error and human level error. How do these two compare with each other? ( \u0014,\u0015, =)\",\n",
      "      \"answer\": \"Solution: Bayes error is a lower bound on the minimum error that can be achieved. Human level error is the error achieved by an expert human on the same task. The Bayes error is \u0014human level error.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"(ii)(2 points) How would you measure human level error for your autonomous driving task?\",\n",
      "      \"answer\": \"Solution: Possible solution: Create a simulator with the same path followed by the car while recording data, and have an expert try following the path.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "..AI processing page 14\n",
      "{\n",
      "  \"questions\": [\n",
      "    {\n",
      "      \"id\": \"(iii)\",\n",
      "      \"points\": 3,\n",
      "      \"text\": \"Human level performance is 96% for your task. Your pipeline should do a lot better! Assuming one component (among C, PandS) is failing, explain how you would identify which one it is.\",\n",
      "      \"solution\": \"Replace every component in turn with the ground truth. For example if you are testing whether C is failing, supply S with ground truth bounding boxes of all the cars, and see if performance of the overall pipeline increases. If it increases significantly, it is likely that the failing component is C.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"(iv)\",\n",
      "      \"points\": 3,\n",
      "      \"text\": \"You have identified that the failing component is P. P fails at detecting pedestrians in 3 distinct settings: at night, when it is snowing, and when it is raining. You cannot solve the 3 problems at once. How would you decide which of the 3 problems to focus on first?\",\n",
      "      \"solution\": \"Do an error analysis. Ascertain how much of your error is due to each of the three cases, by using images only from these cases. Then focus on the case which contributes the most to your error. You could also choose to based your choice in practical situations. If Potato Inc. is in a place which has high rainfall, you might want to focus on the errors made on images with rain.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"(f)\",\n",
      "      \"points\": 2,\n",
      "      \"text\": \"After fixing the only failing component, your pipeline gets a score of 67%. What could be wrong?\",\n",
      "      \"solution\": \"Each of the modules has been trained independently and on perfect outputs from the other components. It is likely that small perturbations/errors in the output of previous components is affecting the performance of the other components, leading to a snowballing of errors.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "..AI processing page 15\n",
      "{\n",
      "  \"question\": \"Question 5 (Traversability Estimation Using GANs, 14 points)\",\n",
      "  \"question_text\": \"In robot navigation, the traversability problem aims to answer the question: can the robot traverse through a situation?\",\n",
      "  \"figure\": \"Figure 3: Example of different situations. Left: traversable; Right: non-traversable\",\n",
      "  \"context\": \"You want to estimate the traversability of a situation for a robot. Traversable data is easy to collect (e.g. going through a corridor) while non-traversable data is very costly (e.g. going down the stairs). You have a large and rich datasetXof traversable images, but no non-traversable images.\\nThe question you are trying to answer is: 'Is it possible to train a neural network that classifies whether or not a situation is traversable using only dataset X?' More precisely, if a non-traversable image was fed into the network, you want the network to predict that it is non-traversable. In this part, you will use a Generative Adversarial Network (GAN) to solve this problem.\",\n",
      "  \"subquestions\": [\n",
      "    {\n",
      "      \"subquestion\": \"(a) Before considering the traversability problem, let us do a warm-up question. Consider that you have trained a network fw:Rnx\u00021!Rny\u00021. The parameters of the network are denoted w. Given an input x2Rnx\u00021, the network outputs ^ y=fw(x)2Rny\u00021. Given an arbitrary output ^ y\u0003, you would like to use gradient descent optimization to generate an input x\u0003 such thatfw(x\u0003) = ^y\u0003.\",\n",
      "      \"subquestion_parts\": [\n",
      "        {\n",
      "          \"part\": \"(i)(2 points) Write down the formula of the l2loss function you would use.\",\n",
      "          \"solution\": \"L(x\u0003;^y) =kfw(x\u0003)\u0000^y\u0003k2\"\n",
      "        },\n",
      "        {\n",
      "          \"part\": \"(ii)(2 points) Write down the update rule of the gradient descent optimizer in terms ofl2norm.\",\n",
      "          \"solution\": \"16\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "..AI processing page 16\n",
      "{\n",
      "  \"questions\": [\n",
      "    {\n",
      "      \"question\": \"(iii)(2 points) Calculate the gradient of the loss in your update rule in terms of\\n@fw(x)\\n@w, and@fw(x)\\n@x(it is not necessary to use both terms).\",\n",
      "      \"answer\": \"Solution: x\u0003\\nt+1=x\u0003\\nt\u00002\u000b\u0001\u0010\\n@fw(x)\\n@xjx=x\u0003\\nt\u0011T\\n\u0001(fw(x\u0003\\nt)\u0000^y\u0003)\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"(i)(2 points) Consider a new image x. How can you \fnd a code zsuch that the\\noutput of the generator G(z) would as close as possible to x?\",\n",
      "      \"answer\": \"Solution: We can apply the backpropagation technique developed in part\\n(a), wherezplays the role of x\u0003, andxplays the role of y\u0003.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"(ii)(2 points) Suppose you've found zsuch thatG(z) is the closest possible value to\\nxout of all possible z. How can you decide if xrepresents a traversable situation\\nor not? Give a qualitative explanation.\",\n",
      "      \"answer\": \"Solution: We compare G(z) to the image xin the sense that if kG(z)\u0000xk2\\nis \\\"big\\\" then xis non-traversable, and vice versa.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"(iii)(2 points) Instead of using the method above, Amelia suggests directly running\\nxthrough the discriminator D. Amelia believes that if D(x) predicts that it is\\na real image, then xis likely a traversable situation. Else, it is likely to be a\\nnon-traversable situation. Do you think that Amelia's method would work and\\nwhy?\",\n",
      "      \"answer\": \"Solution: The reason is if the GAN is trained perfectly, which is the case\\nhere, then the discriminator cannot tell if a generated image by the generator\\nis real or fake, that is, traversable or non-traversable.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"(c)(2 points) The iterative method developed in part (a) is too slow for your self-driving\\napplication. Given ^ y\u0003you need to generate x\u0003in real time. Come up with a method us-\\ning an additional network to generate x\u0003faster, e.g., with a single forward propagation.\",\n",
      "      \"answer\": \"Solution: We can train a second network to spit out the inverse of the network\\nin hand.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "..AI processing page 17\n",
      "{\n",
      "  \"question\": \"What problem would arise if the predicted logits zcontain very large values?\",\n",
      "  \"solution\": \"Due to the exp function, numerical overflow may occur because exp(100000) is too large.\"\n",
      "}\n",
      "\n",
      "..AI processing page 18\n",
      "{\n",
      "  \"questions\": [\n",
      "    {\n",
      "      \"question\": \"Express the loss LCE(^y;y) in terms of the logits vector z and the LSE function.\",\n",
      "      \"answer\": \"LCE(^y;y) = log ^yc = logexp(zc)/Z = zc+ LSE( z)\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"Compute the following partial derivative: @/@zjLSE(z)\",\n",
      "      \"answer\": \"exp(zj)/PK i=1exp(zi) or ^yj or (softmax( z))j\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "..AI processing page 19\n",
      "{\n",
      "  \"questions\": [\n",
      "    {\n",
      "      \"number\": \"(d)\",\n",
      "      \"points\": \"2 point\",\n",
      "      \"text\": \"Compute the following partial derivative (for the correct class c):\",\n",
      "      \"answer\": \"^yc\u00001\"\n",
      "    },\n",
      "    {\n",
      "      \"number\": \"(e)\",\n",
      "      \"points\": \"2 point\",\n",
      "      \"text\": \"Compute the following partial derivative (for an incorrect class j6=c):\",\n",
      "      \"answer\": \"^yj\"\n",
      "    },\n",
      "    {\n",
      "      \"number\": \"(f)\",\n",
      "      \"points\": \"2 points\",\n",
      "      \"text\": \"Using the results of Part (d) and (e), express the following gradient using ^yandy:\",\n",
      "      \"answer\": \"^y\u0000y\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "..AI processing page 20\n",
      "{\n",
      "  \"questions\": [\n",
      "    {\n",
      "      \"question\": \"(g)(3 points) Prove the following identity for some fixed constant v in R:\",\n",
      "      \"answer\": \"LSE(x1;:::;xn) = LSE(x1−v;:::;xn−v) +v\",\n",
      "      \"hint\": \"Hint: exp(a+b) = exp(a) exp(b)\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"(h)(2 points) Explain how the above identity can be used to avoid overflow.\",\n",
      "      \"answer\": \"Solution: You can set v = max{x1;:::;xn} to compute LSE(x) without overflow.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "..AI processing page 21\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/utils/pdf_extraction.ipynb Cell 22\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/utils/pdf_extraction.ipynb#X30sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     output_file\u001b[39m.\u001b[39mwrite(page_text)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/utils/pdf_extraction.ipynb#X30sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m# Process with AI\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/utils/pdf_extraction.ipynb#X30sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m ai_processed_text \u001b[39m=\u001b[39m aiprocessor(page_num, page_text)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/utils/pdf_extraction.ipynb#X30sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m# Dump AI pages if desired\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/utils/pdf_extraction.ipynb#X30sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m page_text_file \u001b[39m=\u001b[39m pdf_file\u001b[39m.\u001b[39mname \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m-AIpage\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(page_num) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.txt\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/utils/pdf_extraction.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/utils/pdf_extraction.ipynb#X30sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m api_params \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m: model, \u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m: messages, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mTrue\u001b[39;00m}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/utils/pdf_extraction.ipynb#X30sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/utils/pdf_extraction.ipynb#X30sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     api_response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mapi_params)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/utils/pdf_extraction.ipynb#X30sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     reply \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mjp/Documents/Study/UNSW/23T3-9444/group-assignment/utils/pdf_extraction.ipynb#X30sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mfor\u001b[39;00m delta \u001b[39min\u001b[39;00m api_response:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/group-assignment-f09OljWU/lib/python3.11/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/group-assignment-f09OljWU/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    131\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    139\u001b[0m ):\n\u001b[1;32m    140\u001b[0m     (\n\u001b[1;32m    141\u001b[0m         deployment_id,\n\u001b[1;32m    142\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    153\u001b[0m     )\n\u001b[0;32m--> 155\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    156\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    157\u001b[0m         url,\n\u001b[1;32m    158\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    159\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    160\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    161\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    162\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    166\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    167\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/group-assignment-f09OljWU/lib/python3.11/site-packages/openai/api_requestor.py:289\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    279\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    280\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    288\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 289\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[1;32m    290\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[1;32m    291\u001b[0m         url,\n\u001b[1;32m    292\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    293\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    294\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    295\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    296\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    297\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    298\u001b[0m     )\n\u001b[1;32m    299\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    300\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/group-assignment-f09OljWU/lib/python3.11/site-packages/openai/api_requestor.py:606\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    604\u001b[0m     _thread_context\u001b[39m.\u001b[39msession_create_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    605\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 606\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    607\u001b[0m         method,\n\u001b[1;32m    608\u001b[0m         abs_url,\n\u001b[1;32m    609\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    610\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    611\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    612\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    613\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[1;32m    614\u001b[0m         proxies\u001b[39m=\u001b[39;49m_thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mproxies,\n\u001b[1;32m    615\u001b[0m     )\n\u001b[1;32m    616\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    617\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/group-assignment-f09OljWU/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/group-assignment-f09OljWU/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/group-assignment-f09OljWU/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/group-assignment-f09OljWU/lib/python3.11/site-packages/urllib3/connectionpool.py:791\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    790\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    792\u001b[0m     conn,\n\u001b[1;32m    793\u001b[0m     method,\n\u001b[1;32m    794\u001b[0m     url,\n\u001b[1;32m    795\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    796\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    797\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    798\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    799\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    800\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    801\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    802\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    803\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    804\u001b[0m )\n\u001b[1;32m    806\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[1;32m    807\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/group-assignment-f09OljWU/lib/python3.11/site-packages/urllib3/connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[39m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    538\u001b[0m \u001b[39mexcept\u001b[39;00m (BaseSSLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    539\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/group-assignment-f09OljWU/lib/python3.11/site-packages/urllib3/connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mresponse\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    460\u001b[0m \u001b[39m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    463\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[39m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1379\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline(_MAXLINE \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1307\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1308\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1309\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1310\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1311\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1312\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1166\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1168\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1169\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Replace with your OpenAI API key and model\n",
    "# Create a list to store AI-processed text\n",
    "ai_processed_text_list = []\n",
    "\n",
    "# Open the PDF file in binary mode\n",
    "pdf_file = \"../data/pdfs/exam.pdf\"\n",
    "with open(pdf_file, 'rb') as pdf_file:\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "    # Iterate through each page and extract text\n",
    "    for page_num in range(len(pdf_reader.pages)):\n",
    "        page = pdf_reader.pages[page_num]\n",
    "        page_text = page.extract_text()\n",
    "\n",
    "        if len(page_text)>20:\n",
    "            # Dump unprocessed pages if desired\n",
    "            page_text_file = pdf_file.name + \"-extractedpage\" + str(page_num) + \".txt\"\n",
    "            with open(page_text_file, 'w', encoding='utf-8') as output_file:\n",
    "                output_file.write(page_text)\n",
    "\n",
    "            # Process with AI\n",
    "            ai_processed_text = aiprocessor(page_num, page_text)\n",
    "\n",
    "            # Dump AI pages if desired\n",
    "            page_text_file = pdf_file.name + \"-AIpage\" + str(page_num) + \".txt\"\n",
    "            with open(page_text_file, 'w', encoding='utf-8') as output_file:\n",
    "                output_file.write(page_text)\n",
    "\n",
    "            # Append the AI-processed text to the list\n",
    "            ai_processed_text_list.append(ai_processed_text)\n",
    "\n",
    "# Combine all AI-processed text into a single string\n",
    "combined_text = \"\\n\".join(ai_processed_text_list)\n",
    "\n",
    "# Define the output text file name (same root name as the PDF)\n",
    "output_text_file = pdf_file.name + \"-AI-all.txt\"\n",
    "\n",
    "# Save the combined text into a .txt file\n",
    "with open(output_text_file, 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write(combined_text)\n",
    "\n",
    "print(f\"AI-processed text saved to {output_text_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_text = \"\\n\".join(ai_processed_text_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI-processed text saved to ../data/pdfs/exam.pdf-AI-all.txt\n"
     ]
    }
   ],
   "source": [
    "output_text_file = pdf_file.name + \"-AI-all.txt\"\n",
    "\n",
    "# Save the combined text into a .txt file\n",
    "with open(output_text_file, 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write(combined_text)\n",
    "\n",
    "print(f\"AI-processed text saved to {output_text_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
