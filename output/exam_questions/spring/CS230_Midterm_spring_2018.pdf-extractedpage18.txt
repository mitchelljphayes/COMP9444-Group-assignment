CS230
Question 5 (Numpy coding, 10 points)
In this question, you will implement a fully-connected network. The architecture is LINEAR
!RELU!DROPOUT!BATCHNORM . This is a dummy architecture that has been
made up for this exercise.
The code below implements the forward propagation, but some parts are missing. You will
need to ll the parts between the tags (START CODE HERE) and (END CODE HERE)
based on the given instructions . Note that we are using only numpy (not tensorow),
and the relu function has been imported for you.
import numpy as np
from utils import relu
def forward_propagation_with_dropout_and_batch_norm(X, parameters, keep_prob = 0.5):
"""
Implements the forward propagation: LINEAR -> RELU -> DROPOUT -> BATCHNORM.
Arguments:
X -- input data of shape (n_x, m)
parameters -- python dictionary containing your parameters:
W -- weight matrix of shape (n_y, n_x)
b -- bias vector of shape (n_y, 1)
keep_prob -- probability of keeping a neuron active during drop-out, scalar
gamma -- shifting parameter of the batch normalization layer, scalar
beta -- scaling parameter of the batch normalization layer, scalar
epsilon -- stability parameter of the batch normalization layer, scalar
Returns:
A2 -- output of the forward propagation
cache -- tuple, information stored for computing the backward propagation
"""
# retrieve parameters
W = parameters["W"]
b = parameters["b"]
gamma = parameters["gamma"]
beta = parameters["beta"]
epsilon = parameters["epsilon"]
### START CODE HERE ###
# LINEAR
Z1 =
# RELU
A1 =
19