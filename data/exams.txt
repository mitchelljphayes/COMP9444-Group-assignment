
Question 3 (Loss Functions, 17 points + 3 bonus points)
Equipped with cutting-edge Deep Learning knowledge, you are working with a biology lab.
Specifically, you're asked to build a classifier that predicts the animal type from a given
image into four ( ny= 4) classes: dog, cat, iguana, mouse . There's always exactly one
animal per image. You decide to use cross-entropy loss to train your network. Recall that
the cross-entropy (CE) loss for a single example is defined as follows:
LCE(^y;y) =nyP
i=1yilog ^yi
where ^y= (^y1;:::; ^yny)>represents the predicted probability distribution over the classes
andy= (y1;:::;yny)>is the ground truth vector, which is zero everywhere except for the
correct class (e.g. y= (1;0;0;0)>fordog, andy= (0;0;1;0)>foriguana ).
(a)(2 points) Suppose you're given an example image of an iguana. If the model correctly
predicts the resulting probability distribution as ^ y= (0:25;0:25;0:3;0:2)>, what is the
value of the cross-entropy loss? You can give an answer in terms of logarithms.
Solution:log 0:3
(b)(2 points) After some training, the model now incorrectly predicts mouse with distri-
butionh0:0;0:0;0:4;0:6ifor the same image. What is the new value of the cross-entropy
loss for this example?
Solution:log 0:4
(c)(2 points) Suprisingly, the model achieves lower loss for a misprediction than for a
correct prediction. Explain what implementation choices led to this phenomenon.
Solution: This is because our objective is to minimize CE-loss, rather than to
directly maximize accuracy. While CE-loss is a reasonable proxy to accuracy, there
is no guarantee that a lower CE loss will lead to higher accuracy.
(d)(2 points) Given your observation from question (c), you decide to train your neural
network with the accuracy as the objective instead of the cross-entropy loss. Is this a
good idea? Give one reason. Note that the accuracy of a model is defined as
Accuracy =(Number of correctly-classified examples)
(Total number of examples)
